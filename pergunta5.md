**Pergunta 5**: Quais são as principais diferenças entre modelos de séries temporais tradicionais, como ARIMA, e modelos baseados em machine learning, como LSTM?

**Segmentos recuperados pelo E5**:
- {'corpus_id': 3375, 'score': 0.8565858006477356, 'doc_id': 3375, 'video_id': 'YRg33lhmAnQ', 'url': 'https://www.youtube.com/embed/YRg33lhmAnQ?start=530&end=666', 'inicio': '00:08:50', 'fim': '00:11:06', 'texto': 'precisa preocupar que a gente tem que entender né que LS tem a mesma coisa né uma célula de memória que funciona melhor do que uma rede recorrente tradicional e por que que ela funciona melhor ó ela tem um monte de elementos aqui mas o que que é por enquanto a gente não vai entrar nessa aqui dentro não mas ela tem algumas portas ou que são ajustáveis é uma porta de esquecimento uma porta de entrada e uma porta de saída Tá e isso então que é as coisas que a gente consegue isso aqui ó tem pesos aqui dentro tá essas portas aqui tem tem pesos que pode ser treinados E aí então ele é stm bem treinada ela vai saber quando que ela tem que esquecer elementos da sequência e quando que ela tem que aceitar elementos da entrada né questão da memória aqui né quando que eu tenho que usar memória que eu já tenho e quando que isso não é mais importante então ela ajusta ou até amplifica né O que que já já veio antes e aqui também né para calcular a saída dela tá e cada porta recebe algumas coisas né recebe algumas coisas e dá como saída né coisas para alimentar outros elementos ali esse que é a questão esse que é o talvez o básico da Leste m né é as portas aqui ó e cada porta dá né processo tem alguma finalidade recebe uma entrada né e dar como saída uma resposta apropriada a gente vai ver cada uma delas ó primeira coisa a porta diz esquecimento a porta desse esquecimento Recebe como entrada não só né a entrada atual ali ó na série temporal né o x naquele momento XT aqui ó fiz naquele momento a entrada o tempo né para digamos tô dizendo a Bitcoin né Qual é o dia aqui que eu quero para dizer não só isso como entrada mas também né o que veio a memória do processamento anterior né então isso aqui foi tudo que ele é time calculou até aqui então da mesma forma que a rede recorrente né'}
- {'corpus_id': 3374, 'score': 0.854716956615448, 'doc_id': 3374, 'video_id': 'YRg33lhmAnQ', 'url': 'https://www.youtube.com/embed/YRg33lhmAnQ?start=420&end=561', 'inicio': '00:07:00', 'fim': '00:09:21', 'texto': 'assim né como se fosse mesmo uma rede compridona lá só que é a cópias da mesma rede aqui né cada momento só que quando eu vou atualizar para primeiro para edição ali eu não consigo é a mesma ideia tá só que uma é comprida digamos conceitualmente mesmo né comprida no fisicamente né Tem muitas camadas e essa aqui é por eu processar né nas camadas dela múltiplas vezes o processamento fica Comprido né e o gradiente soma igual deu para pegar mais ou menos tá então agora eu vou falar um pouquinho da lspm mas aqui também a gente pode pensar na stm como uma caixa preta tá uma rede recorrente né O que que a gente precisa levar embora né da aula de hoje lsm é uma rede recorrente melhor melhor que que o modelo tradicional é melhor que o modelo tradicional o modelo tradicional por isso que esses neurônios da IBM que tem a memória resistiva que tá tá tão tendência para [Música] Como eu posso falar é uma das tendências né O que é que até assim ó para processamento de sequências elast m é muito bom só que em especial texto é uma sequência né só que para texto aí tem os Transformers né que são um tipo de sequência específica chamada texto né Transformers são a melhor tecnologia mas aqui nós vamos ver tá o princípio então a gente não precisa se ficar travado aqui ah não entendi essas matemática aqui dessas ativação Não precisa preocupar que a gente tem que entender né que LS tem a mesma coisa né uma célula de memória que funciona melhor do que uma rede recorrente tradicional e por que que ela funciona melhor ó ela tem um monte de elementos aqui mas o que que é por enquanto a gente não vai entrar nessa aqui dentro não mas ela tem algumas portas ou que são ajustáveis é uma porta de esquecimento uma porta de entrada'}
- {'corpus_id': 1288, 'score': 0.8544754385948181, 'doc_id': 1288, 'video_id': 'e4Z4VThdM7U', 'url': 'https://www.youtube.com/embed/e4Z4VThdM7U?start=3444&end=3576', 'inicio': '00:57:24', 'fim': '00:59:36', 'texto': 'processo de treinamento dos modelos tá outra forma por exemplo se a gente sabe que quando a gente trabalha com probabilidade a gente sabe né alguns Eventos São mais raros então a gente ajusta as probabilidades Com base no conhecimento prévio não Com base no conhecimento dos dados porque às vezes a coleta de dados pode ter alguns artefatos amostrais Enfim então existem sim formas de fazer isso aí e são diversas assim tá mas acho que isso é um ponto importante sim o deles tem uma pergunta pode falar deles né dessa pergunta aqui que foi feita e fiquei com uma curiosidade a partir de tudo que foi falado aí a simbólica tradicional aquela baseada além prologista aquelas coisas de antigamente a gente pode dizer que elas estão em desuso São arcaicas ou ainda Elas têm alguma utilidade no dia de hoje Não elas têm Elas têm utilidade o aprendizado de máquina ele é hoje uma sub área da yá E aí tem todas as outras áreas da Iah que lidam com uma forma mais clássica de ser Tá então não tá não quer dizer que esteja em completo ou seja inútil digamos assim certo mas a gente só tá vai dar ênfase nessa nessa questão desse paradigma que é algo que eu diria assim hoje um dos grandes motivos da ia ser tão famosa como é é o aprendizado de máquina e às vezes a pessoa nem sabem que é aprendizado de máquina nem sabe que aquilo é o termo aprendizado de máquina acham que é claro como eu falei ai é uma área de conhecimento muito grande que tem várias subiárias Então o que permitiu toda essa essa fama essa né enfim daí é justamente a capacidade de análise que o aprendizado de máquina trouxe e que existem outras obviamente outras evoluções Talvez daqui alguns anos a gente veja outras sub áreas de ar fazendo com que a Iara seja tão famosa por ser tão aplicada mas hoje seria aprendizado de máquina tá mas existe'}
- {'corpus_id': 8684, 'score': 0.8542129993438721, 'doc_id': 8684, 'video_id': 'XnWEJPsSf-4', 'url': 'https://www.youtube.com/embed/XnWEJPsSf-4?start=6769&end=6887', 'inicio': '01:52:49', 'fim': '01:54:47', 'texto': 'ele vem a um custo de processamento muito alto e a gente a gente começou a perceber nessa fase da da da nossa histria das nossas etapas aí da da modelagem que isso não ia ser uma coisa legal pra gente poder explorar um pouco mais aí as outras etapas então a gente acabou utilizando ROM resarch validation porque ele trabalha com um um Range menor de variação desses IP parâmetros ele não pega provavelmente né o melhor como Grid search mas ele ele busca o também os melhores parâmetros então Eh Ele trabalha com um conjunto eh de dados Eh vamos dizer assim mais embaralhado mas ao mesmo tempo não rende menor de parâmetros mas é o que a gente conseguiu utilizar para atender em tempo hábil aí pro nosso trabalho tá el Desc comportou até razoavelmente bem para tanto treino né Eh então Eh além da dessa desse tipo de Grid Search aí um pouco diferente a gente acabou também eh iterando como eu falei acima eh todos os tipos de textos que eh se comportariam melhor ou não para cada um desses modelos então a gente ainda fez uma interação para esses textos ou seja o o na base vai se comportar melhor com o texto só com a re mantendo os Stop WS ou só com Rex removendo os Stop WS ou com reex com eh o stem ou lemmatization né então nós fizemos essa interação o que realmente levou um um tempo bem maior do que o o normal né Eh em todas essas essas interações aí essa exploração de de melhores e parâmetros né Eh e melhores tipos de textos pré-processados a gente eh fez a extração aí né para armazenar as métricas eh de acurácia precisão Recall e F1 n isso aí pra gente usar no no no no futuro nós utilizamos no na nas nas etapas finais de avaliação e comparação dos modelos tá Ah daí agora vem os os modelos baseados em de learning que a gente escolheu aí o lstm eh que é um tipo aí de de rede neural recorrente né Eh assim pel pela o que pesquisamos ele se aplicaria bem a nossa tarefa aí de'}
- {'corpus_id': 8316, 'score': 0.852847695350647, 'doc_id': 8316, 'video_id': 'JsTCMb6vOF8', 'url': 'https://www.youtube.com/embed/JsTCMb6vOF8?start=6734&end=6874', 'inicio': '01:52:14', 'fim': '01:54:34', 'texto': 'exemplo ã sei lá influência do tamanho do enrama ou ver h comparar modelo sei lá um modelo eh de transformer versus um lstm bidirecional com ordem beddings assim assado então dá para dá para ser o mesmo tópico desde que as questões de pesquisa sejam sejam diferentes vários modelos de pesquisa Diferentes né é as os modelos ou as as perguntas as questões de pesquisa mesmo Olha eu quero vou fazer esse experimento aqui para descobrir se eu fizer com com lstm bidirecional e se eu fizer com com birth ã qual que é melhor quero comparar diferentes algoritmos ou eu quero comparar o tamanho da qual que é o tamanho da amostra que eu preciso para ter uma classificação boa Qual que é a influência do tamanho da amostra vou pegar ã sei lá 100 aí eu vou pegar 200 vou pegar 400 vou fazer vários testes variando o tamanho da amostra e ver como é que se comporta tô dando só chutando aqui exemplos de coisas que podem ser questões de pesquisa que ser a princípio a gente fez o supervisionado agora a gente vai incluir Um item do modelo não supervisionado mas todos esses que Eu mencionei são supervisionados qual seria o modelo não supervisionado eh Que tente descobrir o Label sem o dar sem o informar o target Mas tu vai usar clustering a gente vai ter que brincar com dataset descob possí é com duas classes não sei que isso é modelagem de tópicos assim que aí tu tu tenta ter um Label tu vai fazer a o cluster dessas embeddings né claro tem outras formas o que nós fizemos foi usar o Grid Search para avaliar o svn e o Nav Bales tá Tu já fez uma comparação ali então tu vê o resultado com uns três ou quatro tipos de hiperparâmetros uhum para achar Qual o melhor modelo para comparar os modelos saber qual era o melhor tem que analisar os teus erros'}
- {'corpus_id': 8419, 'score': 0.8525168299674988, 'doc_id': 8419, 'video_id': 'BxDAHyKSwkQ', 'url': 'https://www.youtube.com/embed/BxDAHyKSwkQ?start=4777&end=4934', 'inicio': '01:19:37', 'fim': '01:22:14', 'texto': 'interessante assim mesmo que tu não faça de tudo é vocês por exemplo podem se dividir vou dar um listar umas 10 antes e depois para poder apresentar ali como isso isso e fica útil fica bom até na apresentação que aí a gente consegue enxergar Ok o que que foi o efeito da da simplificação tá legal perfeito Então acho que tem mais uma uma mão levanta al gart tem Oi professor bom dia bom dia a gente tá trabalhando ali no no modelagem de tópicos né então Eh rodamos ali o aquele um notebook né o o o hello world digamos assim lá do do Be Topic e estamos estudando ali como é que tá funciona né cada etapa e tal então uma das ideias eh pro trabalho eu pensei em comparar né os resultados dele com algum outro notebook que já tenha sido feito com Lda por exemplo seria viável essa comparação aí sim eu acho que é uma boa comparação eh até performance porque o Lda ele é mais ã mais Custoso assim na na minha experiência o que eu tenho observado Então pode também comparar isso né os resultados vão ser diferentes mas tu a ideia seria tu tentar ver se tu reconhece os mesmos tópicos né entendi é é seria uma possibilidade outra outra dificuldade que que se apresentou é assim como como a gente não conhece né o dataset né enfim são 1300 artigos lá e eh e também não é um dataset anotado eh fica aquela a sensação assim mas o que ele traz né de resultado Será que tá correto então Eh essa dificuldade aí eu senti eu não sei se teria talvez se a gente tivesse que mudar para um dataset anotado ou talvez entender que métricas que poderíamos usar para para comparar tem uma métrica de coerência eu posso passar para vocês Garten uma uma fonte dessas métricas aqui onde tu usa o genim para calcular lá tá então ela é Ela é simples mediante tu tu ter as palavras da da que representam o tópico porque tanto o Lda como o be Topic eles nada mais te dão do que eh palavras que representam uma um'}
- {'corpus_id': 8325, 'score': 0.8523058891296387, 'doc_id': 8325, 'video_id': 'YurAJHXP9o4', 'url': 'https://www.youtube.com/embed/YurAJHXP9o4?start=106&end=245', 'inicio': '00:01:46', 'fim': '00:04:05', 'texto': 'bag of WS eh para abordagens bag of WS para reduzir a dimensionalidade tá bom professor eu fico imag eu fico imagido que para técnica é porque essa técnica do B of Wars ela já tá um pouco defasada então eu imagino que as as técnicas novas que vocês foram mostrando elas na verdade não tem necessidade da gente fazer esse tipo de coisa não é isso muitas vezes você já tem um modelo um modelo pré treinado que aí você vai transformar nos como embeds nas embeds e as embeds são os dados que a gente vai entrar para o nosso determinado modelo de de previsão eu tô tô errado ou tô entendendo corretamente essa questão eh a assim do tempo né do tempo das técnicas que n foram se desenvolvendo sim as técnicas mais atuais são baseadas em em Bedin contextuais e em Transformers principalmente acontece que as que que os modelos B of fors eles ainda têm bons resultados para várias tarefas a gente pode ver que nesse dataset das Americanas por exemplo o resultado do SV é melhor do que os resultados que a gente obteve com ah com o o lstm e as worden Bells eu acho que até melhor do que o do burt se bem que o do burt a gente treinou com menos né Igual ficou igual e ele é muito mais barato uhum ah então assim na verdade o que de uma forma geral que a gente pode fazer é de repente utilizar algumas técnicas que são mais baratas que são um pouco mais antigas Óbvio você vai ter Sei lá talvez ela seja nossa baseline e depois você usa outras técnicas mais avançadas E aí você pode comparar e Óbvio aqui se Adapt melhor os dados que você tem você vai utilizar no seu no seu processo né no seu problema isso exatamente às vezes uma técnica mais antiga e mais barata eh se dá um resultado parecido com com uma técnica que é bem mais poderosa então é é preferível até porque é porque é mais barata né mais barata no sentido bom'}
- {'corpus_id': 6704, 'score': 0.8515979051589966, 'doc_id': 6704, 'video_id': 'Ttos4Rv1rVU', 'url': 'https://www.youtube.com/embed/Ttos4Rv1rVU?start=2735&end=2856', 'inicio': '00:45:35', 'fim': '00:47:36', 'texto': 'imagem por vez né isso ele explica por daí Porque fundamentalmente a gente vai usar melhor o recurso computacional se a gente fizer mais eh imagens ao mesmo tempo tem uma perda Com certeza né do ponto de vista da descida do Gradiente porque a gente vai olhar um monte de imens juntas assim mas enfim enfim não vou entrar muitos detalhes que vou deixar vocês assistirem realmente recomendo tá por isso até botei nos slides aqui porque desmistifica bastante coisa OK então H como que a gente programa né então a gente vai usar o keras para programar e o keras é é um site independente estão vendo aqui ó Eu até vou abrir ele pra gente dar uma olhada e é aqui tá api do keras então api do keras ela é bastante rica assim ela ela basicamente tem todos os tipos de layers possíveis que vocês possam imaginar que realmente daí passa pra nossa responsabilidade intelectual de como construir o modelo né que vai ser Aquele modelo que vai resolver o problema né E então assim Tem a parte do modelo que seria api do modelo e depois tem api dos layers e aqui na api dos layers tem ó camadas convolucionais camadas recorrentes camadas de pulling camadas de pré-processamento de regularização enfim tem um horror Ok e isso aqui são só categorias tá pessoal depois por exemplo se tu entra em layers convolucionais daí tem Depois tem os vários tipos de layers convolucionais que possam existir né Se eu pegar os recorrentes tem os vários tipos de layers recorrentes que a gente pode pregar Ok E aí bom se a gente pegar um exemplo aqui vou pegar o lstm aqui ó o lstm daí tá aqui ó como que funciona o lstm e depois todos os milhões de parâmetros que a gente tem disponível para fazer com que o lstm fique do jeito que a gente precisa ok então pensa isso a gente vai estar configurando somente uma das camadas Depois tem uma parte de callbacks que'}
- {'corpus_id': 8685, 'score': 0.8509162068367004, 'doc_id': 8685, 'video_id': 'XnWEJPsSf-4', 'url': 'https://www.youtube.com/embed/XnWEJPsSf-4?start=6854&end=6976', 'inicio': '01:54:14', 'fim': '01:56:16', 'texto': 'eh fez a extração aí né para armazenar as métricas eh de acurácia precisão Recall e F1 n isso aí pra gente usar no no no no futuro nós utilizamos no na nas nas etapas finais de avaliação e comparação dos modelos tá Ah daí agora vem os os modelos baseados em de learning que a gente escolheu aí o lstm eh que é um tipo aí de de rede neural recorrente né Eh assim pel pela o que pesquisamos ele se aplicaria bem a nossa tarefa aí de classificação tá eh então ele a gente colocou Por enquanto só uma camada aí densa de 128 neurônios e a camada de saída tamanho 14 né que é a quantidade dos nossos rótulos aí né das nossas categorias e do tipo soft Max já que a gente tá falando de um de um problema multiclasse né e uma classificação mul clássica e no caso do Bert também que é a gente utilizou o tokenizador prét treinado com com esse esse Bert base aí portugis eh e para ambos os Car esqueci de falar né O O lstm ele foi treinado com comedos prét treinadas aí com o skip S1 né Tem versões maiores mas Ass assim como a gente vai demonstrar logo na na frente aqui é eh o processamento foi foi um um dos nossos calos né ele foi bem bem pesado bem carregado né e e a gente acabou utilizando versões um pouco mais eh simplificadas né da desses bedes pré-treinos E no caso aí do Bert do tokenizador prét treinado também tá eh pros modelos eh que a gente utilizou pros dois modelos aí de rip lâ né a gente utilizou o Lab en coder né a gente precisou fazer a a transformação aí da dos rótulos né né para para numéricos para poder eh continuar com o treino e e e a gente utilizou textos um pouquinho diferentes lá dos machine learning né a gente treinou reex com mantendo seus copy Worlds tá e não utilizamos nenhum lization nemum stem a gente entendeu que não não faz muito sentido Tá mas eh ambos foram utilizados com só com reex e e mantendo as Stop WS a gente também não'}
- {'corpus_id': 2022, 'score': 0.8508728742599487, 'doc_id': 2022, 'video_id': 'Tf2zR245aZ0', 'url': 'https://www.youtube.com/embed/Tf2zR245aZ0?start=3535&end=3667', 'inicio': '00:58:55', 'fim': '01:01:07', 'texto': 'levantando a mão ali pode falar Oi pode falar para a gente é para a gente entender porque os algoritmos mesmo lançavam a gente não precisa fazer esse tipo de comparação gera tudo e tudo mais não é isso eu trato os gastos para ele quais são os algoritmos que ele vai ele vai utilizar mas precisa fazer isso individualmente pegar cada um fazer o carro americano uma média sei lá do dos Não não precisa Isso é um bom eu entendi é um bom Ponto Isso aqui é realmente para vocês entenderem né por baixo da teoria o que que tá acontecendo Então essa ideia agora eu vou falar uma coisa que é interessante vocês verem quando vocês têm uma samba e a gente vai ver depois como como fazer isso é interessante fazer esse tipo de análise como é que a distribuição de desempenho né para todos os modelos básicos os modelos básicos modelos individuais e como é que fica o modelo o desempenho do lançambam em relação a esses individuais Isso é uma análise que eu normalmente acho que é útil fazer para ver se de fato a gente está conseguindo gerar uma robustez né nessa predição no sentido do ano samba ou realmente se sair melhor do que a maioria dos modelos se o meu anbum fica aqui por exemplo aí não tem muita utilidade digamos assim né então isso seria uma questão porque claro isso aqui representa 501 modelos tá então claro que teve um modelo que chegou em 96 né mas a grande maioria se concentra aqui entre 88 e 92 é abaixo daquele desempenho Então isso é interessante vocês observarem acho que isso acho que isso seria legal quando você trabalha tá o Alexandre tem uma pergunta pode falar eu acho que essa já meio que respondeu mas ainda é acho que queria uma explicação eu tenho 81 modelos eu estou vendo ali'}

**Resposta gerada pelo LLM**: O texto compara modelos LSTM com redes recorrentes tradicionais, afirmando que o LSTM possui uma célula de memória que funciona melhor.  Isso se deve a portas ajustáveis (esquecimento, entrada e saída) com pesos treináveis que permitem ao LSTM saber quando esquecer ou aceitar elementos da sequência, usando memória prévia de forma mais eficiente.  Redes recorrentes tradicionais não possuem essa capacidade de controle da memória.

**Referências**:
- [Vídeo de referência](https://www.youtube.com/embed/YRg33lhmAnQ?start=420&end=561)
- [Vídeo de referência](https://www.youtube.com/embed/YRg33lhmAnQ?start=530&end=666)

**Anotação manual**: P

**Answer Relevance (AR)**: 0.000000

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. Supporting Evidence: The RESPONSE discusses LSTM models but does not address ARIMA or the differences between traditional time series models and machine learning models as requested in the PROMPT. It focuses solely on the characteristics of LSTM without comparing it to ARIMA or providing a comprehensive overview of both types of models. Therefore, it does not fulfill the requirements of the PROMPT.  Score: 0

**Bônus: Raciocínio (CR CoT)**:

- *Contexto:* precisa preocupar que a gente tem que entender né que LS tem a mesma coisa né uma célula de memória que funciona melhor do que uma rede recorrente tradicional e por que que ela funciona melhor ó ela tem um monte de elementos aqui mas o que que é por enquanto a gente não vai entrar nessa aqui dentro não mas ela tem algumas portas ou que são ajustáveis é uma porta de esquecimento uma porta de entrada e uma porta de saída Tá e isso então que é as coisas que a gente consegue isso aqui ó tem pesos aqui dentro tá essas portas aqui tem tem pesos que pode ser treinados E aí então ele é stm bem treinada ela vai saber quando que ela tem que esquecer elementos da sequência e quando que ela tem que aceitar elementos da entrada né questão da memória aqui né quando que eu tenho que usar memória que eu já tenho e quando que isso não é mais importante então ela ajusta ou até amplifica né O que que já já veio antes e aqui também né para calcular a saída dela tá e cada porta recebe algumas coisas né recebe algumas coisas e dá como saída né coisas para alimentar outros elementos ali esse que é a questão esse que é o talvez o básico da Leste m né é as portas aqui ó e cada porta dá né processo tem alguma finalidade recebe uma entrada né e dar como saída uma resposta apropriada a gente vai ver cada uma delas ó primeira coisa a porta diz esquecimento a porta desse esquecimento Recebe como entrada não só né a entrada atual ali ó na série temporal né o x naquele momento XT aqui ó fiz naquele momento a entrada o tempo né para digamos tô dizendo a Bitcoin né Qual é o dia aqui que eu quero para dizer não só isso como entrada mas também né o que veio a memória do processamento anterior né então isso aqui foi tudo que ele é time calculou até aqui então da mesma forma que a rede recorrente né
- *Score:* 0.3333333333333333
- *Justificativa:*
  - Criteria: The context should be relevant to the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context discusses LSTM (Long Short-Term Memory) networks, which are a type of machine learning model relevant to the question. However, it does not mention ARIMA or traditional time series models, which are crucial for a complete understanding of the differences being asked about. The context focuses primarily on the mechanisms of LSTM without addressing how it compares to ARIMA or other traditional models. Therefore, while it provides some relevant information about LSTM, it does not cover the entirety of the question.
  - Score: 1

- *Contexto:* assim né como se fosse mesmo uma rede compridona lá só que é a cópias da mesma rede aqui né cada momento só que quando eu vou atualizar para primeiro para edição ali eu não consigo é a mesma ideia tá só que uma é comprida digamos conceitualmente mesmo né comprida no fisicamente né Tem muitas camadas e essa aqui é por eu processar né nas camadas dela múltiplas vezes o processamento fica Comprido né e o gradiente soma igual deu para pegar mais ou menos tá então agora eu vou falar um pouquinho da lspm mas aqui também a gente pode pensar na stm como uma caixa preta tá uma rede recorrente né O que que a gente precisa levar embora né da aula de hoje lsm é uma rede recorrente melhor melhor que que o modelo tradicional é melhor que o modelo tradicional o modelo tradicional por isso que esses neurônios da IBM que tem a memória resistiva que tá tá tão tendência para [Música] Como eu posso falar é uma das tendências né O que é que até assim ó para processamento de sequências elast m é muito bom só que em especial texto é uma sequência né só que para texto aí tem os Transformers né que são um tipo de sequência específica chamada texto né Transformers são a melhor tecnologia mas aqui nós vamos ver tá o princípio então a gente não precisa se ficar travado aqui ah não entendi essas matemática aqui dessas ativação Não precisa preocupar que a gente tem que entender né que LS tem a mesma coisa né uma célula de memória que funciona melhor do que uma rede recorrente tradicional e por que que ela funciona melhor ó ela tem um monte de elementos aqui mas o que que é por enquanto a gente não vai entrar nessa aqui dentro não mas ela tem algumas portas ou que são ajustáveis é uma porta de esquecimento uma porta de entrada
- *Score:* 0.6666666666666666
- *Justificativa:*
  - Criteria: The context must be relevant and helpful for answering the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context discusses LSTM as a type of recurrent neural network and mentions its advantages over traditional models, which aligns with the question about differences between ARIMA and LSTM. However, the context does not provide specific information about ARIMA or traditional time series models, which limits its relevance. It primarily focuses on LSTM without contrasting it with ARIMA, thus failing to address the entirety of the question.
  - Score: 2

- *Contexto:* processo de treinamento dos modelos tá outra forma por exemplo se a gente sabe que quando a gente trabalha com probabilidade a gente sabe né alguns Eventos São mais raros então a gente ajusta as probabilidades Com base no conhecimento prévio não Com base no conhecimento dos dados porque às vezes a coleta de dados pode ter alguns artefatos amostrais Enfim então existem sim formas de fazer isso aí e são diversas assim tá mas acho que isso é um ponto importante sim o deles tem uma pergunta pode falar deles né dessa pergunta aqui que foi feita e fiquei com uma curiosidade a partir de tudo que foi falado aí a simbólica tradicional aquela baseada além prologista aquelas coisas de antigamente a gente pode dizer que elas estão em desuso São arcaicas ou ainda Elas têm alguma utilidade no dia de hoje Não elas têm Elas têm utilidade o aprendizado de máquina ele é hoje uma sub área da yá E aí tem todas as outras áreas da Iah que lidam com uma forma mais clássica de ser Tá então não tá não quer dizer que esteja em completo ou seja inútil digamos assim certo mas a gente só tá vai dar ênfase nessa nessa questão desse paradigma que é algo que eu diria assim hoje um dos grandes motivos da ia ser tão famosa como é é o aprendizado de máquina e às vezes a pessoa nem sabem que é aprendizado de máquina nem sabe que aquilo é o termo aprendizado de máquina acham que é claro como eu falei ai é uma área de conhecimento muito grande que tem várias subiárias Então o que permitiu toda essa essa fama essa né enfim daí é justamente a capacidade de análise que o aprendizado de máquina trouxe e que existem outras obviamente outras evoluções Talvez daqui alguns anos a gente veja outras sub áreas de ar fazendo com que a Iara seja tão famosa por ser tão aplicada mas hoje seria aprendizado de máquina tá mas existe
- *Score:* 0.3333333333333333
- *Justificativa:*
  - Criteria: The context must be relevant and helpful for answering the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context provided discusses machine learning and its applications but does not specifically address the differences between ARIMA and LSTM models. It mentions the importance of machine learning and its various subfields, but it lacks direct information about the characteristics, methodologies, or comparisons of ARIMA and LSTM. Therefore, while it touches on machine learning, it does not provide relevant information to answer the question about the specific differences between the two types of models.
  - Score: 1

- *Contexto:* ele vem a um custo de processamento muito alto e a gente a gente começou a perceber nessa fase da da da nossa histria das nossas etapas aí da da modelagem que isso não ia ser uma coisa legal pra gente poder explorar um pouco mais aí as outras etapas então a gente acabou utilizando ROM resarch validation porque ele trabalha com um um Range menor de variação desses IP parâmetros ele não pega provavelmente né o melhor como Grid search mas ele ele busca o também os melhores parâmetros então Eh Ele trabalha com um conjunto eh de dados Eh vamos dizer assim mais embaralhado mas ao mesmo tempo não rende menor de parâmetros mas é o que a gente conseguiu utilizar para atender em tempo hábil aí pro nosso trabalho tá el Desc comportou até razoavelmente bem para tanto treino né Eh então Eh além da dessa desse tipo de Grid Search aí um pouco diferente a gente acabou também eh iterando como eu falei acima eh todos os tipos de textos que eh se comportariam melhor ou não para cada um desses modelos então a gente ainda fez uma interação para esses textos ou seja o o na base vai se comportar melhor com o texto só com a re mantendo os Stop WS ou só com Rex removendo os Stop WS ou com reex com eh o stem ou lemmatization né então nós fizemos essa interação o que realmente levou um um tempo bem maior do que o o normal né Eh em todas essas essas interações aí essa exploração de de melhores e parâmetros né Eh e melhores tipos de textos pré-processados a gente eh fez a extração aí né para armazenar as métricas eh de acurácia precisão Recall e F1 n isso aí pra gente usar no no no no futuro nós utilizamos no na nas nas etapas finais de avaliação e comparação dos modelos tá Ah daí agora vem os os modelos baseados em de learning que a gente escolheu aí o lstm eh que é um tipo aí de de rede neural recorrente né Eh assim pel pela o que pesquisamos ele se aplicaria bem a nossa tarefa aí de
- *Score:* 0.3333333333333333
- *Justificativa:*
  - Criteria: The context should be relevant to the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context discusses various modeling techniques and mentions LSTM, which is relevant to the question. However, it does not provide a clear comparison with ARIMA or detail the differences between traditional time series models and machine learning models. The focus is more on the process of model selection and parameter tuning rather than the specific characteristics or differences of the models in question.
  - Score: 1

- *Contexto:* exemplo ã sei lá influência do tamanho do enrama ou ver h comparar modelo sei lá um modelo eh de transformer versus um lstm bidirecional com ordem beddings assim assado então dá para dá para ser o mesmo tópico desde que as questões de pesquisa sejam sejam diferentes vários modelos de pesquisa Diferentes né é as os modelos ou as as perguntas as questões de pesquisa mesmo Olha eu quero vou fazer esse experimento aqui para descobrir se eu fizer com com lstm bidirecional e se eu fizer com com birth ã qual que é melhor quero comparar diferentes algoritmos ou eu quero comparar o tamanho da qual que é o tamanho da amostra que eu preciso para ter uma classificação boa Qual que é a influência do tamanho da amostra vou pegar ã sei lá 100 aí eu vou pegar 200 vou pegar 400 vou fazer vários testes variando o tamanho da amostra e ver como é que se comporta tô dando só chutando aqui exemplos de coisas que podem ser questões de pesquisa que ser a princípio a gente fez o supervisionado agora a gente vai incluir Um item do modelo não supervisionado mas todos esses que Eu mencionei são supervisionados qual seria o modelo não supervisionado eh Que tente descobrir o Label sem o dar sem o informar o target Mas tu vai usar clustering a gente vai ter que brincar com dataset descob possí é com duas classes não sei que isso é modelagem de tópicos assim que aí tu tu tenta ter um Label tu vai fazer a o cluster dessas embeddings né claro tem outras formas o que nós fizemos foi usar o Grid Search para avaliar o svn e o Nav Bales tá Tu já fez uma comparação ali então tu vê o resultado com uns três ou quatro tipos de hiperparâmetros uhum para achar Qual o melhor modelo para comparar os modelos saber qual era o melhor tem que analisar os teus erros
- *Score:* 0.3333333333333333
- *Justificativa:*
  - Criteria: The context must be relevant and helpful for answering the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context provided discusses various machine learning models and their comparisons, but it does not specifically address the differences between ARIMA and LSTM. It mentions LSTM in passing but does not explain how it differs from ARIMA or traditional time series models. The focus is more on general model comparisons and research questions rather than the specific characteristics or methodologies of ARIMA versus LSTM.
  - Score: 1

- *Contexto:* interessante assim mesmo que tu não faça de tudo é vocês por exemplo podem se dividir vou dar um listar umas 10 antes e depois para poder apresentar ali como isso isso e fica útil fica bom até na apresentação que aí a gente consegue enxergar Ok o que que foi o efeito da da simplificação tá legal perfeito Então acho que tem mais uma uma mão levanta al gart tem Oi professor bom dia bom dia a gente tá trabalhando ali no no modelagem de tópicos né então Eh rodamos ali o aquele um notebook né o o o hello world digamos assim lá do do Be Topic e estamos estudando ali como é que tá funciona né cada etapa e tal então uma das ideias eh pro trabalho eu pensei em comparar né os resultados dele com algum outro notebook que já tenha sido feito com Lda por exemplo seria viável essa comparação aí sim eu acho que é uma boa comparação eh até performance porque o Lda ele é mais ã mais Custoso assim na na minha experiência o que eu tenho observado Então pode também comparar isso né os resultados vão ser diferentes mas tu a ideia seria tu tentar ver se tu reconhece os mesmos tópicos né entendi é é seria uma possibilidade outra outra dificuldade que que se apresentou é assim como como a gente não conhece né o dataset né enfim são 1300 artigos lá e eh e também não é um dataset anotado eh fica aquela a sensação assim mas o que ele traz né de resultado Será que tá correto então Eh essa dificuldade aí eu senti eu não sei se teria talvez se a gente tivesse que mudar para um dataset anotado ou talvez entender que métricas que poderíamos usar para para comparar tem uma métrica de coerência eu posso passar para vocês Garten uma uma fonte dessas métricas aqui onde tu usa o genim para calcular lá tá então ela é Ela é simples mediante tu tu ter as palavras da da que representam o tópico porque tanto o Lda como o be Topic eles nada mais te dão do que eh palavras que representam uma um
- *Score:* 0.0
- *Justificativa:*
  - Criteria: The context should be relevant to the question about the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The provided context discusses topics related to modeling and comparisons between different methodologies, but it does not address time series models or the specific differences between ARIMA and LSTM. The context is focused on topic modeling and performance comparisons, which are unrelated to the question about time series analysis. Therefore, it does not provide relevant information to answer the question.
  - Score: 0

- *Contexto:* bag of WS eh para abordagens bag of WS para reduzir a dimensionalidade tá bom professor eu fico imag eu fico imaginando que para técnica é porque essa técnica do B of Wars ela já tá um pouco defasada então eu imagino que as as técnicas novas que vocês foram mostrando elas na verdade não tem necessidade da gente fazer esse tipo de coisa não é isso muitas vezes você já tem um modelo um modelo pré treinado que aí você vai transformar nos como embeds nas embeds e as embeds são os dados que a gente vai entrar para o nosso determinado modelo de de previsão eu tô tô errado ou tô entendendo corretamente essa questão eh a assim do tempo né do tempo das técnicas que n foram se desenvolvendo sim as técnicas mais atuais são baseadas em em Bedin contextuais e em Transformers principalmente acontece que as que que os modelos B of fors eles ainda têm bons resultados para várias tarefas a gente pode ver que nesse dataset das Americanas por exemplo o resultado do SV é melhor do que os resultados que a gente obteve com ah com o o lstm e as worden Bells eu acho que até melhor do que o do burt se bem que o do burt a gente treinou com menos né Igual ficou igual e ele é muito mais barato uhum ah então assim na verdade o que de uma forma geral que a gente pode fazer é de repente utilizar algumas técnicas que são mais baratas que são um pouco mais antigas Óbvio você vai ter Sei lá talvez ela seja nossa baseline e depois você usa outras técnicas mais avançadas E aí você pode comparar e Óbvio aqui se Adapt melhor os dados que você tem você vai utilizar no seu no seu processo né no seu problema isso exatamente às vezes uma técnica mais antiga e mais barata eh se dá um resultado parecido com com uma técnica que é bem mais poderosa então é é preferível até porque é porque é mais barata né mais barata no sentido bom
- *Score:* 0.0
- *Justificativa:*
  - Criteria: The context must be relevant and helpful for answering the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context provided discusses various modeling techniques, including older methods and newer approaches, but it does not specifically address the differences between ARIMA and LSTM. It mentions techniques like "bag of words" and "embeddings," which are not directly related to the question about time series models. The context lacks specific information about ARIMA, LSTM, or their comparative advantages and disadvantages, which are crucial for fully answering the question.
  - Score: 0

- *Contexto:* imagem por vez né isso ele explica por daí Porque fundamentalmente a gente vai usar melhor o recurso computacional se a gente fizer mais eh imagens ao mesmo tempo tem uma perda Com certeza né do ponto de vista da descida do Gradiente porque a gente vai olhar um monte de imens juntas assim mas enfim enfim não vou entrar muitos detalhes que vou deixar vocês assistirem realmente recomendo tá por isso até botei nos slides aqui porque desmistifica bastante coisa OK então H como que a gente programa né então a gente vai usar o keras para programar e o keras é é um site independente estão vendo aqui ó Eu até vou abrir ele pra gente dar uma olhada e é aqui tá api do keras então api do keras ela é bastante rica assim ela ela basicamente tem todos os tipos de layers possíveis que vocês possam imaginar que realmente daí passa pra nossa responsabilidade intelectual de como construir o modelo né que vai ser Aquele modelo que vai resolver o problema né E então assim Tem a parte do modelo que seria api do modelo e depois tem api dos layers e aqui na api dos layers tem ó camadas convolucionais camadas recorrentes camadas de pulling camadas de pré-processamento de regularização enfim tem um horror Ok e isso aqui são só categorias tá pessoal depois por exemplo se tu entra em layers convolucionais daí tem Depois tem os vários tipos de layers convolucionais que possam existir né Se eu pegar os recorrentes tem os vários tipos de layers recorrentes que a gente pode pregar Ok E aí bom se a gente pegar um exemplo aqui vou pegar o lstm aqui ó o lstm daí tá aqui ó como que funciona o lstm e depois todos os milhões de parâmetros que a gente tem disponível para fazer com que o lstm fique do jeito que a gente precisa ok então pensa isso a gente vai estar configurando somente uma das camadas Depois tem uma parte de callbacks que
- *Score:* 0.3333333333333333
- *Justificativa:*
  - Criteria: The context must be relevant and helpful for answering the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context provided discusses LSTM and its implementation using Keras, which is relevant to machine learning models. However, it does not mention ARIMA or traditional time series models, nor does it provide a comparison between the two types of models. Therefore, while it touches on LSTM, it fails to address the entirety of the question regarding the differences between ARIMA and LSTM.
  - Score: 1

- *Contexto:* eh fez a extração aí né para armazenar as métricas eh de acurácia precisão Recall e F1 n isso aí pra gente usar no no no no futuro nós utilizamos no na nas nas etapas finais de avaliação e comparação dos modelos tá Ah daí agora vem os os modelos baseados em de learning que a gente escolheu aí o lstm eh que é um tipo aí de de rede neural recorrente né Eh assim pel pela o que pesquisamos ele se aplicaria bem a nossa tarefa aí de classificação tá eh então ele a gente colocou Por enquanto só uma camada aí densa de 128 neurônios e a camada de saída tamanho 14 né que é a quantidade dos nossos rótulos aí né das nossas categorias e do tipo soft Max já que a gente tá falando de um de um problema multiclasse né e uma classificação mul clássica e no caso do Bert também que é a gente utilizou o tokenizador prét treinado com com esse esse Bert base aí portugis eh e para ambos os Car esqueci de falar né O O lstm ele foi treinado com comedos prét treinadas aí com o skip S1 né Tem versões maiores mas Ass assim como a gente vai demonstrar logo na na frente aqui é eh o processamento foi foi um um dos nossos calos né ele foi bem bem pesado bem carregado né e e a gente acabou utilizando versões um pouco mais eh simplificadas né da desses bedes pré-treinos E no caso aí do Bert do tokenizador prét treinado também tá eh pros modelos eh que a gente utilizou pros dois modelos aí de rip lâ né a gente utilizou o Lab en coder né a gente precisou fazer a a transformação aí da dos rótulos né né para para numéricos para poder eh continuar com o treino e e e a gente utilizou textos um pouquinho diferentes lá dos machine learning né a gente treinou reex com mantendo seus copy Worlds tá e não utilizamos nenhum lization nemum stem a gente entendeu que não não faz muito sentido Tá mas eh ambos foram utilizados com só com reex e e mantendo as Stop WS a gente também não
- *Score:* 0.3333333333333333
- *Justificativa:*
  - Criteria: The context should be relevant to the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context discusses LSTM as a type of machine learning model and mentions its application in classification tasks. However, it does not provide any information about traditional time series models like ARIMA or compare them directly with LSTM. The context is focused on the implementation details of LSTM and other machine learning techniques without addressing the question about the differences between ARIMA and LSTM.
  - Score: 1

- *Contexto:* levantando a mão ali pode falar Oi pode falar para a gente é para a gente entender porque os algoritmos mesmo lançavam a gente não precisa fazer esse tipo de comparação gera tudo e tudo mais não é isso eu trato os gastos para ele quais são os algoritmos que ele vai ele vai utilizar mas precisa fazer isso individualmente pegar cada um fazer o carro americano uma média sei lá do dos Não não precisa Isso é um bom eu entendi é um bom Ponto Isso aqui é realmente para vocês entenderem né por baixo da teoria o que que tá acontecendo Então essa ideia agora eu vou falar uma coisa que é interessante vocês verem quando vocês têm uma samba e a gente vai ver depois como como fazer isso é interessante fazer esse tipo de análise como é que a distribuição de desempenho né para todos os modelos básicos os modelos básicos modelos individuais e como é que fica o modelo o desempenho do lançambam em relação a esses individuais Isso é uma análise que eu normalmente acho que é útil fazer para ver se de fato a gente está conseguindo gerar uma robustez né nessa predição no sentido do ano samba ou realmente se sair melhor do que a maioria dos modelos se o meu anbum fica aqui por exemplo aí não tem muita utilidade digamos assim né então isso seria uma questão porque claro isso aqui representa 501 modelos tá então claro que teve um modelo que chegou em 96 né mas a grande maioria se concentra aqui entre 88 e 92 é abaixo daquele desempenho Então isso é interessante vocês observarem acho que isso acho que isso seria legal quando você trabalha tá o Alexandre tem uma pergunta pode falar eu acho que essa já meio que respondeu mas ainda é acho que queria uma explicação eu tenho 81 modelos eu estou vendo ali
- *Score:* 0.0
- *Justificativa:*
  - Criteria: The context must be relevant and helpful for answering the entire question regarding the differences between traditional time series models like ARIMA and machine learning models like LSTM.
  - Supporting Evidence: The context provided does not address the question about the differences between ARIMA and LSTM models. It discusses general analysis and performance of various models but does not specifically mention ARIMA, LSTM, or their differences. Therefore, it fails to provide relevant information that would help in answering the question.
  - Score: 0

**Bônus: Raciocínio (GD CoT)**:

- *Score:* 0.8888888888888888
- *Justificativa:*
  - STATEMENT 0:
  - Criteria: O texto compara modelos LSTM com redes recorrentes tradicionais, afirmando que o LSTM possui uma célula de memória que funciona melhor.
  - Supporting Evidence: A fonte menciona que "LSM é uma rede recorrente melhor que o modelo tradicional" e que "uma célula de memória que funciona melhor do que uma rede recorrente tradicional". Isso indica uma comparação direta entre LSTM e redes recorrentes tradicionais, destacando a superioridade do LSTM em termos de funcionamento da célula de memória.
  - Score: 1.0
  - STATEMENT 1:
  - Criteria: Isso se deve a portas ajustáveis (esquecimento, entrada e saída) com pesos treináveis que permitem ao LSTM saber quando esquecer ou aceitar elementos da sequência, usando memória prévia de forma mais eficiente.
  - Supporting Evidence: A fonte menciona que o LSTM possui "portas ou que são ajustáveis é uma porta de esquecimento uma porta de entrada e uma porta de saída" e que "ela vai saber quando que ela tem que esquecer elementos da sequência e quando que ela tem que aceitar elementos da entrada", o que confirma que as portas ajustáveis e os pesos treináveis são fundamentais para o funcionamento do LSTM em relação ao uso eficiente da memória.
  - Score: 1.0
  - STATEMENT 2:
  - Criteria: Redes recorrentes tradicionais não possuem essa capacidade de controle da memória.
  - Supporting Evidence: A fonte menciona que "LSM é uma rede recorrente melhor que o modelo tradicional" e discute como a LSTM possui "uma célula de memória que funciona melhor do que uma rede recorrente tradicional", implicando que as redes recorrentes tradicionais não têm a mesma capacidade de controle da memória que as LSTM. Portanto, a afirmação é apoiada pela comparação feita na fonte.
  - Score: 0.6666666666666666
---
