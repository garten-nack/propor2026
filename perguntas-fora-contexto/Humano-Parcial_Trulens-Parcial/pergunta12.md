**Pergunta 12**: Quais são as vantagens e desvantagens de utilizar modelos baseados em deep learning, como redes neurais recorrentes, para séries temporais em comparação com abordagens baseadas em regressão tradicional?

**Segmentos recuperados pelo E5**:

- *Corpus ID:* 8268
- *Score:* 0.8621248602867126
- *URL:* oculto
- *Início:* 00:22:15
- *Fim:* 00:24:50
- *Transcrição:* muito e a gente vai ver depois Outro fator é que esses modelos estão disponíveis na rug face e a gente consegue facilmente usar e adaptar e o o modelo mais conhecido mais influente é o burt cujo cujo a sigla né vem da da do título do artigo que é by directional Encoder representations from Transformers e então é um artigo de 2018 Ah esse número tá desatualizado ele tem mais de 90.000 citações da da última vez que eu vi Acho que foi semana passada é é um número absurdo de de citações eh e esses aqui são foram os proponentes então ele foi são hã empregados da da Google né funcionários lá da Google só que o birt ele é aqui é o pico da montanha dessa montanha aqui eh tem esse e e o nome vem do de um personagem do dos muets virou meio que uma assim coisa de nerde um colocou um personagem dos Muppets o outro colocou também E aí foi foi indo agora esgotaram os personagens eles estão indo buscar em outros lugares tem o Bart por exemplo bom mas então Eh esse tutorial aqui que tem no no YouTube ele vai explicando foi dali que eu peguei essa essa imagem e ele vai explicando o que que são cada um desses componentes que que meio que embasaram a criação do do modelo birth a gente vai passar rapidamente por eles então primeiro componente aqui são as redes neurais recorrentes o primeiro assim building Block desse ah desse grande modelo as redes neurais recorrentes elas são eficazes no processamento de informações sequenciais ou seja texto então o que que é uma rede neural recorrente é qualquer rede que tenha um ciclo nas suas conexões esse ciclo ele ele significa que o valor da unidade de corrente ele ele vem eh ele alimenta a próxima a a próxima unidade né então qualquer rede que o valor de uma unidade seja diretamente ou indiretamente dependente da saída do que

- *Corpus ID:* 8254
- *Score:* 0.8618961572647095
- *URL:* oculto
- *Início:* 01:22:56
- *Fim:* 01:25:37
- *Transcrição:* mais complexas porque aqui essas essas redes neurais recorrentes por exemplo elas já tiveram uma boa performance para tradução coisas ali que no tfidf nem nem sei Professor como é Faria Então essas essas essas redes elas elas performam melhor nessa mesmo nessas tarefas mais simples como classificação quando o dataset é difícil mas principalmente em tarefas mais complexas que não é simplesmente uma classificação do de uma Instância né Tá certo obrigado Mais alguma pergunta eu tô na dúvida se a gente já se a gente faz o intervalo ou se a gente fala da tokenização em subpav ras que é um pedaço curtinho acho que já é três pras 10 né Não sei então a gente faz o intervalo um pouquinho mais cedo hoje então Voltamos às eh 10:17 10:18 Professor Deixa eu só fazer uma pergunta eh diga eu fiz uma pergunta no fum na verdade foi uma curiosidade que eu fiz no meu trabalho ahi lá a eu eu cheguei um pouquinho atrasado na aula tive um uma problema aqui em casa e eu não sei se foi falado não a gente não comentou a gente deixa pro pro final eu queria entender melhor o que que tu o que que tu calculou ali porque o parâmetro que a gente estima lá na na na distribuição do do zif é o que dá inclinação da da reta e da ele é um só tá mas aí eu te eu posso te posso passar o link aqui da da Wikipedia que tem que tem uma explicação mais mais detalhada e eh mas a gente fala depois no no final da aula tá bom e também em relação ao trabalho final né sobre as escolhas dos dos temas de dataset lá das áreas isso a gente vai falar também eu já eu dei uma olhadinha agora na planilha eu vi que já quase todos já Já escolheram mas a gente fala disso no final tá obrigado Professor até daqui a pouquinho então certo par grafar né Por causa do Ah verdade

- *Corpus ID:* 7732
- *Score:* 0.8615602254867554
- *URL:* oculto
- *Início:* 00:04:00
- *Fim:* 00:06:23
- *Transcrição:* dataset que a gente tem para treinar tá a gente tem que levar em consideração e o o custo computacional que a gente quer a gente tem problema de custo ou não tem né basicamente posso assumir que eu tenho máquina infinita ou não tá E aí isso obviamente que impacta o a escolha de vocês e isso que guia o desenvolvimento de novos back Bon tá então tem algumas lá shufflenet é uma outra rede compacta tem uma outra um outro backbone que é muito usado para detecção de objetos tem uma versão V2 deficient net tá e tem uma rede que é connex tá que curiosamente o ano passado saiu um artigo comparando vários back Bon tá e um dos findings é que essa connex é um um dos uma dos back que atinge melhor resultado bom eh aqui vai ficar só um Pointer para vocês tá eh provavelmente vocês já ouviram falar em Transformers ainda a a disciplina acho que da Viviane vem logo depois tá E aí certamente vocês vão ver isso aí todo mundo já ouviu falar pelo menos esse termo Transformer não sim sabem o que que é é um filme né o robozinho que muda lá o carro também ém também é horrível pr pra rede eu acho horrível Tá mas tudo bem Tá então assim toda a moral dessa da dos Transformers tá eh é assim ó eh talvez vão fazer o contraponto entre uma rede convolucional e uma rede baseada em Transformer convolucional A ideia é que eu tenho Kernel e o tamanho do Kern limita quem vai falar com quem né então 3 por TR significa que eu tenho um bloco de nove pixels que vai conversar entre si certo ah então essas todas as operações são locais tá a ideia do transformer é usar uma camada que não é local potencialmente todo mundo conversa com todo mundo tá então A ideia é que todo mundo vai conversar com todo mundo em tese o

- *Corpus ID:* 6687
- *Score:* 0.861181914806366
- *URL:* oculto
- *Início:* 00:17:49
- *Fim:* 00:19:55
- *Transcrição:* naqueles dados pra gente poder criar esse melhor modelo possível né ã a gente não precisa usar também a prisada fundo quando os métodos tradicionais eh funcionam bem e aí aqui eh eu coloquei métodos tradicionais de aidade máquina mas a gente poderia tornar isso aqui mais amplo onde os métodos estatísticos já funcionam bem né então por exemplo se eu quero fazer uma regressão linear esse tipo de coisa tem métodos que já funcionam super bem e que já geram um um feit eh suficientemente adequado né E que a gente não precisa daí empregar precisar profunda né até a gente brinca de vez em quando a a febre tá tão grande em relação a esse assunto né o Hype o pessoal fala né que por vezes o pessoal tem usado Prado Prof fundo em meso em situações assim que que realmente claramente não precisa mas é mais assim para Ok estou usando aprendizado profunda eh Tem situações também onde os dados estão bem estruturados onde eu tenho bastante conhecimento dos dados já estão limpos esse tipo de de coisa então daí não precisa usar também né Porque existe uma estrutura já bem clara né E tem uma outra situação que é quando não usar é quando o modelo que vai ser criado ele precisa ser explicável né então por exemplo eu crio um modelo isso tem muita gente trabalhando né quando eu crio um modelo de aprendizado profundo né que é uma rede neural dificilmente eu consigo explicar os valores dela uma vez ela tenha sido treinada né Tem gente trabalhando nisso né explicar assim ah como por que que ela funciona né Por que que essa rede neural consegue identificar os objetos ou classificar os objetos por né a pessoa tenta olhar visualizar os valores dessa rede neural muito difícil explicar né então nessas situações não se a gente precisa que o modelo seja explicável né daí a gente não vale a pena usar PR profundo porque isso é uma coisa que tá ninguém sabe

- *Corpus ID:* 6794
- *Score:* 0.8603287935256958
- *URL:* oculto
- *Início:* 00:20:57
- *Fim:* 00:22:51
- *Transcrição:* vamos seg seguir falando sobre isso ok então ã só para relembrar o fluxo habitual né do modelo sequencial lá de treinamento de redes Profundas a gente Define um modelo esse modelo vai ser um modelo sequencial porque envolve camadas né Essas camadas são sequenciais a gente vai configurar o modelo com compile E aí depois a gente vai fazer o treinamento com Fit e essa aqui é a parte que em geral vai demorar Ok dependendo da nossa escolha de betat size esse tipo de coisa né então H como essa parte demora em geral e ela pode demorar né não sei se vocês chegaram a olhar as estatísticas né até posso tentar recuperar aí uma palestra que eu assisti no primeiro semestre de um pessoal da parte de llms né comentando quanto tempo levou para treinar GPT 4 gpt3 esse tipo de coisa né mas é na ordem de algumas de algum de vários dias assim até posso dizer até acho que algumas semanas para os primeiros casos né usando muito poder computacional né então assim como é um negócio que demora né tu quer poder acompanhar isso aí né tipo assim tu não quer poder quer lançar e esquecer né tipo assim tu tem que ver como é que tá indo as coisas afinal de contas Tu quer testar um monte de possibilidades né de hiperparâmetros e tudo mais né então para isso existem essa essa ideia de callbacks que torna esse acompanhamento possível e praticamente todos esses ambientes de treinamento né essas ferramentas de treinamento de redes Profundas têm suporte callbacks né então eh a ideia então é que quando a gente faz o a chamada Fit a gente possa passar um parâmetro chamado callbacks que vai ter uma lista ali de callbacks que a gente vai executar e esses métodos são basicamente operações de acompanhamento né que vão permitir a gente fazer diferentes coisas né então só para lembrar o que que é um callback para quem não lembra né mas é basicamente

- *Corpus ID:* 3020
- *Score:* 0.8594921231269836
- *URL:* oculto
- *Início:* 00:33:12
- *Fim:* 00:35:31
- *Transcrição:* aí ao final Eu vou dar a saída E aí eu vou conseguir ter corretamente classificado meus pontos vamos testar isso lá no playground lá daqui deixa eu adicionar uma camada oculta com dois neurônios vamos ver se vai melhorar aqui ó no outro né o custo aqui tava nem 05 vamos ver se vai melhorar aliás é um custo inicial ó tá melhorando vamos uma curvinha aqui mudando olha só o que que ele foi capaz de fazer Olha isso que superfície de decisão mais complexa né então ele meio que definiu não um mais dois pontos de corte é muito mais sofisticado nessa sigmoide é como se fosse ela alta aqui aí abaixa e aí sobe de novo não é outra cor sigmoide não tem essa característica mas ele bolou uma outra superfície que agora consegue abraçar esses pontos corretamente né então é bem mais poderoso agora né Agora sim nós estamos falando de redes neurais tá agora sim é redes neurais e agora a palavra né redes neurais Profundas se tem uma camada oculta já pode falar que é redes neurais Profundas tá porque já mudou a capacidade dela tá não precisamos mais né não tem não precisa ter a necessidade de ter dezenas de camadas né aquele negócio de redes neurais com que a gente ouve falar aí né Com centenas de camadas só só aquilo que a região era profunda não tá o poder já é o Super Poder aqui ó de poder de conseguir né se ajustar aponta os dessa forma já é o mesmo dia antes de com mais camadas ou não Tá então a partir do momento que você ganha uma camada oculta você sai do reino linear e ao sair do reino linear você né pode chamar de rede neural profunda Claro tem algumas pesquisadores mas puristas que vão defender esse ponto de vista né de que ah não tem que ter dezenas de camadas se não não aceito que chamam de profunda mas em termos de poder matemático se saiu do reino linear já pode chamar de rede profunda então para o nosso propostas aqui nós vamos adotar

- *Corpus ID:* 3973
- *Score:* 0.8590923547744751
- *URL:* oculto
- *Início:* 00:00:12
- *Fim:* 00:02:49
- *Transcrição:* voltamos a segunda parte antes de seguir adiante sobre essa questão que foi questionada antes né da do uso ou não em modelos mais recentes né em especial dos Transformers redes neurais enfim profundas eu fiz uma pesquisa rápida que tem várias situações eu vi algumas pessoas fazendo experimentos com e sem um até mostrou que o resultado numa situação específica foi semelhante ou não fez diferença usar Stop World na criação dos modelos mas são estudos feitos por pessoas que colocam essas essas redes sociais não é a gente fica sempre meio preocupado com isso eu encontrei uma discussão e até um artigo eu quero mostrar para vocês vou botar no mundo vocês poderem acompanhar depois e lerem Esse é interessante porque ele tá publicado no arcafe arcaver não é uma conferência nem um periódico mas é um repositório reconhecido na comunidade acadêmica de física de computação como repositório quente digamos assim né para resultados de pesquisa e pessoas que estão fazendo pesquisas e ainda não publicaram colocam lá e a comunidade ele é um espaço para as pessoas discutirem aquilo que tá lá em melhorar em seus artigos seu resultados Então até uns até tem considerado isso como como periódico né confiável Mas enfim o mais interessante é que esse artigo ele foi feito por pesquisadores da Microsoft tem um pelo menos cada Microsoft uma dessa Universidade aqui que eu não pesquisei onde é que é CN Eu acho que isso aqui é que é faxina Ok chineses tempestade muita coisa nessa área mas enfim o repositório que eles usaram é público tá disponível em estudo de 2019 antigo é claro que a avaliação deles foi feita especificamente aqui para ver os comportamentos né desse tipo de modelo

- *Corpus ID:* 3021
- *Score:* 0.8590060472488403
- *URL:* oculto
- *Início:* 00:35:01
- *Fim:* 00:37:21
- *Transcrição:* partir do momento que você ganha uma camada oculta você sai do reino linear e ao sair do reino linear você né pode chamar de rede neural profunda Claro tem algumas pesquisadores mas puristas que vão defender esse ponto de vista né de que ah não tem que ter dezenas de camadas se não não aceito que chamam de profunda mas em termos de poder matemático se saiu do reino linear já pode chamar de rede profunda então para o nosso propostas aqui nós vamos adotar essa convenção se tem uma camada oculta ou seja não é só um neurônio de sair daqui mas tem né um processo neurônios intermediários aqui que fazem processamento Então já pode chamar de rede neural profunda ok pessoal Então tá visualizado aqui né o nosso poderio já de um e é uma região até simples né Tem uma camada oculta com dois neurônios e um neurônio de saída e já conseguiu esse efeito aqui né para o nosso propósitos ficou tá bem melhor né do que antes que estava errando praticamente todos os pontos ok tá aí vocês vão poder brincar aqui à vontade tá o link é esse aqui ó playground tá vai poder brincar à vontade e só né um pequeno parentes né o exercício dessa semana é implementar um neurônio linear no colab e a planilha de classificação né preencher de classificação agora com duas entradas né eu fiz para uma entrada vocês vão fazer com duas né que em vez de S é o z tá é só assim ajuste aí e aqui é mais ou menos no mesmo mesmo paradigma né do exercício anterior aí escreve a programação aqui que implementa faz um neurônio calcular corretamente para uma entrada né e o teste e aqui para regressão né para essa implementar essa função encontrar os pesos e o baias né Vocês vão meio que fazer o não precisa implementar o algoritmo tá vocês podem chutar aqui na mão mesmo e ver né Qual o peso qual baias faz o negócio funcionar aí daqui para baixo vocês

- *Corpus ID:* 6694
- *Score:* 0.8588250875473022
- *URL:* oculto
- *Início:* 00:29:17
- *Fim:* 00:31:29
- *Transcrição:* conclusão Olha só esse meu modelo é tribom muito da da da teu intelecto foi eh dedicado em escolher Quantas camadas tinha como é que é conexão entre as camadas quais são os tipos de camadas Qual é a sequência Ok então por isso que normalmente o pessoal não disponibiliza o modelo né quando esse modelo foi foi investido um tempo muito grande para conceber ele depois para treinar Claro T um tempo muito grande e paraar mas aí mas aí tipo Google da vida uma essas grandes esses grandes players eles vão ter um parque tecnológico bem grande e então eles têm eles vão ter tempo eles têm poder de processamento até encontrar de repente uma rede eles vão por exemplo sempre tá na frente de uma pessoa que pode ter até um intelecto melhor para produzir isso mas ele não tem tempo nem tem infraestrutura suficiente para gerar essa rede é né e é é bem vender alguma coisa por exemplo é tem é realmente bem complicado porque não basta tu ter boas ideias para tu construir a rede né e ah não essa minha rede neural é bem inovadora ela que tem essa essa configuração de camadas os tipos são desse se depois para tu avaliar tu vai precisar treinar ela né e treinar é um tu precisa de um parque computacional muito grande né para então acaba acontecendo realmente que quem ganha é quem tem mais recurso computacional para fazer o treinamento para testar né tipo assim para chegar numa determinada versão de rede neural né provavelmente teve muito teste antes que falhou né que foi treinado treinado depois não ficou bom E aí muda Enfim então tem isso tem toda uma área de de pesquisa sobre isso aí né bom então a gente vai usar o tensor Flow né uma biblioteca para Prado profunda ela é de código aberto desde 2015 escrita em C mais mais ela foi iniciada dentro do Google né e depois ela foi Tornada pública digamos assim pra comunidade

- *Corpus ID:* 8463
- *Score:* 0.8579090237617493
- *URL:* oculto
- *Início:* 01:13:59
- *Fim:* 01:16:21
- *Transcrição:* Teoricamente eles aprendem mais eles têm mais capacidade de identificar padrões eh complexos esse esse eh dimensionality do do modelo não é simplesmente o número a quantidade de estados de pesos Não é só isso porque ele tem cabeças de atenção então tem uma série de de coisas dentro dessa arquitetura Tá mas entendam que é um uma um vetor de 768 que vai sendo levado em cada de de uma camada para outra e que também depois nós vamos usar para isso que é AD beding que cada um desses modelos e provê tá então aqui tem mais eh tem aqui o o trabalho ó que se chama o artigo que foi publicado isso aqui foi tudo aberto então eles de fato revelaram como como treinaram explicaram então o nome era language models are un supervised multitask learners né então eles já viram a capacidade dele fazer várias tarefas aqui né E esses modelos a gente tem formas então de avaliar ele a forma mais utilizada são essas avaliações extrinsic então no momento que a gente tem um GPT treinado né como é que a gente diz que ele Ah ele para tarefa tal ele ele ele atinge um um determinado score né então o Bert por exemplo então eu vou fazer uma tarefa de classificação uma tarefa de eh geração de texto né então ele essa avaliação extrínseca ela mede o quanto a execução dessa tarefa melhora então o melhor exemplo aqui eu o do Bert nas usando classificação ou ou até a sumarização né uma tarefa bem específica na avaliação intrínseca ela já é mais difícil da gente compreender porque ela mede a qualidade de um modelo Independente de qualquer aplicação ou tarefa Então como é que se faria isso então tem uma métrica que é usada que é a perplexidade ou perplexity é usada nessa avaliação intrínseca e ela é uma métrica assim bem complexa ela tá relacionada à entropia

**Resposta gerada pelo LLM**: O contexto menciona que redes neurais recorrentes (RNNs) têm boa performance em tarefas de tradução e outras tarefas complexas onde métodos como TF-IDF não são eficazes.  Por outro lado, métodos tradicionais como regressão linear funcionam bem em situações onde os dados estão bem estruturados, limpos e o modelo precisa ser explicável, algo difícil de alcançar com redes neurais profundas.  O uso de deep learning pode ser desnecessário quando métodos tradicionais já fornecem resultados adequados.  Além disso, o treinamento de modelos de deep learning pode ser muito demorado e exigir grande poder computacional.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: P

**Answer Relevance (AR)**: 0.666667

**Anotação automatizada**: P

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address both the advantages and disadvantages of using deep learning models, specifically recurrent neural networks, for time series analysis compared to traditional regression approaches. Supporting Evidence: The RESPONSE discusses the performance of recurrent neural networks (RNNs) in complex tasks and contrasts this with traditional regression methods, highlighting their effectiveness in structured and clean data scenarios. It also mentions the challenges of deep learning, such as the need for significant computational power and the potential for unnecessary complexity when traditional methods suffice. However, while it touches on both advantages (performance in complex tasks) and disadvantages (training time, computational requirements, and lack of explainability), it does not explicitly list or compare these aspects in a structured manner as requested by the PROMPT. Therefore, it partially addresses the prompt but lacks a comprehensive comparison.  Score: 2

---