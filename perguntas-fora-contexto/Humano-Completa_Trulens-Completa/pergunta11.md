**Pergunta 11**: Quais estratégias podem ser usadas para gerenciar a coleta e a preparação de dados em projetos de ciência de dados, garantindo qualidade e consistência ao longo do processo?

**Segmentos recuperados pelo E5**:

- *Corpus ID:* 8314
- *Score:* 0.8519754409790039
- *URL:* oculto
- *Início:* 01:48:36
- *Fim:* 01:50:56
- *Transcrição:* ã para esse tipo de coleta dá para ver se se ele tem um arquiv chamado robots.txt eh e aí nesse arquivo tá tá explicado se pode ser coletado se não tem nada específico ó não é para coletar então tu pode supor que que tá ok mas a menos que ele Diga explicitamente não os esses dados não podem ser coletados para nenhum propósito etc então aí se não diz que não pode então pode tá bom mas professora essa essa coleta que a gente vai fazer vai ser considerada no trabalho ou seria mais importante a gente já pegar um dataset pronto e tocar pro resultado que que a senhora acha a coleta não é não é avaliada o foco não é o foco da disciplina né o foco da disciplina modo e o rest tá bom Exatamente é só só é só complementando a vantagem da gente fazer a vantagem não né O que a gente traz de novo é que é um dataset novo que assim não tem disponibilizado e que ele é ele nasceu em português não ele não foi traduzido ele não tem er de tradução ele não sofreu uma tradução automática que pode conter um um viés aí de né de Mas é só isso não mas eu acho que que assim revendo O que eu o o que eu falei a assim a coleta em si não mas o o produto da da coleta é uma contribuição também então Data Set coletei um dat set que eu posso disponibilizar pros colegas depois usarem Então acho que vou publicar um artigo depois disponibiliza o dat aí tu vê vê se pode antes e e E aí po tem como calcular o rótulo né se tem como calcular o rótulo fazer datap é uma coisa muito importante sabe não é desvalorizar aqui mas tu também vai ter que concluir né isso que tu tá dizendo né Professor tem que fazer Os experimentos então último assim que a senhora vai vai chegar naturalmente professora que o de baixo também tem análise de sentimento alum problema mas tem que ser outro data tem

- *Corpus ID:* 2170
- *Score:* 0.8516871333122253
- *URL:* oculto
- *Início:* 00:12:17
- *Fim:* 00:14:18
- *Transcrição:* que como eu falei ela não necessariamente é linear ela começa a linear mas muitas vezes a gente tem que revisar etapas anteriores voltar né E essas diferentes decisões opções de métodos estratégias isso junto né O que a gente prega é a nossa metodologia Então quando vocês forem escrever um desenvolvimento do modelo vocês vão iniciar descrevendo inclusive decisões de pré-processamento de dados porque faz parte da metodologia de trabalho de vocês Né desde essa etapa de coleta preparação né e um pré-processamento específico então assim essa sequência de decisões que a gente toma né E todos esses pontos em que a gente tem que tomar realmente decisões porque existem diferentes possibilidades isso faz a nossa metodologia claro que a gente normalmente tem um conjunto de aspectos que a gente precisa prestar atenção e dependendo de um problema Alguns alguns aspectos eu vou levar em conta outros eu vou ver que não preciso certo por exemplo você não tem o valores faltantes eu não preciso pensar numa estratégia de como que eu vou lidar com essa situação mas é de uma forma geral né os pontos que a gente é tem que prestar atenção né são é um grande conjunto de opções e a gente conforme né caso a caso a gente vai discutindo e decidindo como abordar cada uma delas dependendo das demandas do projeto dos dados tá então isso é a metodologia então aqui fica claro a ideia da disciplina a ideia da disciplina é a gente passar por vários dessas desses problemas né ou decisões que a gente tem que tomar ao desenvolver o modelo é que na disciplina anterior a gente não fez porque o foco era a gente entender um pouquinho mais como é que ocorre o aprendizado supervisionado ok então na disciplina anterior né a gente focou como eu tinha comentado entendeu Nacional por trás de algoritmos pressionado Como é que os algoritmos

- *Corpus ID:* 5852
- *Score:* 0.8514443635940552
- *URL:* oculto
- *Início:* 00:38:23
- *Fim:* 00:40:28
- *Transcrição:* um sistema que coleta informação registra ex dado né Tu tem uma imaginação assim de que tu sabe Ah não ele coleta os dados dessa forma achar que os dados depois vão estar exatamente como a gente imaginou é uma ilusão porque pode acontecer um monte de coisa errada entendeu E esse monte de coisa errado acaba influenciando sujando o dado e tudo mais então a gente tem que sempre ter usar o pandas para isso para verificar que o dado é consistente aí depois a gente daí fez essa verificação né daí a gente chega às conclusões e isso é especialmente importante quando a gente tá fazendo as médias e né gerando estatística né porque se a gente não olhar a qualidade do dado estatística não vai fazer sentido algum a gente toma decisão depois um monte de coisa que não tem nada a ver Entendeu então é bem delicado e E aí sim isso é uma questão assim de da responsabilidade sabe mas fechando parênteses Então tá Eh vamos olhar rapidamente então o ad09 vocês estão comigo então migramos agora pro ad09 Ah o ad09 o ad9 é um dado bem legal assim que de novo eu coletei ele assim tipo quando eu tava montando o curso eu gosto de sempre ir atrás de dados que eu tenho interesse assim né Tipo depois né mas enfim eh são os dados desse gsod aqui né E O legal é que eles T dados de 1929 até hoje certo e são dados do quê vocês Imaginem que tem várias estações de de de monitoramento do tempo espalhados pelo mundo inteiro tá pelo mundo inteiro e essas estações de monitoramento elas medem lá algumas métricas né Por exemplo temperatura pressão enfim chuva e tal e aí eh eles basicamente criaram um dataset que agrega do mundo inteiro então vocês Imaginem a quantidade de informação a quantidade de coisa que dá para tu concluir né ou ou ou enfim tentar concluir a partir disso né em vista do aquecimento global esse tipo de coisa né da do do que a gente tá vivenciando no hoje né tentar explicar o

- *Corpus ID:* 2330
- *Score:* 0.8505544066429138
- *URL:* oculto
- *Início:* 00:25:19
- *Fim:* 00:27:33
- *Transcrição:* inclusive se vocês vão projetar algo que envolve coleta de dados envolve né como é que eu vou armazenar esses dados já você já devem pensar que é importante vocês planejarem esse tipo de questão porque facilita muito análise de dados Tá tem outra pergunta da Caroline pode falar valores nulos não se enquadram nessa categoria né seria ruído o valor nulo o valor nulo seria valor faltante também a questão é como como esse vazio está codificado tá esse vazio ele pode estar codificado né é que depende muito do da base até da linguagem de que a gente está usando para usar como por exemplo site que ele olha esse Nan como como valores faltantes Tá mas poderia ser só que aí tem essa questão né antes de responder Definitivamente a tua pergunta é tem a questão da gente identificar Por que que o valor Tá Faltando Se essa se esse valor faltante ele tem uma razão aleatória ou não porque daí dependendo o nulo pode ter um significado que daí a gente quer manter por exemplo tá valores faltados por razão aleatória tá então erro da coleta de dados né então por exemplo há pessoas que são de preencher aquele Campo a coleta de dados ali será automática falhou naquele instante de tempo algo que é monitorado o tempo enfim algo aconteceu aleatóriamente não existe uma justificativa para aquela falta do dado então faltou poderia ter faltado e outra Instância naquela Instância ali ouve esse esse erro né de coleta tá agora por outro lado tem os valores faltantes por razão não aleatória que daí a falta desse Registro tem o significado tá então um exemplo mais tradicional disso é quando por exemplo você tem questionários com perguntas e cadeadas então por exemplo Ah você assina sei lá TV a cabo sim ou não aí a outra pergunta se você assina né Qual é o seu plano mensal anual então a pessoa

- *Corpus ID:* 9103
- *Score:* 0.8501539826393127
- *URL:* oculto
- *Início:* 00:58:38
- *Fim:* 01:01:17
- *Transcrição:* ponto de vista né também pode ser um trabalho conceitual de estudar uma técnica mais específica né ah quero estudar cluster né então vou pegar vários algoritmos quero ver como ele se comporta em bases de dados diferentes Então eu não sei que resultado eu vou chegar mas o trabalho é fazer essa exploração né e o trabalho quando ele é mais prático eu sei onde eu quero chegar e eu vou fazer né a aquele trabalho para chegar naquele resultado então nos dois casos a gente vai ter a mesma estrutura de trabalho né então eh uma metodologia que é o passo a passo do que a gente vai fazer com os dados e depois né uma execução e uma análise dos resultados eh tá aqui eu só vou passar porque ali foi né O que a gente já falou aqui indo para uma parte mais prática né que talvez até não tinha pensado nisso mas talvez responda um pouco mais ali a pergunta do Vanderley assim a gente tem três tipos de a gente classifica os dados né em Fontes primárias secundárias e terciárias né Então as fontes primárias voltando lá então são os dados brutos que em geral é o que a gente vai usar para fazer Os experimentos né então no momento que a gente tá pensando e tentando definir eh o tema do trabalho é bem importante a gente olhar se a gente tem dados né para fazer o trabalho né Podem ser dados reais ou dados sintéticos Então essa é uma parte bem importante né agora desse momento que aconteceu né com alguns alunos já tem uma ideia legal mas aí não tem os dados para fazer a análise e aí a gente não consegue né fazer o trabalho em importante aí pode ser dados reais ou pode ser dados né Eh de benchmark da né produzidos sinteticamente aqui como tem um exemplo eu posso fazer experimentos com usuários para coletar os dados aí nesse caso peço só que vocês tomem cuidado com o tempo né porque aí quando a gente Depende de usuários para fazer questionários ou

- *Corpus ID:* 2316
- *Score:* 0.8494856953620911
- *URL:* oculto
- *Início:* 00:03:12
- *Fim:* 00:05:14
- *Transcrição:* coleta e preparo dos dados Então quais são os atributos como é que vou transformar esse atributos como é que eu vou particionar os dados para desenvolver o modelo né treinar validar otimizar e preparamos como é que eu vou lidar com valores faltantes enfim tá E também a parte de engenharia de atributos Então a gente vai estar discutindo um pouquinho essas etapas iniciais esse processo de preparar os dados e a gente também falou da nossa introdução que existe esse level aprendizado de máquina que que diz que se eu tiver garba jeans garba de alto né então tudo que eu pretendo fazer com o meu modelo de aprendizado de máquina de metodologia não vai ser tão boa quanto forem é só te será tão bom quanto foi os meus dados então a gente precisa prestar muita atenção nessa etapa dos dados de preparar os dados para ter dados que sejam mais digamos assim limpos e utilizáveis né o seu valor mais facilmente explorável pelos modelos de aprendizado de máquina tá Então acho que vocês lembram né que a gente discutiu e realmente existe a estimativa que a gente gasta boa parte do nosso tempo nessa etapa de preparar os dados o que que são bons dados e aprendizados de máquina Tá eu vou passar por alguns pontos aqui porque acho que é bom a gente ter uma ideia quando a gente fala que a gente quer tomar os ossos dados melhores para serem mais facilmente explorados pelos modelos é bom a gente pensar em algumas características do que são bons dados tá então primeiros são Dados que são informativos ou seja são Dados que possuem características descritas das atributos dos dados tanto em quantidade quanto o nível de gravidade suficiente para aquele problema que eu tô querendo abordar de pressão então

- *Corpus ID:* 6634
- *Score:* 0.8492241501808167
- *URL:* oculto
- *Início:* 00:30:19
- *Fim:* 00:32:26
- *Transcrição:* tá mas tem todos esses outros aqui tem até do read clipboard que é tu faz contrl c e depois tu faz read clipboard né eh Mas aí tem por exemplo capítulo sete data cleaning preparation que maior parte do tempo quando a gente vai analisar dados a gente gasta com limpeza desses dados porque normalmente os dados que a gente pega eles são meio tem coisas estranhas dentro a gente precisa entender bem as todas as coisas estranhas para não gerar conclusões erradas né então é na na disciplina de aprendizado de máquina a gente viu isso na prática que a gente gastava perto de 90% para limpar a nossa base isso para poder depois e processá-la isso E aí Aqui tem os métodos de como é que a gente limpa como é que a gente entende Quais são os nas que que a gente faz com esses nas que é são informações faltantes né como é que remove como é que verifica se existe enfim tem uma série de coisas que dá pra gente fazer aqui tem vários exemplos ó feeling in Miss data isso aqui extremamente perigoso fazer isso dependendo do cenário né tipo assim tu tem lá uma dado faltante tu quer colocar um dado lugar ali será que isso faz sentido entendeu ver como é que isso funciona né para olhar eh e e tomar decisões importantes ali na na hora da preparação né antes de fazer qualquer coisa então todas essas operações fio na aqui Peg pegi um exemplo aqui aqui né tipo sei lá qualquer outra operação aqui drop duplicates que a gente já viu que a gente inclusive já usou essas operações todas elas já estão paralelizador a gente acaba é muito mais importante como como acho que foi o Denis que resumiu né ficar bem prolífico nessa api aí porque depois existe uma correspondência com com rapids aqui no caso por que que eu eu lancei essa discussão porque eu queria entender se eu preciso continuar explicando cada um dos elementos aqui entendeu para vocês eu sei que a gente tá meio que talvez na

- *Corpus ID:* 9053
- *Score:* 0.8486188054084778
- *URL:* oculto
- *Início:* 00:49:55
- *Fim:* 00:52:31
- *Transcrição:* tem tá chovendo tem mais acidentes mas em muitos pontos tinha mais acidentes Quando tava só porque as pessoas corriam mais Então essas análises ali que ele fez né E também cruzou com dados de do calendário para olhar assim feriados quando tinha um feriado mais estendido se tinha mais acidentes ou não então ele fez o cruzamento dessas três bases de dados e daí o usou né algumas técnicas de mineração de dados Então como regras de associação daí para analisar os resultados ali do trabalho né então Eh o que a gente vai fazer aqui é isso então não ten um problema aí que técnica eu vou usar vou que dados eu vou usar né que experimentos eu faço então no levantamento bibliográfico né no primeiro momento a gente vai olhar para trabalhos que são semelhantes ao que a gente quer fazer né a gente vai procurar alguns artigos científicos então A ideia é sempre olhar nos artigos assim a que dados eles usam para verificar se a gente pode usar também né ou onde eles conseguiram os dados e que métricas de avaliação eles usa né paraa gente né aproveitar o que já foi feito e também eh e usar o senso comum assim né que as pessoas usam para avar eh tá esse aqui eu vou pular Esse aqui foi mais um trabalho experimental que Ele olhou a gente não teve essa parte aqui né mas metodologias para data Science né todo o processo todo o ciclo de vida e daí ele fez né estudou as várias met olias existentes queria adotar uma metodologia nova eess se era uma pessoa que trabalhava num hospital e daí ele fez um estudo de caso para deita Science usando a metodologia ali dentro do hospital né mas aqui no curso a gente não não focou nessa parte né de metodologia e e a parte Tem uma parte bem grande de preparação e limpeza de dados que daí

- *Corpus ID:* 2261
- *Score:* 0.8480521440505981
- *URL:* oculto
- *Início:* 01:14:57
- *Fim:* 01:17:11
- *Transcrição:* meu caso eu recol tá é e depois dentro dessas médias pegar aí colocar dentro desse do meu meu data que a gente colocar as médias as médias aí colocar um desvio padrão por essa métrica por esse algoritmo depois tipo fazer assim entendeu fazer uma ordenação porque eu pegaria aqueles que ficarem com a Ricardinho com a ou seja com as maiores médias do recol e depois eles ordenariam pelo que tinham menor desvio padrão fazia duas Ordenações e de repente escolhi uns 10 só para mostrar ao invés de mostrar isso tudo sim entendi não eu acho assim ó os gráficos são bem legais tá eu acho que ele tá bem tá bem informativo assim eu acho que isso tá foi bem explorado uma das coisas que que é o problema de ter muitos é que você variou muito se preparamos então percebam que tem né basicamente o projeto A ideia é que você super não consigo escrever aqui não tá para editar que a ideia por exemplo do projeto é que você daqui a pouco tivesse essas duas linhas só iniciais de modelos né E esse não é uma questão que eu percebi mais de um grupo o pessoal variando muito e preparando muito é não Exatamente porque assim o que que acontece ao vocês variarem muitos e quilômetros na verdade você já estão antecipando uma etapa de otimização de preparamos que tem um custo bem maior e que a ideia é que essa etapa seja feita só para alguns algoritmos selecionados não para todos por isso que no Spot checking a gente normalmente pega um número menor de modelos variando algoritmo para cada algoritmo um pouquinho me preparando para ver olha vale a pena continuar explorando árvore de decisão na evidência KNN sdm Enfim então isso resolveria tá ter um conjunto menor de valores porque não seria o intuito do trabalho ter tantos tantos modelos ou tantas variações te preparamos

- *Corpus ID:* 5471
- *Score:* 0.8475238680839539
- *URL:* oculto
- *Início:* 00:03:06
- *Fim:* 00:05:00
- *Transcrição:* a gente vai olhar pô mas aqui eh a gente tem uma uma série de estratégias que podem ser usadas para melhorar essa performance né como foi citado aí agora a pouco né eh ah usa tabelas temporales depois você né gera uma final Eh vamos vamos dizer assim modularizar aí né o a tua consulta né ou uma aplicação spart que seja né eh ah assim aí eu fiquei na dúvida Qual a gente tem alguma ferramenta que a gente vai conseguir trabalhar aí no no no laboratório né no nosso ambiente de de trabalho aí acadêmico eh que consiga identificar essa essa questão né de o que que eu posso melhorar aqui na minha aplicação ou se realmente esse problema é é grande suficiente a ponto de eh eu não consigo mais espremer a aplicação é e assim não existe uma ferramenta mágica que te diga o problema está aqui entendeu pontualmente assim na realidade existem ferramentas que te trazem indícios entendeu então por exemplo na linguagem r quando tu tem um workflow lá que uma série de passos de transformação né tu pode usar um profiler esses profilers eles existem também em outras linguagens como Python né e Normalmente eles indicam assim ó essa essa query aqui ó Exatamente esse comando aqui tá levando tanto tempo entendeu ou tá levando sei lá 60% do tempo e aí tem muitas coisas assim que de vez em quando assim do ponto de vista sequencial só sequencial tá Talvez tu tenha como fazer isso que tu mencionou Ah vou criar uma tabela temporária vou fazer alguma coisa preliminar entendeu Vou fazer uma outra forma essa minha transformação e resolve Tá mas isso não é o foco dessa disciplina o foco dessa disciplina é que tu T já alguma coisa eficiente sequencialmente e tu quer ficar fazer ela ficar mais rápida entendeu E aí as escolhas não são exatamente assim a esse comando aqui entendeu as escolhas são mais assim em que Quais são os pedaços que foram tipo assim eu tenho o meu volume de dados

**Resposta gerada pelo LLM**: Para garantir a qualidade e consistência na coleta e preparação de dados, é crucial planejar como armazenar os dados para facilitar a análise (2330).  É importante verificar a consistência dos dados usando ferramentas como o pandas para evitar conclusões erradas (5852, 6634).  Deve-se considerar a qualidade dos dados, especialmente ao calcular médias e estatísticas (5852).  A identificação da razão pela qual valores estão faltando (aleatória ou não) é crucial, pois o nulo pode ter significado (2330).  O pré-processamento de dados faz parte da metodologia e deve ser descrito (2170).  A limpeza de dados pode levar até 90% do tempo de análise (6634).  É preciso atenção à etapa de preparação dos dados para obter dados limpos e utilizáveis (2316).  Dados informativos, com atributos descritos em quantidade e nível de gravidade suficientes para o problema, são desejáveis (2316).  Considerar o uso de dados reais ou sintéticos (9103) e, se usar dados reais, tomar cuidado com o tempo necessário para coletar dados de usuários (9103).  Para melhorar a performance, estratégias como o uso de tabelas temporais e modularização de consultas podem ser empregadas (5471).  No entanto, não existe uma ferramenta mágica para identificar todos os problemas; ferramentas como profilers podem ajudar a identificar gargalos (5471).  A verificação de um arquivo chamado robots.txt pode auxiliar na coleta de dados online (8314).

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.000000

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT, addressing strategies for managing data collection and preparation in data science projects while ensuring quality and consistency. Supporting Evidence: The RESPONSE provides a comprehensive overview of various strategies for managing data collection and preparation, including the importance of planning data storage, verifying data consistency, assessing data quality, identifying missing values, and the significance of data cleaning. It also mentions the use of tools like pandas and profilers, discusses the implications of using real versus synthetic data, and highlights the need for attention during the preparation phase. All these points are directly relevant to the PROMPT, which asks for strategies to ensure quality and consistency in data management.  Score: 3

---