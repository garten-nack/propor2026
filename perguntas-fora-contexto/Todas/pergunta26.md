**Pergunta 26**: Quais técnicas de validação cruzada são recomendadas para avaliar o desempenho de modelos de séries temporais?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2229
- *Score:* 0.8592476844787598
- *URL:* oculto
- *Início:* 00:19:38
- *Fim:* 00:21:47
- *Transcrição:* garanto que eu tô pegando 20% todos diferentes podem ser os mesmos certo Ficou claro essa divisão entre validação cruzada e voltar Auto repetido Então o roldauto a validação cruzada na verdade assim do ponto de vista teórico ela é extremamente simples tá digamos assim Foi eu não sei eu não eu realmente não sei de cabeça Quem inventou Mas foi muito feliz a pessoa que inventou porque é um método extremamente simples que virou o padrão ouro né de avaliação de modelos e que funciona bem para estimar desempenho em função da variação dos dados detectando esses casos extremos né esses possíveis problemas de avaliações muito otimistas ou muito pessimistas né em relação ao uso da validação cruzada a gente acaba tendo que optar pelo valor de k né que é um hiper parâmetro do método né quantas partições então tem valores que são comumente utilizados cá igual a 5 k igual a 10 certo são valores visuais assim como eu falei que o 280% para rodar o teu usual tá igual assim colocar igual 10 são bem comuns Mas claro que a escolha depende dos dados e das classes por exemplo se eu tenho algumas classes com poucos exemplos se eu dividir em 10 foldes no meu dado de teste eu posso vir a ficar com dois três exemplos de uma classe né Se tiver por uma classe com 30 exemplos e dividir em 10 forças ou ficar dois três exemplos no caso então é pouco aí talvez seja melhor usar cinco né então a decisão do que usar depende um pouquinho da quantidade dos dados se for classificação se for regressão é mais tranquilo né porque é regressão a divisão vai ser aleatória e são valores numéricos E aí a gente espera a gente não chega a controlar tanto isso pode estar amostragem mas a gente espera que as saídas né estejam igualmente representadas em todos os pontos tá já a validação cruzada com classificação a

- *Corpus ID:* 2196
- *Score:* 0.8564521670341492
- *URL:* oculto
- *Início:* 00:53:19
- *Fim:* 00:55:28
- *Transcrição:* modelos em dados que não sejam Independentes ou seja dados que não foram vistos em tempos de treinamento e existem diversas estratégias para a gente fazer isso né então a gente vai discutir essas mais comuns e uma que é considerada tipo assim talvez né o chamaria o estado da arte tinha mais recomendada que a validação cruzada tá e tem diversas variações Ainda assim eu trouxe outras que vocês inclusive já ouviram falar que também são utilizadas mas o foco assim eu diria que se tem uma metodologia de avaliação de modelos que vocês deveriam saber aprendizado de máquina é a cruzada O roldauto que é mais que a gente já viu e a validação cruzada tá bom então a gente tá falando nessa questão Quando eu digo selecionar os melhores ou determinar o melhor modelo ao final de todo o processo tem essa etapa de avaliação do modelo tá E aqui falando de modelo preditivo né então o modelo de classificação ou regressão a parte da divisão de dados se aplica aos dois tá tanto classificação quanto regressão a parte das métricas é diferente porque regressão é a gente viu na disputa anterior são mais voltados a minimização de um erro erro médio o erro quadrado enfim aí classificação tem mais métricas obviamente que são baseadas na comparação de rótulos por exemplo Então como é que eu avalia o modelos preditivos em aprendizados de máquina tá a gente já discutiu um pouco sobre isso e a gente viu uma estratégia que ela é muito utilizada né inclusive por exemplo aprendizado profundo muitas vezes se usa o rolda alto tá mas a gente vai ver que o roldauto ele tem algumas questões que podem limitar né então a gente vai discutir só ao longo da aula um ponto importante que eu deixei que como slide Inicial que independe da Estratégia que vocês usarem para avaliar os modelos ou seja Independente de como

- *Corpus ID:* 2250
- *Score:* 0.8538180589675903
- *URL:* oculto
- *Início:* 00:56:08
- *Fim:* 00:58:43
- *Transcrição:* queria mostrar para vocês como eu falei eu vou pelo menos mostrar que a questão do notebook para vocês terem uma ideia o que que vocês vão avaliar pedindo uma atualização aqui tá queria mostrar para vocês deixa eu ver aqui tá ele tá aparecendo né esse notebook eu disponibilizei ele para vocês né Eu só queria mostrar que a estrutura porque o site tem ele tem formas a gente trabalhar ou dar outro repetido a gente já fez né já fez alguns notebooks certo e ele também tem a possibilidade de trabalhar com validação cruzada e a validação cruzada ele tem duas funções tá que eu vou comentar por alto aqui com vocês ele tem duas funções Então esse notebook o que que ele faz para esses dados que a pressão de risco de diabetes enfim dá um zoom aqui ele faz um treinamento de modelos com cafold cross-valiation tá usando as duas funções possíveis então basicamente são funções muito parecidas que executam avaliações a partir dos k folds tá mas uma das funções permite que por exemplo que vocês especifiquem um vetor de métricas a ser analisada enquanto a outra classe escola ela é focada em uma métrica só então tem algumas variações assim de implementação que historicamente né o século depois colocou essa ali dentro aí ficaram essas duas versões certo e aí depois o que vocês verificar é primeiro uma avaliação visual mesmo para vocês entenderem como que a validação cruzada de vídeo Os foldes e qual é a diferença de ter uma validação cruzada com muitos pontos e ela repetida tá então que seria que o repitais fold e por fim a otimização de prepaâmetros usando wrest e nessa do crossligation e aqui é interessante porque como eu falei para

- *Corpus ID:* 2251
- *Score:* 0.8534529805183411
- *URL:* oculto
- *Início:* 00:58:05
- *Fim:* 01:00:18
- *Transcrição:* ficaram essas duas versões certo e aí depois o que vocês verificar é primeiro uma avaliação visual mesmo para vocês entenderem como que a validação cruzada de vídeo Os foldes e qual é a diferença de ter uma validação cruzada com muitos pontos e ela repetida tá então que seria que o repitais fold e por fim a otimização de prepaâmetros usando wrest e nessa do crossligation e aqui é interessante porque como eu falei para vocês a questão de como fazer isso no século Club é bastante simples a gente vai usar uma combinação dessas funções de cafo outras estratificado né para gerar os foldes com essas funções de Bridge Ou seja que faz uma busca em Grid de ir parâmetros né definidos como a gente vem usando só que usando cafoda aqui então como ela já usa cafoda se eu chamar isso dentro de uma outro loop de validação cruzada eu tenho uma validação cruzada alinhada então só vou mostrar para vocês a chamar atenção que nessa parte o que vocês vão verificar né através dessa célula é a diferença de desempenho quando eu faço uma validação cruzada tradicional uma validação cruzada alinhada tá é interessante que vocês observem que tem diferença e a validação cruzada tende a ser mais otimista no geral tá se eu não faço embutido e basicamente a questão de fazer uma validação cruzada alinhada né pulando um pouquinho digamos assim a questão do notebook mas só para vocês verem como uma simples essa célula aqui ela basicamente faz uma validação cruzada alinhada eu defino um KNN com os parâmetros que eu vou explorar para esse algoritmo tá então vou ter que cinco valores possíveis de k tá enfim poderia ter outros né E aí para o loop interno que é aquele que vai fazer a utilização de preparamos

- *Corpus ID:* 2530
- *Score:* 0.8531391024589539
- *URL:* oculto
- *Início:* 01:17:44
- *Fim:* 01:19:45
- *Transcrição:* desse atributo em relação àquela classe Então se nessas instâncias estão nos dados de teste ou de validação a gente acaba tendo vazamento de dados mas o efeito não é tão grande porque como você está comentando a gente não faz avaliação e termos do desempenho né então o desempenho atrelado a um modelo então a técnica de baseado em filtro eu diria que aquela mais tolerável da gente fazer fora da validação cruzada porque ela seria menos sensível a esse tipo de tá E até porque ela também é uma técnica a gente usa muito com grandes volumes de dados E aí fazer Deixa eu só voltar aqui fazer esse tipo de abordagem com grande volume de dados a gente mesmo que obviamente não vou usar a livonaldo eu vou usar por exemplo tendo folder validation mas acaba sendo muitas vezes inviável em praticável assim porque exige muito tempo né computacional então às vezes a gente não consegue arcar com esses custos ou com o tempo né para fazer esse tipo de experimentos então por isso que às vezes a gente acaba muitas vezes simplificando algumas coisas da metodologia que a gente sabe que não é a melhor forma de fazer mas a gente não consegue arcar com o curso de fazer tudo da forma correta ou né do ponto de vista sem vazamento de dados então por exemplo do ponto de vista de pesquisa quando a gente está fazendo a gente tenta ver assim o que que vai minimizar o meu Impacto o que que vai ter o menor impacto em relação a pergunta de pesquisa que eu tô querendo fazer né então se eu tô querendo comparar a questão de algoritmos por exemplo bom então vou fazer uma baseada em filtro antes em todos os meus dados e eu vou dizer Olha foi feita essa forma porque meus dados eram muito volumosos e porque o nosso objetivo era comprar outros aspectos e não a sensibilidade

- *Corpus ID:* 2239
- *Score:* 0.8516424298286438
- *URL:* oculto
- *Início:* 00:37:05
- *Fim:* 00:39:32
- *Transcrição:* escolher os e preparamos e avaliar o com o quão bons eles são para o nosso modelo a gente tem que fazer isso com dados independentes e às vezes quando a gente tem poucos dados fica difícil fazer isso né fazer essa divisão por isso que tem se usado muito na literatura honesta do Cross validation tá do ponto de vista do que a gente vai trabalhar na disciplina quando vocês quiserem fazer a validação cruzada tradicional Tá ok né digamos assim porque realmente é um método apropriado mas existem formas de fazer isso é que consegue planner que também não é mais difícil que acontece é que implica em mais repetições então é mais gostoso computacionalmente tá o Alexandre tem uma pergunta pode falar Alexandre isso eu poderia sempre relativo que por exemplo porque isso surgiu no trabalho da gente né a gente fazendo trabalho por isso que é 7000 é pouco é muito dois mil e pouco é muito e aí ficou essa dúvida né eu posso estimar ou é relativo sempre é difícil a gente a gente ser um valor fixo assim depende muito dos dados mesmo também depende do número de atributos enfim então não não tem como dizer um valor absoluto tá realmente depende bastante da estrutura do teu conjunto de dados a medida que vocês forem trabalhando com isso vocês também vão adquirindo um certo peeling né Se vocês acham que são poucas instâncias ou poucas instâncias em algumas classes por exemplo E aí a entender e pensar em estratégias de divisão de dados que trabalha melhor dentro dessa limitação de dados que você tem sempre a melhor solução é ter mais dados mas muitas vezes não é viável fazer isso alguma dúvida mais vocês vão ver lá no notebook disponibilizei tem um trecho ali que compara avaliação de desempenho com a validação cruzada e com o nesta crossvalidez e é interessante você

- *Corpus ID:* 2235
- *Score:* 0.8514400720596313
- *URL:* oculto
- *Início:* 00:29:38
- *Fim:* 00:32:07
- *Transcrição:* ele classifica não a partir dos dados originais dos atributos mas das predições dos modelos que a gente gerou certo mas alguma dúvida pessoal certo vamos lá então tá então isso o livonaldo é uma variação da validação cruzada e tem uma outra variação eu diria que isso aqui é um nível um pouquinho mais avançado Tá mas vocês vão provavelmente ouvir falar porque assim muitas vezes a gente tem uma base de dados que a gente considera pequena e a gente precisa usar essa base de dados para otimizar ou se parâmetros escolher o melhor modelo tá então a gente quer fazer o máximo possível subdividir esses dados para tomar essas decisões então a validação cruzada alinhada ela nada mais é né o próprio nome tá dizendo é uma validação cruzada dentro da outra E aí como é que como é que funciona esse processo Então a gente vai ter um loop externo que é o halter e o loop interno que é o Winner tá vamos Vamos por partes aqui vamos supor que os meu conjunto de dados tá ele tá sendo primeiro aqui são as três interações ele tá primeiro sendo dividido em Três Fontes aqui tá eu vou tentar botar uma cor melhor laranja talvez tá esses três Fontes eu vou dar um zoom aqui só para vocês entenderem tá esses três Fontes basicamente eu tô usando né esse mais claro para teste esses aqui para treino e nessa interação eu claro gostaria de avaliar um modelo por exemplo como é que se sai um KNN enfim como é que sai uma árvore gestão mas esse modelo tem e para parâmetros e eu queria avaliar esse modelo ou melhor e preparamento possível mas eu preciso escolher se preparaâmetro né então uma abordagem que se faz é para esse conjunto de treino que surgiu dessa divisão da validação cruzada aqui dentro a gente faz mais um cafunto próximo ele deixa usando só os dados de treino

- *Corpus ID:* 2246
- *Score:* 0.8499563932418823
- *URL:* oculto
- *Início:* 00:48:52
- *Fim:* 00:51:17
- *Transcrição:* momento tá mas eu vou fazer um esquema aqui supondo que eu tenho todos os dados certo que meus dados originais e Eu dividi esses dados em treino e validação e teste tá fiz uma divisão aqui com roldauto certo para que pronto aí apareceu e aí eu decidi Como eu queria escolher para parâmetros modelos eu decidi que esse dado de treino aqui eu ia fazer uma validação cruzada eu vou botar três fotos tá então basicamente a gente tem que Três Fontes e cada hora fica um para teste que é esse azul Tá eu vou chamar de teste não vou chamar de validação tá Então nesse nesse nessa questão dos treinos os que são vermelhos são usados para treino o azul para validação E aí vamos supor que aqui eu tô usando uma validação cruzada simples para sempre ficar um pouco o esquema Mas o que eu vou tirar de conclusão daqui é o melhor modelo tá do ponto de vista da minha análise aqui de otimização de super parâmetros Qual é o melhor modelo tá E vamos supor que eu vou chutar nessa vamos supor que eu defini que é um svm Radial com o c de 0.01 tá testei M possibilidades n modelos e cheguei nesse aqui melhor tá então aqui já fazendo uma otimização de preparamos isso aqui já tô falando de um ponto final mesmo que eu já tinha passado pelo spotch então fazendo otimização de preparando se eu quero definir qual é o melhor modelo esse melhor modelo eu avalio aqui com os dados de teste E aí eu digo Olha esse modelo svm Radial com esse valor de regularização enfim esse melhor modelo ele só voltando um pouquinho né que eu acho que teve um ponto que eu esqueci de

- *Corpus ID:* 2525
- *Score:* 0.8494847416877747
- *URL:* oculto
- *Início:* 01:09:48
- *Fim:* 01:11:56
- *Transcrição:* São nesses dados e que a gente costuma ver maior efeito de deita líquida para qualquer coisa no geral né para qualquer tipo de tarefa então aqui isso é uma figura dos autores eles assim qual é o protocolo padrão que a maioria das pessoas usa para fazer seleção de atributos pega todos os dados tá faz uma seleção de atributos usando uma estratégia F qualquer filtro em Beta enfim rapper tá E aí seleciona um subconjunto s de atributos E aí usa esse subconjunto para fazer treinamento e validação de modelos usando validação cruzada e aqueles usam L O que é o livro anato Cross validation porque é uma técnica que para pequenas amostras né pequenos dados é mais usada E aí então assim selecionei os atributos com todos os dados Depois desses dados eles passaram por uma validação cruzada então o treinamento e validação aqui existe vazamento de dados porque a seleção de atributos foi feito com os dados juntos e não separados E aí se reporta aqui uma acurácia por exemplo do modelo usando esse subconjunto de atributos E aí o que os autores alertam é que aqui tem uma contaminação de dados Tá mesmo que a gente vem a pensar bom para grandes conjuntos de dados o impacto no desempenho na Estimativa de desempenho pode ser pequeno mas é um vazamento de dados quando eu faço a seleção de atributos para todos os dados E aí uso esses dados depois para treinar o modelo reportar o desempenho tem contaminação de dados e o que acontece é que para um pequeno número de instâncias né quando tem um conjunto de dados que é que tem essa alimentação de ter um pequeno tamanho amostral o viés nesse desempenho que a gente reporta desse modelo com seleção de atributos tende a ser otimista então ou seja esse desempenho estimado para esse modelo tem ainda ser um pouquinho acima do que ele de fato é e na verdade para alguns dados aqueles mostraram que não é só um pouquinho assim mas às vezes é bastante

- *Corpus ID:* 2226
- *Score:* 0.849216639995575
- *URL:* oculto
- *Início:* 00:14:43
- *Fim:* 00:16:48
- *Transcrição:* uma vez no conjunto de teste tá esse método ele é conhecido como a validação cruzada ok falou ele deixa né então a validação cruzada com a fonte ou inglês a gente acaba tocando os termos Cronos Qual é a proposta desse conjunto de dados aquele conjunto que eu tô usando no desenvolvimento do modelo então percebam que normalmente isso aqui deriva fazer uma só para vocês lembrarem eu tenho meu conjunto de dados aqui os meus dados originais fiz uma divisão normalmente usando roldaut em teste e por exemplo treino tá então esse treino na verdade é o que a gente usa para desenvolvimento do modelo aí esse conjunto de treino eu tô subdividindo só que ao invés de dividir com outro roldauto eu tô usando a validação cruzada E aí a ideia da validação cruzada é justamente o seguinte ao invés de eu pegar amostras por exemplo 20% 10% ou 15% aleatória e repetido eu pego os meus dados eu crio partições partições de juntas ou seja nenhuma Instância tá em duas partições ou mais né Cada Instância tá somente em uma partição de tamanho aproximadamente igual né e normalmente de forma estratificada se for classificação ou seja mantendo a proporção de exemplos por classe ao fazer isso então por exemplo esses meus dados percebam que aqui nos slides eu vou sempre mostrar esses fundos de uma forma consecutiva para facilitar a representação mas não quer dizer que essa divisão tem que ser exatamente consecutiva que eu não posso ter a distância dois no fold 2 A 3 no 3 assim por diante tá é só para simplificar a representação aqui dos slides Então vamos supor que eu divido os meus dados em cinco Fontes Então esse k aqui no nosso cafunto diz respeito ao número de partições o número de fontes então todos esses dados eu vou


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: A resposta deve ser relevante para o prompt, abordando as técnicas de validação cruzada recomendadas para avaliar o desempenho de modelos de séries temporais. Supporting Evidence: A resposta não fornece nenhuma informação relevante sobre as técnicas de validação cruzada para modelos de séries temporais e não tenta responder à pergunta. Portanto, não atende a nenhum dos critérios de relevância.  Score: 0


---
