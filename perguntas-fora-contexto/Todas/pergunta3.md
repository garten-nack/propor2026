**Pergunta 3**: Quais técnicas de pré-processamento são mais eficazes para séries temporais não estacionárias?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 5471
- *Score:* 0.8458731174468994
- *URL:* oculto
- *Início:* 00:03:06
- *Fim:* 00:05:00
- *Transcrição:* a gente vai olhar pô mas aqui eh a gente tem uma uma série de estratégias que podem ser usadas para melhorar essa performance né como foi citado aí agora a pouco né eh ah usa tabelas temporales depois você né gera uma final Eh vamos vamos dizer assim modularizar aí né o a tua consulta né ou uma aplicação spart que seja né eh ah assim aí eu fiquei na dúvida Qual a gente tem alguma ferramenta que a gente vai conseguir trabalhar aí no no no laboratório né no nosso ambiente de de trabalho aí acadêmico eh que consiga identificar essa essa questão né de o que que eu posso melhorar aqui na minha aplicação ou se realmente esse problema é é grande suficiente a ponto de eh eu não consigo mais espremer a aplicação é e assim não existe uma ferramenta mágica que te diga o problema está aqui entendeu pontualmente assim na realidade existem ferramentas que te trazem indícios entendeu então por exemplo na linguagem r quando tu tem um workflow lá que uma série de passos de transformação né tu pode usar um profiler esses profilers eles existem também em outras linguagens como Python né e Normalmente eles indicam assim ó essa essa query aqui ó Exatamente esse comando aqui tá levando tanto tempo entendeu ou tá levando sei lá 60% do tempo e aí tem muitas coisas assim que de vez em quando assim do ponto de vista sequencial só sequencial tá Talvez tu tenha como fazer isso que tu mencionou Ah vou criar uma tabela temporária vou fazer alguma coisa preliminar entendeu Vou fazer uma outra forma essa minha transformação e resolve Tá mas isso não é o foco dessa disciplina o foco dessa disciplina é que tu T já alguma coisa eficiente sequencialmente e tu quer ficar fazer ela ficar mais rápida entendeu E aí as escolhas não são exatamente assim a esse comando aqui entendeu as escolhas são mais assim em que Quais são os pedaços que foram tipo assim eu tenho o meu volume de dados

- *Corpus ID:* 8325
- *Score:* 0.8458723425865173
- *URL:* oculto
- *Início:* 00:01:46
- *Fim:* 00:04:05
- *Transcrição:* bag of WS eh para abordagens bag of WS para reduzir a dimensionalidade tá bom professor eu fico imag eu fico imagido que para técnica é porque essa técnica do B of Wars ela já tá um pouco defasada então eu imagino que as as técnicas novas que vocês foram mostrando elas na verdade não tem necessidade da gente fazer esse tipo de coisa não é isso muitas vezes você já tem um modelo um modelo pré treinado que aí você vai transformar nos como embeds nas embeds e as embeds são os dados que a gente vai entrar para o nosso determinado modelo de de previsão eu tô tô errado ou tô entendendo corretamente essa questão eh a assim do tempo né do tempo das técnicas que n foram se desenvolvendo sim as técnicas mais atuais são baseadas em em Bedin contextuais e em Transformers principalmente acontece que as que que os modelos B of fors eles ainda têm bons resultados para várias tarefas a gente pode ver que nesse dataset das Americanas por exemplo o resultado do SV é melhor do que os resultados que a gente obteve com ah com o o lstm e as worden Bells eu acho que até melhor do que o do burt se bem que o do burt a gente treinou com menos né Igual ficou igual e ele é muito mais barato uhum ah então assim na verdade o que de uma forma geral que a gente pode fazer é de repente utilizar algumas técnicas que são mais baratas que são um pouco mais antigas Óbvio você vai ter Sei lá talvez ela seja nossa baseline e depois você usa outras técnicas mais avançadas E aí você pode comparar e Óbvio aqui se Adapt melhor os dados que você tem você vai utilizar no seu no seu processo né no seu problema isso exatamente às vezes uma técnica mais antiga e mais barata eh se dá um resultado parecido com com uma técnica que é bem mais poderosa então é é preferível até porque é porque é mais barata né mais barata no sentido bom

- *Corpus ID:* 8326
- *Score:* 0.8449081182479858
- *URL:* oculto
- *Início:* 00:03:32
- *Fim:* 00:05:49
- *Transcrição:* Sei lá talvez ela seja nossa baseline e depois você usa outras técnicas mais avançadas E aí você pode comparar e Óbvio aqui se Adapt melhor os dados que você tem você vai utilizar no seu no seu processo né no seu problema isso exatamente às vezes uma técnica mais antiga e mais barata eh se dá um resultado parecido com com uma técnica que é bem mais poderosa então é é preferível até porque é porque é mais barata né mais barata no sentido bom tanto no sentido de de custo computacional como às vezes eu vou comparar com usar um um sei lá algum modelo da Open ai por exemplo que aí eu tenho que pagar então aí mais barato em termos ficeiros também ok E no caso do pré-processamento a gente continua fazendo esse pré-processamento mesmo utilizando esses modelos mais avançados a parte de retirada de acentuação maiúsculo etc Depende por exemplo no no bir é a resposta que eu que eu dei no no início eh no se a gente vai pegar o modelo do birth tem o temho a versão cased e a versão un Case quer dizer uma versão que tá que tão todos para minúsculos e a outra que Manteve o casing original então eu vou vou usar de acordo com o modelo pré-treino que eu peguei eh Se o se eu tô tentando classificar eu tô tentando trabalhar com dados que foram gerados pela internet dados de Twitter as pessoas não usam ã acentos na maioria das vezes ou muitas vezes então a gente talvez melhor não usar acento agora se eu vou lidar com com um texto de de eh sei lá texto jurídico que em geral um texto eh razoavelmente mais mais bem escrito as pessoas escrevem eh sem a sem tanta abreviatura sem gíria de internet então vai ter que adap usar o que seja adequado pro pro dataset que que eu tô trabalhando pros dados que a gente tá trabalhando remoção de stopwords e stemming a gente costuma aplicar quando a gente trabalha com vetores B bag ofws

- *Corpus ID:* 8109
- *Score:* 0.8411996960639954
- *URL:* oculto
- *Início:* 00:16:38
- *Fim:* 00:19:12
- *Transcrição:* nos modelos que a gente tenta gerar eh texto a gente não pode omitir as coisas que a gente tem mais no no no texto que são as stopwords e a gente e para gerar texto coerente ele precisa enxergar que eh se eu digo eu estou não vai vir a nem apresentar nem apresentação nem apresente tem que ser apresentando é o que que é esperado vir depois entendi então de maneira análoga é questão de remoção de pontuação essas coisas também não não é adequado para uso para esse uso né Model exatamente a gente quase não pré-processada unidade mas mantém tudo mantém tows mas aqui a gente tá ainda na na pros algoritmos estatísticos por exemplo o que que eu vou fornecer para um svm que que eu vou fornecer para um para uma para uma regressão logística para ele me conseguir me dar classe da da entrada por exemplo E aí o sufixo provavelmente não não ajuda não ajuda a diferenciar e e e para esse tipo de de algoritmo onde a gente usa cada a palavra como feature aqui eu fazendo stemming eu ganho aqui eu economizo em vez de ter quatro fatos diferentes quatro colunas diferentes eu vou ter uma só e aí isso ajuda também a reduzir a dimensionalidade dos dos dados que a gente também vai falar um pouquinho sobre isso então como é que a gente faz um stemmer bom normalmente são algoritmos baseados em regras para inglês esse assunto a estudado desde o final dos anos 60 Então teve um algoritmo chamado lov and stammer eh e depois o mais popular foi lançado em 1980 que é o Porter stammer que ele mostrou que ele era tão bom ou equivalente ao lovins em termos de de taxa de erros e acertos e para a taxa que ele melhorava a as tarefas de recuperação de informação eh só que ele era bem mais simples ele tinha um conjunto bem menor de regras existem vários né para para inglês e exemplos de como essas regras são formadas é por exemplo eu tenho uma regra que se a

- *Corpus ID:* 6008
- *Score:* 0.8391071557998657
- *URL:* oculto
- *Início:* 00:05:03
- *Fim:* 00:07:10
- *Transcrição:* então para isso eh houve um pré-processamento isso aqui eu tô contando a história um pouco do livro ali que a gente tá usando como referência tá houve um pré-processamento para que os dados sejam concatenados para evitar que esses arquivos muito pequenos sejam o gargalo digamos assim da de uma eventual análise de dados tá E aí uma questão que surge que a gente pode fazer seria assim qual é a maior temperatura global registrada em cada ano neste conjunto de dados Então como tem estações meteorológicas do do do mundo inteiro né Eh saber qual é a que registrou a maior temperatura em cada ano ao longo de todos os anos né seria essa questionamento né que eu acho que nem é um questionamento que tinha naquele naquela Nossa atividade dirigida sobre esse assunto aí bom se a gente for olhar esse eh tentar responder essa pergunta unicamente com ferramentas Linux e Unix né então o livro ele traz uma solução com awk não sei se alguém de vocês já teve experiência com awk Mas é uma ferramenta do tipo sed não sei se alguém usou the stream editors né que usa massivamente expressões regulares né para eh fazer o a transformação dos dados A análise dos dados né então a ideia aqui ah usando ferramentas Unix né é que para cada ano então a gente vai olhar para cada ano já que cada ano tem uma um um arquivo agora né foi feita essa transformação a gente tem que descomprimir os dados daquele ano e aí para cada registro a gente vai verificar se a temperatura é válida esse essa coluna temp aqui seria para ela ser válida ela tem que ser diferente do número 999 que é uma codificação que tem lá no nos dados tá e uma outra coluna a coluna Q Q de qualidade tá para dizer que é bom aquele dado porque pode ter por exemplo uma medida que ela é válida ou seja ela é diferente de 999 mas o que não Tá informando que aquilo é uma coisa de

- *Corpus ID:* 8594
- *Score:* 0.8381351828575134
- *URL:* oculto
- *Início:* 00:49:44
- *Fim:* 00:52:15
- *Transcrição:* rodo quein várias vezes com vários n clusters aqui e imprime isso num num gráfico né onde a o resultado ele usa uma ele usa uma métrica que eu acho que é da silueta não não recordo mas isso tem Ah como é que é o nome disso cotovelo isso pode ajudar ó elbo a curva de elbo isso curva de elbo tá então tem no no Pit learn vocês podem usar para definir o melhor número Às vezes isso não funciona pessoal nem sempre às vezes tu fica com um cotovelo meio estranho ali aí tu roda um pouquinho depois parece melhorou porque tem aquela coisa da interpretabilidade dos Tópicos não é um simplesmente um clustering normal né então dá para usar essa curva de elbow e e outras técnicas Então tá aqui eu gerei um modelo chamado cluster Model passo dentro da variável chamada hdb Scan que é como o Berto ap chama e pronto aí eu tô tô trabalhando tô passando por cima por cima do que tá defold e o último modelo que é o count vectorizer a gente falou de dimensionalidade eh redução de dimensionalidade clustering e agora a gente vai falar da representação que é a parte final do pipeline então ele tem o o cal factorize é uma classe lá no pych Le que pode ser usada aqui mas tem outras formas de fazer isso tá eu ele chama que deic vectorizer porque eu tô chamando desse nome porque ele usa a variável vectorizer Model tá E ele gosta de receber na forma que o count vectorizer entrega tá esse count vectorizer ele permite que a gente faça várias coisas então a gente passa stopwords passa uma função de pré-processamento lembra que a gente usou isso lá no TF IDF eh ele eh eu posso sobrescrever Então olha só tem várias vários parâmetros aqui e uma del deles é o pre-process aqui ó que sobrescreve funções de pré-processamento ó assentos lower Case então eu posso passar a minha função

- *Corpus ID:* 3353
- *Score:* 0.837171196937561
- *URL:* oculto
- *Início:* 01:03:33
- *Fim:* 01:05:47
- *Transcrição:* é bom que a rede não tem a memória tá a rede não tem memória e de sem memória O que é ok né para esses exemplos aqui de entradas Independentes né Então tá tudo ok aqui tudo certo minha rede não tem memória e não é para ter mesmo não é para influenciar Mas vamos supor né Isso é a entrada se os dados tem relação temporal né então quem já ouviu aquela palavra chave de séries temporais é isso aqui o que que acontece numa série temporal o dado em algum momento Depende do que aconteceu antes né então que um exemplo sabe cotação de Bitcoin tá ultrapassado isso aqui tá do ano passado ó Setembro do ano passado agora tá é outro gráfico né Mas o importante é que é o seguinte vamos pegar um ponto Qualquer aqui esse ponto aqui ó esse ponto aqui tá esse x aqui né tem essa esse x que sei lá alguma data entre junho e setembro o Bitcoin tava valendo esse valor aqui de reais ó tá valendo esse valor aqui deve 125.000 parecendo né e olha só né aqui tava numa tendência de subida Né tava deu uma subidinha depois estabilizou depois caiu mas olha aqui tem outro ponto aqui no mesmo valor né no mesmo patamar só que em outro momento da vida do Bitcoin e nesse momento aqui tá numa tendência de queda aqui numa tendência eu vou te chamar de estabilidade aqui né aqui não a tendência de queda que não tem nem estabilidade na mesma mesmo 125 mil reais só então né agora o próximo digamos assim ó soltou no momento de dizer se o Bitcoin vai aumentar ou Subir se eu devo comprar ou vender Bitcoin ou qualquer outra ação no mercado de valores eu tenho que saber o que que veio antes eu tenho que saber o que que veio antes tudo isso é importante para saber o que vai acontecer daqui para frente da mesma forma aqui ó não é tudo que veio antes é importante para saber o que vai acontecer dali para frente tá esse

- *Corpus ID:* 1408
- *Score:* 0.8368098139762878
- *URL:* oculto
- *Início:* 00:44:24
- *Fim:* 00:46:40
- *Transcrição:* usar esse tipo de análise para definir questões de pré-processamento como por exemplo a questão de como fazer a normalização dos dados né a gente basicamente o que a gente tem algumas combinações às vezes que podem até sair melhor por exemplo de questão de preprocessamento com algoritmo mas é coisas que são aspectos muito difícil da gente estabelecer regras tá então a gente normalmente isso se vocês podem fazer esse tipo de avaliação é interessante comprar é processamento de normalização de forma de imputação de valores a questão é que muitas vezes isso decorre um custo muito grande computacional porque a gente vai estar fazendo isso em Loops depois a gente vai ver que tem vários looks de avaliação quando a gente discutir na próxima disciplina estratégia de validação cruzada enfim tá então mas eu posso dizer o seguinte o impacto dessas normalizações ele não é tão grande assim tá assim no sentido de que vai mudar drasticamente o resultado do modelo de vocês tá pode ser que uma técnica ajude um pouco mais a determinar alguns padrões mas isso a gente tem que observar caso a caso mas por exemplo se vocês têm tempo e recurso limitado vale muito mais a pena selecionar uma um tipo de normalização usei scoremax de forma bem arbitrária digamos assim olha escolhi mimax né E focar em explorar mais algoritmos do que gastar mais tempo com essa parte escolher uma estratégia de normalização invariavelmente vai funcionar porque porque não importa se os teus dados obedecem distribuição normal ou não o mimax você pode utilizar e talvez assim olha só não tô realmente tô pensando agora se você tem uma distribuição que é normal aí você aplica normalização ele vai colocar ali com a média zero dizia o padrão é um É sim Ali vai se adaptar vai ficar legal assim é mais você independente do tipo de distribuição que

- *Corpus ID:* 2150
- *Score:* 0.8367496728897095
- *URL:* oculto
- *Início:* 01:13:07
- *Fim:* 01:15:26
- *Transcrição:* estratégias a gente vai discutir na próxima disciplina não queria aprofundar isso aqui porque não seria o caso mas se vocês forem pensar em estratégias para lidar com esses problemas né eu diria que são essas duas nesse momento tá o Jefferson tem uma pergunta pode falar Jefferson É verdade eu queria saber se tem algo especial que eu consigo enxergar alguma vantagem através do gráfico de e a outra é no exemplo que foi passado no mundo lá o código de GG que tem um artigo eu achei bem legal exemplo eu tô tentando achar um equilíbrio entre aquele código e o que foi passado em sala de aula para fazer o trabalho final e tem a hora que ele vai fazer o pré-processamento ele usa o Neymar e o stander daí eu quero saber se eu uso esses dois em conjunto ou só um mate é o suficiente e como que seria o Slender que a gente acho que não viu né Isso tá eu vou começar então pela segunda pergunta tá eu sugiro que vocês usem Um só e o mimax é suficiente para gente tá suficiente assim até por essa questão assim de como a gente não aprofundou tanto em questões de preprocessamento não não espero que vocês aprofundam no projeto em relação a isso então podem usar o mi Max tá em relação a primeira pergunta do dry Potter na verdade ele é muito parecido com esse tipo de plot aqui né então esse aqui seriam curvas de densidades que a gente coloca no mesmo gráfico mesmo mesmo eixo X aqui para avaliar o desempenho né e comparar a questão do Joy Potter acho que a única diferença pelo menos que eu que eu lembro é que ele faz isso é como se fosse em camadas né e ordenado de uma forma coerente que Facilite a visualização então por exemplo se eu tiver três modelos né algo assim

- *Corpus ID:* 7618
- *Score:* 0.8365141153335571
- *URL:* oculto
- *Início:* 00:02:39
- *Fim:* 00:05:03
- *Transcrição:* eh acho que as aulas mais importantes né para para entender essa base foi foram a a de sexta-feira com Cláudio né E essa primeira metade aí tá é claro que a gente não quer necessariamente que vocês saiam né aplicando wavelets e entendendo exatamente como é que esse negócio funciona e tudo mais E por que que tem os subespaços vetoriais e blá blá blá Não nada disso né o que a gente quer com esse monte de coisa é mostrar que existem informações que são melhor percebidas na eh num nível de de resolução do que em outro tá eh e no final das contas a ferramenta pré Deep learning que melhor fazia isso até então era essa de Tá além disso a ideia também era mostrar que a noção de frequência né Ela é importante eh paraa filtragem e para outras tarefas né como reconhecimento de de partes ali né como a gente viu no no no último exemplo do da imagem com ruído né h e que essa noção de frequência ela é compartilhada querendo ou não com a parte de fourrier alguma coisa aqui de wavelets também então a gente tem a ideia de cada nível de decomposição captura uma frequência diferente né e por sua vez frequência filtragem de por frequência no domínio fourrier pode ser também né feita de uma forma um pouco menos eh inteligível digamos assim no domínio espacial original né que foi alguma coisa que o Claudio deve ter mostrado lá para vocês do na galciana no espaço no e no e no espectro né Tá então acho que essas aulas são as mais importantes definitivamente pré eh até agora pelo menos tá pra gente entender o resto mas pra gente entender essas aulas a gente também precisava das aulas anteriores né então assim eh de novo é uma disciplina introdução programa introdução a apressamento de imagens É uma disciplina de 60 horas introdução a visão computacional É uma disciplina de 60 horas a gente a gente junta as duas


**Resposta gerada pelo LLM**: O contexto não menciona técnicas específicas de pré-processamento para séries temporais não estacionárias.  Menciona-se a importância de considerar a relação temporal dos dados em séries temporais (passagem <id>3353</id>), mas não há detalhes sobre métodos de pré-processamento para lidar com a não estacionariedade.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to the techniques of pre-processing for non-stationary time series. Supporting Evidence: The RESPONSE does not address the PROMPT directly, as it fails to mention any specific techniques for pre-processing non-stationary time series. Instead, it only discusses the importance of considering the temporal relationship of the data without providing actionable information or relevant techniques. Therefore, it does not fulfill the requirement of being relevant to the entire PROMPT.  Score: 0


---
