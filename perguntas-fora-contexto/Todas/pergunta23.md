**Pergunta 23**: Dado um conjunto de treinamento com tweets e seus retweets, como você preveria o número de retweets de um determinado tweet após 7 dias, tendo observado apenas 2 dias de dados?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 7786
- *Score:* 0.8696811199188232
- *URL:* oculto
- *Início:* 00:07:59
- *Fim:* 00:10:35
- *Transcrição:* treinado né Eu não vou mexer nesse cara vou mexer só naquelas camadas em que eu adicionei tá que são aquelas camadas que vocês adicionaram de de totalmente conectada certo eh depois que esses dois modelos tiverem treinados vocês vão avaliar eles no conjunto de teste e ver né como é que foi o desempenho desse cara tá eh Aqui pede que vocês avaliem só acurácia tá eh mas assim Vocês poderiam ver né Qual foi o desempenho do método para cada uma das classes independentemente eh especialmente se o conjunto de dados for desbalanceado tá H bom se lá pelas tantas vocês escolheram um backbone ou uma arquitetura de rede final ali né das camadas finais que não foi Ok do ponto de vista que né para aquilo que vocês esperavam né Vocês podem usar algum tipo de data augmentation né ou fazer um fine tuning do modelo né ã que significa basicamente treina esses essas últimas camadas aqui e depois descongela tudo e e retre né só com com o com os dados que vocês têm agora tá então é basicamente esse o o trabalho dois tá é um problema de classificação que a gente vai ver um pouquinho mais de detalhe agora na aula de hoje tá E vocês vão usar back BS pré treinados ou ou não né mas back BS conhecidos para essa atividade tá tranquilo ok Então vamos lá eu vou fazer aquela aí tá Então pessoal eh a aula de hoje ela fala justamente sobre classificação né Então esse é um dos pontos e detecção eu sei que o Cláudio já de uma forma ou de outra ele já adiantou parte desse

- *Corpus ID:* 6928
- *Score:* 0.8692358136177063
- *URL:* oculto
- *Início:* 00:42:58
- *Fim:* 00:45:08
- *Transcrição:* dados estar na memória é uma Raridade entendeu normalmente não é o caso quando tu vai fazer treinamentos grandes mesmo né daí eh tem uma outra tem um outro exemplo que o repeat pode ser útil também que é quando a a noção de época eh porque assim normalmente a gente tá tá enraizado na gente que uma época é passar sobre a integralidade dos dados né Ah mas veja bem passar sobre a integralidade dos dados pode levar muito tempo entendeu se a gente tiver muito dado entende tipo assim o que que esse muito tempo eh pode ser tipo eh sei lá meses vamos supor né então tu pode mudar a definição de época entendeu para tu ter um feedback entendeu de como é que tá indo o treinamento entendeu então tipo assim talvez tu não precise ler a integralidade dos dados né tu lê sei lá tu faz com que o tua época seja 10% dos teus dados entende de maneira que tu já tenha uma validação entende porque esperar muito tempo para tu ter uma validação E aí tu já vai poder talvez fazer o stop né porque talvez aquele teu modelo já não seja bom entendeu Então nesse caso a definição de época vale a pena mudar também entendeu nesse sentido sabe ã buen Acho que vamos adiante então bom então mas alguma dúvida zero dúvida então tá então assim ah aqui a gente tá falando então do do do Bet né e o Bet vocês podem perceber né que ele agrupa então aqui em 16 em 16 elementos e esses esses bets aqui agora né eles serão fornecidos então pro treinamento treinamento vai fazer vai consumir eles né ao mesmo tempo lembrem que cada no nosso caso aqui a gente tá simando que cada item é um número né então a gente realmente vai fazer o estímulo da nossa rede né usando 16 números ao mesmo tempo Ok por questões de desempenho como eu falei antes eh então assim existe a situação dessa última desse último caso aqui estar

- *Corpus ID:* 3390
- *Score:* 0.8674687147140503
- *URL:* oculto
- *Início:* 00:35:59
- *Fim:* 00:38:25
- *Transcrição:* sem treinar né esse texto foi não tá no data 7 Tá mas só para ver o que que esse filme foi legal animação foi muito boa recomendo e ele classificou como negativo ou menos aqui né o acho que essa ativação aqui é tangente perbólica né que deu negativo então ele tá se ficou como ruim então tá destreinado aí vamos aqui a nossa entropia cruzada né já que é uma classificação o otimizador acurácia e aqui o treinamento e aí ao executar o treinamento por 10 épocas vamos ver na curaça subindo 87% ao final de 10 épocas E aí o avaliação né desse modelo mostrando no data 7 de no conjunto de teste 84% de acurácia aqui os gráficos mostrando a curasse ao longo do tempo e o que eu queria mostrar agora que ele mesmo review com o data 7 com a rede treinada ó mesmo review que não tava na data 7 né isso aqui tá foi gerado escrito E aí pedia para edição e realmente deu uma predição bem positiva Pronto né então agora faz sentido o resultado o modelo tá aparentemente bem treinado só uma dúvida com relação ao a implementação naquele código lá em cima Então as duas camadas ali tanto do início para o fim como o contrário estão codificadas de uma naquela parte ali do bidirecional né isso Exatamente exatamente o que que vai ser de pedir direcional uma lsm vai ser colocada nas duas direções nesse sentido beleza e aqui ó aí tá aí o problema eu não vou entrar mais nessa essa parte aqui né de empilhar as as lsdms que aí é uma coisa um pouco mais avançada foge do nosso escopo aqui eu só quero abrir agora a discussão tá queria botar um novo slide em branco mas não vai dar vai vir é até dar aqui né no slide em branco

- *Corpus ID:* 2688
- *Score:* 0.8674078583717346
- *URL:* oculto
- *Início:* 01:55:44
- *Fim:* 01:58:03
- *Transcrição:* entre até o grupo Os colegas Podem trazer pontos diferentes mas é esperado que vocês comentem um pouco sobre isso tá então os passos são esses aí eu só sinalizei com alguns detalhes a mais mas pegou bem no espaço tá obrigado de nada até a pergunta do Antônio pode falar Antônio eu posso por exemplo pegar outro teste split é colocar lá os 15% né que seria o de teste eu digo que eu vou estratificar pela variável alvo então ele vai obedecer ali aquela espécie aqueles percentuais riável sobre data 7 E aí eu pego só o teste e a parte E aquele teste eu posso utilizar como treino daí para frente interessante seria seria na verdade usar o Twitter para gerar a tua amostra né eu tenho 130 mil Antes de eu começar a fazer qualquer coisa eu pego eu uso o treino teste Sprint startifico né de colocar a estratégia lá da estratificação que ele vai manter as porcentagens dos valores das classes E aí como eu coloquei 15% no teste o meu data 7 que eu vou chamar de treino é o que ele retorna como o data 7 de Teste será que seria uma boa estratégia essa entendeu Pode ser na verdade eu te comentei conversar usando essa função para gerar a tua amostra dos dados né e a porção que fica para teste que normalmente é o que a gente deixa menor tu vai usar para treino isso depois eu uso para o intestino de novo para fazer aí esse cara treino e teste sim eu acho que sim Acho que é uma boa para garantir a parte das classe estratificação das classes né Tá pessoal mais perguntas aqui sobre o conteúdo sobre o trabalho Se tiver tudo ok vou encerrar a gravação mas eu antes a gente por intervalo eu quero dar mais uma palavrinha com vocês tudo certo aqui posso encerrar essa primeira parte ok

- *Corpus ID:* 6926
- *Score:* 0.8662075996398926
- *URL:* oculto
- *Início:* 00:39:37
- *Fim:* 00:41:50
- *Transcrição:* treinamento na minha cabeça né até até agora por que que eu preciso repetir Não entendi sabe é que por exemplo assim eh eh vamos supor é que assim eh deixa eu tentar imaginar uma situação é que assim disso tu separou tu separou os dados para fazer o treinamento beleza ent entende tranquilo mas vamos supor que esses dados não sejam eh numerosos o suficiente entendeu vamos supor que tu não tem tantas observações e e tu quer tentar mesmo assim fazer um treinamento entendeu Aí eh o fato de tu tá por exemplo usando só o dat sem o repeat vamos supor que tu tenha 1000 dados anotados e tudo mais aí tu vai fazer 1000 aí tu chegou numa determinada acura e fim entendeu o fato de tu poder usar o repeat junto com shuffle depois a gente não tá usando shuffle aqui ainda né vai te permitir replicar aqueles mesmos dados entendeu Tipo assim em vez de fazer 1000 tu vai fazer 3.000 são são os mesmos dados repetidos só que como vai ter um embaralhamento isso vai ter permitir gerar uma riqueza que Talvez melhore a cura do teu modelo um pouquinho mais ainda entendeu então seria nesse sentido entende claro que se tu tem um volume muito grande daí não vai talvez não vha a pena entendeu Tipo assim daí talvez não faz sentido mesmo entendeu tá E aí o shuffle eh impede que o overfitting aconteça porque eu tô oferecendo os mesmos dados um monte de vezes né isso E aí tu tem que certamente fazer o shuffle entendeu tu tem que misturar porque senão vai ser sempre os 1000 na mesma ordem né Aí uma hora vai dar vai tipo assim a rede vai viciar Então o que tu vai fazer tu vai fazer o repeat 3.000 vezes mas desde o início inclusive os 1000 primeiros valores que são originais nesse caso já vão estar embaralhados né tá usando o shuffle tá então isso vai te ajudar aí essa seria uma uma situação onde o repeat seria útil no caso tá e tem tem uma parte aqui do notebook que fala que se forem eh poucos dados pode eh tipo a gente

- *Corpus ID:* 8637
- *Score:* 0.865671694278717
- *URL:* oculto
- *Início:* 00:26:53
- *Fim:* 00:29:22
- *Transcrição:* Preciso assim né Eh os textos são um pouco mais curtos do que o esperar eh e aí o indicador blil ele ser um valor um pouco baixo acabou sendo realista em relação ao que a gente observa na na análise visual e dos resultados né E como recomendações eh a continuação seria fazer a a minha ideia original ou seja treinar de fato o modelo lá com um conjunto diferente Talvez para avaliar o desempenho dele né e e fazer o procedimento completo que não foi possível é por causa do tempo de treinamento e ainda E ainda ter outras avaliações automáticas lá indicadores por exemplo o Spice né o Spice eu falei rapidamente né ele ele aparece Nesse artigo aqui deix Sim tu já falaste aham não tá bem claro Ricardo a gente estourou bastante o tempo mas eu eu nem te eh nem te interrompi não porque teu trabalho eu quero te dar os parabéns teu trabalho é um trabalho complexo tu tá abordando um termo um tema bastante complexo e desafiador e tu estás fazendo sozinho então eu quero te dar os meus parabéns e eh te dizer que eu acho que tu tem um TCC aí tu pode tentar melhorar isso né esse esse indicador o eu eu eu li muito que ele tem alguns problemas então tu até cita que ele foi até realista porque eu acho que tu também já deve ter lido que às vezes ele pode tá baixo mas tu tem boa qualidade né então H acho que tu tem um ótimo trabalho aí e fez além do que era esperado e ainda mais com com o contratempo que tu me comentaste que tu tiveste aí é né de saúde então ah Meus parabéns algum colega ten perguntas pessoal é professora não pergunta mas assim ah de fato o tema é bem desafiador E junta né o conteúdo dessa disciplina com a disciplina de visão computacional show de bola é é é tu tá tu foi Além tá Ricardo muito bom mesmo muito bom apesar da gente ver ali as imagens eh que ele

- *Corpus ID:* 7983
- *Score:* 0.8656668066978455
- *URL:* oculto
- *Início:* 00:52:43
- *Fim:* 00:55:39
- *Transcrição:* aí já né pra gente né Tá mas é vocês tem tempo para fazer vocês conseguiram convencer para fazer com tempo tá ó aqui ó só para voltar aqui agora rodou agora acabou o treinamento Então vamos ver como é que ficou aqui Ahã a acurácia Olha pelo de novo né Eu não cheguei a plotar o o gráfico desse cara aqui mas tá com todo jeito de que eu tinha essa porcaria tá bem em cima tá Ah ó a acurácia de treinamento ela segue seguindo tá tá eh Pelo visto eu tenho potencial de aumentar o número de épocas Tá mas a e Corá de validação ela parece que meio que deu uma estagnada lá em 86 87 86 87 tá então parece que a acurácia de validação já estagnou para essa quantidade de épocas aí tá mas até que o GAP entre a curá de treinamento e a de validação não deu muito grande Tá e aqui tem tem um exemplo aqui de de inferência tá visualmente o resultado tá bem decente né tá aqui a máscara Ground truth a máscara inferida né eu posso mudar aqui o índice da imagem só para ver imagens distintas aí ver o que que acontece também né resultado bem bem Decente isso no nos dados de teste o que interessante né porque a rede não viu essa imagem tá E aí vocês vão ver que tem alguns resultados que de fato não ficam muito legal não tá olha até essa imagem aqui que que é bem Tem vários pontos aí de na na coleira do do cachorrinho ali ele não aprendeu razoavelmente bem tá ele errou um pouquinho na boca do cachorro né então a boca do cachorro ele inferiu como sendo contorno Tá e aí o que a gente pode fazer aqui no final é de fato é Fazer uma avaliação objetiva tá porque claro que avaliação visual ela é eh ela é importante tá elação qualitativa Tá mas infelizmente se eu tenho um dataset com 1 milhão de imagens eu não tenho Como avaliar uma por

- *Corpus ID:* 3130
- *Score:* 0.8655622005462646
- *URL:* oculto
- *Início:* 00:44:06
- *Fim:* 00:46:40
- *Transcrição:* Mas ó ainda não não reduziu não chegou não digamos no mínimo né ainda tá talvez agora enfim o agora tá com aparência de estagnado né porque os números aqui não estão mudando Ou seja a variação tá menor do que 10 a menos três né 10 a menos três é no milésimo aqui ó tem uns milésimos já não estão mexendo né e a curva meio que estagnou mesmo e aqui já tá mais de tipo sem época sem mexer né E aí a gente poderia configurar lá o critério de parada é razoável né supor para esse problema né que a nós quando ela variar menos que 10 a menos três durante sei lá sem episódios é razoável ok agora tem uma outra consideração importante que ficou sem avisar sem notícia aqui né Isso aqui é o tamanho do Bet que que é um bets no nosso caso porque o Bet na verdade é aqui nesse nessa versão do algoritmo né o fórum e o beck propagation é eu faço uma aliás eu faço uma atualização o backpropagation ele é feito para cada Exemplo né para cada linha do meu data 7 eu faço um back propaguete e na outra versão deixa eu resgatar o slide aqui da outra vez só deixa eu abrir ele aqui do nosso descida de Gradiente Deixa eu lembrar que se é esse esse exemplo aqui aqui então decida de Gradiente né para regressão linear quando a gente só tinha um neurônio né então esse era o algoritmo inicializava os pesos e países daquele neurônio aleatoriamente e aí repetir até o critério de parada isso aqui ó para isso para calcular isso aqui a gente mexe atualizava quer dizer percorrido data 7 inteiro calculando o erro médio né Isso aqui é o a média o Diego vezes X1 né para fazer uma atualização de peso para fazer e aqui percorriam data 7 todo calculando a

- *Corpus ID:* 8304
- *Score:* 0.8654137849807739
- *URL:* oculto
- *Início:* 01:31:25
- *Fim:* 01:33:46
- *Transcrição:* se conseguir melhor eu acho que que para para ter uma ideia ver quanto que quanto tempo leva para fazer 100 e e is é é que o tempo que que vai levar é o tempo que ele desconecta é isso que tu tá preocupado né sim sim tanto tanto pra questão de de de limitações do ambiente né quanto para tempo mesmo né tempo tempo de mas olha eu eu acho que se tu subir ele tu faz para é só o tempo mesmo de ficar conectado aí a conta a conta que não paga nada eu acho que ele desconecta tá mas se tu se tu subir o modelo por exemplo tem a Não tô dizendo para vocês fazerem isso né que pode ser como a professora orientou essas centenas mas se tu conseguir subir o modelo na conta de r$ 50 lá 54 eu acho que ele mantém entendi ou faz até onde dá Vai vai Pou é Vai vendo até o onde D num tempo aceitável porque vamos supor que tu faça uma conta Ah vai levar uma semana uma semana é muito tempo porque precisa ainda analisar os resultados Então faz o que dá em sei lá 24 horas entendi OK E aí tu explica lá na tua metodologia Olha o o dataset original tinha x000 resumos e e abstracts a gente fez uma amostra aleatória de de tanto e e todo o trabalho foi feito em cima dessa dessa amostra o que que era o o como é que separou em em foi foi feita inferência para tudo como é que foram que métricas tu usou para paraa tradução se usa Rouge e e BL então o que que foi o que que foi usado entendi OK era uma pergunta era a próxima pergunta que eu ia fazer mas eu acho que eu vou tomar o tempo dos colegas eu mando no fórum né da da melhor métrica de avaliação é rugge e B rugge é bem e as duas T scripts prontos que tu que tu manda lá o o script desculpa manda o o resumo gerado e o resumo de referência o abstracto o que tu tiver usando como o a a resposta correta T mand a resposta

- *Corpus ID:* 6762
- *Score:* 0.8644659519195557
- *URL:* oculto
- *Início:* 01:05:48
- *Fim:* 01:07:52
- *Transcrição:* eh muito grande e uma acurácia da validação muito baixa isso indicaria de que a gente tá fazendo overfitting nos nossos no nosso treinamentos tá tá indo tá ficando viciado da o modelo naqueles dados mas como a gente tá usando a validação para controlar isso né durante o treinamento a gente consegue já observar que a validação a acurácia da validação tá sendo próxima da acurácia do treinamento indicando então que eh que tipo para dados diferentes que aquele modelo nunca viu tá indo bem né Então tá evitando o overfit eh bom então aqui a gente pode treinar de novo né então havia feito sido feito um primeiro treinamento de 50 épocas né E aí a gente pode depois de ter feito isso a gente pode dizer não mas treina mais 10 épocas tá com a mesma quantidade de Bat size tudo mais né Aí ele treina mais 10 épocas e a gente consegue fazer a a mesma visualização aqu ele gerou esse objeto History 2 e e esse objeto History 2 a gente consegue visualizar E aí aqui olhando só as últimas 10 épocas que foram feitas que foi dessa última dessa último Fit que a gente fez a gente consegue ver que realmente tem uma estabilização e que não tá mais valendo a pena treinar tipo assim a gente só tá jogando ciclos de processamento fora aqui porque né mas eh se a gente não tiver satisfeito com esse resultado daí não tem muito o que fazer quer dizer que tipo a gente atingiu o máximo que esse modelo consegue dar de validação de acurácia E aí nesse caso a gente tem que voltar a prancheta de projeto lá voltar a avaliar se nosso modelo dá para fazer algum tipo de alteração nele para melhorar essa acura então teria que fazer uma exploração Alé colega falou ali talvez automática ã depois que a gente fez a a parte de Treinamento a gente pode avaliar o modelo né então para isso a gente tem os


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should provide a relevant context or answer to the question posed in the PROMPT. Supporting Evidence: The RESPONSE states that it is not possible to answer given the context provided. This indicates a complete lack of engagement with the PROMPT, which asks for a method to predict retweets based on a specific dataset. Since the RESPONSE does not attempt to address the question or provide any relevant information, it fails to meet any of the criteria for relevance.  Score: 0


---
