**Pergunta 1**: Como a metodologia ágil pode ser adaptada e aplicada em projetos de ciência de dados, e quais são as vantagens e desvantagens dessa abordagem em comparação com métodos tradicionais como o Waterfall?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 9052
- *Score:* 0.8488333821296692
- *URL:* oculto
- *Início:* 00:48:00
- *Fim:* 00:50:36
- *Transcrição:* importante eh é o propósito né O que vai ser feito isso aí bem mas de qualquer maneira eu acho interessante entender melhor a diferença até para porque pelo menos na comparação aqui no slide né tem aquela escala de Rigor científico né para entender um pouco melhor a diferença tá ótimo eh e aí o principal já Aproveitando né a pergunta do Ant que a que é né o resultado aqui dessa disciplina é a proposta então é que que vai ter na proposta vou ter Qual que é o meu objetivo Qual que é o problema né social que eu resolvo qual que técnicas eu vou usar como eu vou comparar né então pode cair em um daqueles tipos pode eh contemplar mais de um assim né então aqui é só paraa gente ver essa diferença mas na hora h de propor o trabalho aí né Eh a gente vai focar no objetivo do trabalho Ok obrigado aqui só daí para mostrar para vocês né então esse aqui do César ele ele era economista né então ele trabalhava na da prefeitura pegou dados de acidentes de carro de trânsito e daí ele fez eh vários cruzamentos né então ele pegou eh tinha os dados da localização dos acidentes aí ele cruzou eh num período acho que de 2is anos então ele cruzou com a a os dados de previsão de tempo né para ir com dados de calendários então ele queria analisar Por exemplo quando tá chovendo quando tá sol se quando tem mais acidentes né então sei lá quando tem tá chovendo tem mais acidentes mas em muitos pontos tinha mais acidentes Quando tava só porque as pessoas corriam mais Então essas análises ali que ele fez né E também cruzou com dados de do calendário para olhar assim feriados quando tinha um feriado mais estendido se tinha mais acidentes ou não então ele fez o cruzamento dessas três bases de dados e daí o usou né algumas técnicas de mineração de dados Então como regras de associação daí para

- *Corpus ID:* 5442
- *Score:* 0.848158597946167
- *URL:* oculto
- *Início:* 00:58:37
- *Fim:* 01:00:44
- *Transcrição:* eh comparações faz eh joins esse tipo de coisa entendeu é bastante fácil de fazer né eh não sei se tu tu vendo o SQL lá onde tu vê todos aqueles comandos SQL então Tha divers dá uma forma de programar em SQL eh a assim muito alto nível muito alto nível que fica muito mais simples é mais rápido entendeu E conciso sabe tu coloca ali só o que realmente precisa para tu fazer as transformações dos seus dados entendeu E então depois se vocês quiserem dar uma olhada né tipo tem o o código dessa ferramenta é aberto só aqui no github só procurar tar vz no meu user queor E aí vocês vão em source aqui daí uma olhar no código vocês vão ver os workflows que tem aqui é é é realmente bastante eh eh pequeno tá bom H obrigado Beleza então acho que era isso então sobre a parte de apresentação da disciplina tá então vimos o cronograma vimos o plano aí de como é que feito a avaliação a biografia ã acho que agora então vamos pr pra aula propriamente dita antes de de de abordar isso eu só queria perguntar para vocês Em que momento mais ou menos vocês têm feito a pausa assim em torno das 10 horas isso mais ou menos em torno desse horário em torno das 10 horas perfeito Beleza então então bom então eh deixa eu só trazer o próximo material aqui esse aqui bom então agora a gente vai fazer então uma uma rápida introdução ao processamento de autodesempenho tá onde que a gente começa a ver as as intuições então e a programação paralela é uma coisa que existe bastante tempo né então só que antigamente era bem mais difícil de fazer hoje em dia ela se tornou eh digamos assim ela permeia meio que tudo né desde o celular que tá no bolso da gente até o cluster de computador com um monte de nó né então H lá antigamente existiam né as máquinas sequenciais aquelas máquinas a gente tinha um único núcleo de processamento e por consequência a gente conseguia fazer só

- *Corpus ID:* 6044
- *Score:* 0.8476148843765259
- *URL:* oculto
- *Início:* 01:02:53
- *Fim:* 01:04:54
- *Transcrição:* eh seria o ideal digamos assim né Então aqui tem uma outra razão possível eh é o fato da distribuição não ser homogênea dos dados né então por exemplo tem algumas tarefas que elas vão ter mais eh que ficar com blocos eh que T mais anos por exemplo vai fazer com que eh tenha que se comunicar com mais eh tarefas de redução né então alguns anos T mais medidas que outros isso sugere então um desbalanceamento né porque tu imagina tu vai ter uma chave nesse volume de dados que é muito mais presente do que outra se tu tem um uma tarefa de mapeamento que tem poucas Chaves ela vai ter que se comunicar menos né E por por consequência vai ela terminar mais rápido né do que ah do que uma outra tarefa de mapeamento que teve nela muito mais Chaves diferentes mais anos Diferentes né Isso vai fazer com que essa tarefa de mapo tem que se comunicar com mais tarefas de redução Então não é só um equilíbrio do do cálculo mas é um equilíbrio também da comunicação que ela vai tá acontecendo todo mundo com todo mundo né a gente gostaria que ela fosse equilibrada também né que todo mundo enviasse a mesm volume de dados porque daí todo mundo termina mais ou menos ao mesmo tempo né então Eh mas de qualquer forma eh eu teria que pegar esse caso né o e e e realmente assim tu colocar sondas né na na no rup ali o rup tem formas de a gente extrair essa informação né quando que começou e quando que terminou cada tarefa de mapeamento quando e quando terminou o shuffle de cada uma da da dos mapeamentos né E quando que começou e quando que terminou cada reduce né e observar esse gráfico de espaçoo né para tentar entender onde é que tá tendo onde é que tá o problema que vai explicar Então por que que a gente não ficou tão 10 vezes mais rápido eh Bom eu acho que era isso então só passar rapidamente aqui antes da gente ir pr pra nossa atividade dirigida tá

- *Corpus ID:* 5471
- *Score:* 0.8473694324493408
- *URL:* oculto
- *Início:* 00:03:06
- *Fim:* 00:05:00
- *Transcrição:* a gente vai olhar pô mas aqui eh a gente tem uma uma série de estratégias que podem ser usadas para melhorar essa performance né como foi citado aí agora a pouco né eh ah usa tabelas temporales depois você né gera uma final Eh vamos vamos dizer assim modularizar aí né o a tua consulta né ou uma aplicação spart que seja né eh ah assim aí eu fiquei na dúvida Qual a gente tem alguma ferramenta que a gente vai conseguir trabalhar aí no no no laboratório né no nosso ambiente de de trabalho aí acadêmico eh que consiga identificar essa essa questão né de o que que eu posso melhorar aqui na minha aplicação ou se realmente esse problema é é grande suficiente a ponto de eh eu não consigo mais espremer a aplicação é e assim não existe uma ferramenta mágica que te diga o problema está aqui entendeu pontualmente assim na realidade existem ferramentas que te trazem indícios entendeu então por exemplo na linguagem r quando tu tem um workflow lá que uma série de passos de transformação né tu pode usar um profiler esses profilers eles existem também em outras linguagens como Python né e Normalmente eles indicam assim ó essa essa query aqui ó Exatamente esse comando aqui tá levando tanto tempo entendeu ou tá levando sei lá 60% do tempo e aí tem muitas coisas assim que de vez em quando assim do ponto de vista sequencial só sequencial tá Talvez tu tenha como fazer isso que tu mencionou Ah vou criar uma tabela temporária vou fazer alguma coisa preliminar entendeu Vou fazer uma outra forma essa minha transformação e resolve Tá mas isso não é o foco dessa disciplina o foco dessa disciplina é que tu T já alguma coisa eficiente sequencialmente e tu quer ficar fazer ela ficar mais rápida entendeu E aí as escolhas não são exatamente assim a esse comando aqui entendeu as escolhas são mais assim em que Quais são os pedaços que foram tipo assim eu tenho o meu volume de dados

- *Corpus ID:* 5407
- *Score:* 0.8472054600715637
- *URL:* oculto
- *Início:* 00:01:48
- *Fim:* 00:04:00
- *Transcrição:* assim digamos a uma perspicácia assim de perceber Como que eu posso aproveitar o processamento paralelo para melhorar os meus workflows de de dados digamos assim né Então acho que eu vou começar por me me apresentando contar um pouco assim um pouco da da minha história como é que eu vim parar aqui ah deixa eu só ver aqui a lista de vocês então eu vou maximizar agora isso então apresentar me apresentar rapidamente Então antes da gente começar então meu nome é Lucas Melo schnor ã eu sou professor aqui da UFRGS eh desde 2013 então já faz aí 10 anos que eu tô aqui na URC um pouco mais até eh a minha formação acadêmica Então olha acompanhando os slides aqui ela é bastante focada em Ciência da Computação então eu me formei na Federal de Santa Maria em 2003 bacharelada em ciência da computação e logo em seguida fiz um mestrado eh em computação aqui na UFRGS em e termido em 2005 e depois eu me engajei num doutorado de 4 anos né E esse doutorado eu tive a felicidade então de ter um ador que me estimulou bastante a trabalhar em nível Internacional e aí por conta disso eu acabei fazendo um doutorado franco-brasileiro digamos assim né que é quando a gente se matricula também numa instituição lá fora nesse caso que foi na cidade de grenoble na França e lá e aí a gente faz esses eh digamos assim um único doutorado assim conjunto entre as duas instituições com um orientador aqui e um orientador lá né então foi uma experiência bem legal me estimulou a aprender francês também e eh durante esse período e e me trouxe digamos assim uma uma visão ã complementar a a visão que eu tinha aqui da como fazer ciência esse tipo de coisa então isso foi há bastante tempo como vocês podem ver né mas eh desde então então eu acabei fazendo depois antes de ingressar na ufgs né Eu Fiz

- *Corpus ID:* 4906
- *Score:* 0.8458198308944702
- *URL:* oculto
- *Início:* 00:46:03
- *Fim:* 00:48:19
- *Transcrição:* embora você você também possa fazer diferentes filtragens no Google e dizer eh eh qual tipo de texto livre que você tá procurando mas aqui você já tem categorias pré-definidas que você tá querendo buscar então se você tá querendo uma coisa específica que por exemplo com dados de clima né aí você clica aqui e aí ele vai fazer uma filtragem e vai acesso a dados de né E aqui ele vai dizer qual é o órgão e quais são as tags que são os textos lives que são colocados Associados Então esse tipo de de interface é bastante interessante né para ser feita também parecido com teu diga pode falar Alexandre Desculpa eu não tinha visto sua mão levantada não eu levantei agora eh como eu sou Lego eu não tenho ideia da quantidade né Essa e o senhor falou que uma uma planilha na nuvem já poderia ser utilizada para para montar um um quadro desse então assim e qual é a ideia mais ou menos de quantitativo assim que é válido para para esse tipo de visualização né Eu perguntei na aula passada sobre gráficos O senhor falou entre 5.000 e 20.000 registros não foi isso Eu não lembro assim exatamente mas especificamente sim sim sim ó só para vocês terem uma ideia esse aqui ele ele tá um pouquinho lento vocês viram que ele demorou um pouco para carregar Mas ele tem 100.000 registros tá vendo aqui aqui em cima né esse é o que tá rodando aqui né Eu não sei se é porque essa é a versão a versão que ele ele tem um serviço que ele vende né Aí você basicamente você tem que fazer uma um pagar pagar mensal para para poder usar eh dados de planilhas que são que são protegidas né a versão gratuita que eu uso é só quando você tem um Google sheets que que é público né você pode compartilhar o link com pessoas e as pessoas podem pelo menos ver né mas não é um caso corporativo né no caso corporativo onde vocês têm dados que são sigilosos vocês não podem usar ou

- *Corpus ID:* 2530
- *Score:* 0.8453941345214844
- *URL:* oculto
- *Início:* 01:17:44
- *Fim:* 01:19:45
- *Transcrição:* desse atributo em relação àquela classe Então se nessas instâncias estão nos dados de teste ou de validação a gente acaba tendo vazamento de dados mas o efeito não é tão grande porque como você está comentando a gente não faz avaliação e termos do desempenho né então o desempenho atrelado a um modelo então a técnica de baseado em filtro eu diria que aquela mais tolerável da gente fazer fora da validação cruzada porque ela seria menos sensível a esse tipo de tá E até porque ela também é uma técnica a gente usa muito com grandes volumes de dados E aí fazer Deixa eu só voltar aqui fazer esse tipo de abordagem com grande volume de dados a gente mesmo que obviamente não vou usar a livonaldo eu vou usar por exemplo tendo folder validation mas acaba sendo muitas vezes inviável em praticável assim porque exige muito tempo né computacional então às vezes a gente não consegue arcar com esses custos ou com o tempo né para fazer esse tipo de experimentos então por isso que às vezes a gente acaba muitas vezes simplificando algumas coisas da metodologia que a gente sabe que não é a melhor forma de fazer mas a gente não consegue arcar com o curso de fazer tudo da forma correta ou né do ponto de vista sem vazamento de dados então por exemplo do ponto de vista de pesquisa quando a gente está fazendo a gente tenta ver assim o que que vai minimizar o meu Impacto o que que vai ter o menor impacto em relação a pergunta de pesquisa que eu tô querendo fazer né então se eu tô querendo comparar a questão de algoritmos por exemplo bom então vou fazer uma baseada em filtro antes em todos os meus dados e eu vou dizer Olha foi feita essa forma porque meus dados eram muito volumosos e porque o nosso objetivo era comprar outros aspectos e não a sensibilidade

- *Corpus ID:* 4995
- *Score:* 0.8452863693237305
- *URL:* oculto
- *Início:* 00:37:42
- *Fim:* 00:40:15
- *Transcrição:* em artigos para tentar desenvolver uma metodologia para você desenvolver projetos né então tem um pouco de digamos assim de de análise de subjetiva deles de de de como que eles acham que esse os projetos têm que ser desenvolvidos né E aí eles tem um um um plot um diagrama que é que é bem interessante que eles colocaram no começo do trabalho deles que é um tem basicamente dois eixos assim que é quando você vai olhar para um problema tem a a localização dos dados ou da informação e a claridade das tarefas o que vocês veem eles estão relaciodo duas pontas daquele triângulo que a gente estava falando né os dados e as T né E esse esses eixos aqui eles eles vê eh basicamente da esquerda paraa direita eh São definidos da seguinte forma Então os dados ou eles estão só na cabeça do usuário isso é não tem uma representação a armazenamento físico Ou eles estão completamente armazenados no computador e qualquer coisa que entre da esquerda paraa direita é uma transição que vai de totalmente no computador ou totalmente na cabeça da pessoa ou pedaço né da mesma forma ele define as tarefas desde as tarefas que são nebulosas faz aqui né a palavra e crispy crispy shar e eh precisas né são são aqui dezas imprecisas precisas e algo nessa variação E aí eles tento identificar aonde que projetos de visualização de dados podem atuar nesse nesse digamos assim nesse diagrama 2D Então tem um começo aqui uma área aqui que hum eles identificam que não porque ou não tem dados porque os dados estão mais na cabeça dos usuários tem poucos dados né Então nesse caso aqui sem dados não tem como você tentar fazer algum tipo de automatização tipo de análise visual sem dados você não vá à frente e não importa se a tarefa tá bem especificada ou ou tem não tá não tá clara isso aqui não então essa parte aqui então a partir de

- *Corpus ID:* 2141
- *Score:* 0.8447850942611694
- *URL:* oculto
- *Início:* 00:57:49
- *Fim:* 01:00:00
- *Transcrição:* ser quanto a isso O que eu vou observar mais essa questão né Desse esforço de vocês em aplicar esses potec tipo implementar fazer essa coleta das métricas de forma repetida né para gerar uma distribuição e o esforço para reportar e interpretar isso né através de gráficos de tabelas e do próprio relatório discutindo um pouquinho o que que vocês estão tão encontrando tá então realmente não precisa se preocupar quanto a isso se o desempenho tiver abaixo do que vocês gostariam tá robertoson tem uma pergunta pode falar se a gente não encontrar o data certa vai ficar meio o que a gente não vai ter o resultado aí que a gente não tem necessidade fica difícil a gente avaliar Qualquer coisa se o estado não tão assim fornecendo pra gente conclusões que a gente possa reportar né mas fica meio que não Loop Infinito aí será que seria Qual é a sugestão da senhora Eu não entendi bem a tua Desculpa eu não entendi bem a tua dúvida é desses desempenhos forem ruins o resultado com os dados que eu tenho né porque os dados pode ser tão tão ruins assim tão difícil que a gente não pode chegar nesse ponto não olha até pode acontecer né nesse caso vocês não vão ter obviamente tempo para revisar essa escolha né e fazer de novo se vocês uma das possibilidades é vocês por exemplo quem tá usando kega ou às vezes não quer igual tem notebooks que as pessoas avaliaram vocês podem ter uma ideia pelo menos né Para onde os projetos Olha tem pessoas que fizeram coisas que parecem produtivas aqui tá mas na pior dos casos você chegarem nessa conclusão né O que daria para a gente pensar sem função do tempo é isso vocês vão ter sei lá 10 modelos sendo avaliados vocês vão reportar os três menos piores assim olha né de uma forma geral os resultados

- *Corpus ID:* 4699
- *Score:* 0.8445695042610168
- *URL:* oculto
- *Início:* 00:30:31
- *Fim:* 00:32:55
- *Transcrição:* ao invés de sempre comparar com os mesmos dias a gente permitia com que essa janela de comparação de 41 dias ela deslizasse ou para cima ou para baixo e aí a gente conseguia achar os anos mais próximos usando essa janela deslizante e nesse caso aqui Ah o mais próximo que eles que que a gente achou foi 2011 com c dias após Então você conseguia achar informações diferentes a gente não nenhuma digamos assim aplicação dessas análises para nenhum contexto agora falando para vocês eu fico pensando uma possível aplicação disso seria por exemplo para para vindima de de de vinho quando você tem que fazer a coleta de uvas se você tem uma monitoramento da tua região e você CONSEG entendeu os padrões Talvez isso Possa possa te dar um indicativo de qual seja o melhor dia de você fazer a a colheita por exemplo você a cada ano você tem condições climáticas diferentes você tem né níveis de chuva e coisas do gênero de repente eh você teve uma safra muito boa de um determinado ano e essa safra muito boa ela ela tem vários fatores que influ cam né os fatores climáticos mas também tem essa questão de quando que você faz o processo de coleta das uvas né não tinha pensado nisso Olha que eu tô trabalhando nesse negócio há muito tempo a sempre quis fazer um projeto na área de vind aqui eu vou ver se vou tentar entrar com em contato com o pessoal aqui no Rio Grande do Sul tem muitas muitas vinículas aqui para saber se eles teria o interesse em algo parecido como com isso aqui bom eh agradeço vocês estão por falar sobre o projeto e mostrar Bom enfim Ahã Então a nossa interface ela ela era interativa né permitia você poder trocar os anos e essa parte de combinar a interação com a visualização é bastante importante Principalmente quando você faz também eh configurações do algoritmo você você faz filtragens você quer


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to all parts of the PROMPT, including the adaptation of agile methodology in data science projects and the comparison of its advantages and disadvantages with traditional methods like Waterfall. Supporting Evidence: The RESPONSE does not address any part of the PROMPT. It states that it was not possible to respond, which indicates a complete lack of relevance and engagement with the question posed. Therefore, it fails to provide any relevant context or information regarding agile methodology, its application in data science, or a comparison with Waterfall.  Score: 0


---
