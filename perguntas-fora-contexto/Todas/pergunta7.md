**Pergunta 7**: Como o escalonamento de dados (normalização ou padronização) afeta a precisão de modelos preditivos para séries temporais?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 1709
- *Score:* 0.8781948089599609
- *URL:* oculto
- *Início:* 00:24:31
- *Fim:* 00:26:43
- *Transcrição:* eles vão ficar mais bonitinhos ali também em torno e sei lá de repente em volta do zero eu tenho que eu não sei se a palavra é essa Mas eu tento reduzir como é que eu vou dizer o que eu vou transformar o que eu quero que preciso transformar eu preciso transformar para que digamos assim eu não tenho um valor de menos mil até um milhão eu tento tentar fazer com que os valores se encaixa ali próximos do zero e alguma transformação vai fazer isso para mim exatamente o tipo de transformação depende do dado da variável né agora o cara quer que a gente teria que cuida é não mas mesmo variável numérica né O que a gente teria que cuidar é só essa questão de transformações juntas por exemplo a gente tem que lembrar que o mi Max vai colocar entre 0 e 1 o stander scaler que é esse da padronização aqui apresentar de menos dois a dois depende totalmente dos dados então isso a gente só tem que a gente tem que cuidar um pouco porque se a gente tá usando um algoritmo que é sensível a escala se eu usar duas padronizações diferentes eu posso estar ainda com o mesmo problema então que é exatamente transformar numa distribuição original mas se eu tiver uma variável que ela varia de menos três a três que não é muito absurdo a gente pensar não não não não na verdade não é inscrito se tiver uma variável que ao se transformava varia entre -3 e a outra eu apliquei um Max que vai variar entre 0 e 1 eu continuo tendo escalas diferentes e isso para este tipo de algoritmo por KNN para rede neural vai continuar gerando o mesmo tipo de problema tá então é só isso que você isso que a gente tem que cuidar Ou para processar os dados mas assim isso dá mesma escada na mesma escala exato exatamente por isso que na prática o que a gente faz é

- *Corpus ID:* 1706
- *Score:* 0.8777875304222107
- *URL:* oculto
- *Início:* 00:19:42
- *Fim:* 00:22:04
- *Transcrição:* fronteira com a simples transformação com base nesse nessa estratégia de padronização tá então enfim poderia usar o Max também a gente nesse momento dessa disciplina a gente também não vai discutir tão a fundo todos essas possibilidades para processamento Mas a questão é vocês garantirem que as escalas aqui são comparáveis tá então uma das formas é usar o Max que transforma entre zero e um ou esse de padronização que calcula esses lei score então todo o valor vai ser dado né com a distância dele para média tá menos ou mais que a média se você tem uma variável uma variável numérica qualquer porque você falou padronização né que é exatamente transformar lá no centro ou seja pegar cada variável cada valor dessa dessa variável e aplicar lá você discorda transformar numa distribuição normal aí a minha dúvida é a seguinte eu posso mesmo que a distribuição não seja normal e eu não tô falando que não é normal no sentido dela ser aproximadamente normal não é realmente não não ser normal De forma algum tem bem diferente é normal eu posso aplicar a padronização nela para fazer isso aí ou eu tenho análise uma igreja normalização então tem alguns casos que essa mudança pode acabar interferindo um pouco no desempenho do algoritmo certo assim no sentido de ser bom a gente está fazendo a transformação que não é mais adequada para o dado então isso pode acontecer tá se isso quando eu falo isso eu também falo o seguinte às vezes aplicar o aplicar lá a escala de blog porque outra coisa também que eu percebi é o seguinte toda vez que que você faz lá o teu você faz lá por exemplo ser histograma e que ele começa ali perto do zero perto do eixo Y ele tá bem alto e ele vai reduzindo até que conforme aumenta ele ele vai ficando bem os valores vão sendo contados para bem poucos É nesse caso

- *Corpus ID:* 3437
- *Score:* 0.8736684322357178
- *URL:* oculto
- *Início:* 01:09:40
- *Fim:* 01:12:13
- *Transcrição:* prontas aqui e a gente vai poder ver e pegar mas por enquanto tá o a motivação desse material é para mostrar né que tem existem técnicas que viabilizam o treinamento de redes muito Profundas uma delas é o Beth normalization e uma coisa é que assim ó a gente sabe que normalização é importante e normalmente ela é feita para toda a data sede né normaliza todas as imagens lá e divide né fica a escala toda é faz toda numa escala só né ou seja né a normalização gente é uma data 7 inteiro lá deixa eu desenhar aqui um quadrado mais bonitinho aqui inteiro E aí tem as imagens ou os pixels ali né X1 até X N se for uma tabela né são as colunas se for uma imagem são os pixels e aqui o alvo né a classe e aí normalmente a gente faz a normalização pelo no data 7 inteiro né então eu pego todos os valores do X1 aqui e vou e faça normalização por exemplo a normalização por padronização ou por escala de mini Max né Se for por padronização que é no caso aqui ó padronização cada valorzinho aqui né Cada valorzinho x né o novo X vou chamar de X chapéu aqui ó recebe o x original menos a média de toda aquela coluna né então a média de tudo aqui dividido pelo desvio padrão né O desvio padrão do X e aqui é isso é padronização E aí quando eu faço isso Os Novos Valores dessa coluna toda vão estar né vão ser né ditados por uma distribuição normal comédia zero e desvio padrão não é que vão estar entre zero e um né porque o que fazer eu me Max que ele né que seria esse aqui ó x recebe o atual menos o mínimo o x mínimo dividido pelo máximo - x-me Então essa é o escala né Por mínimo máximo my Max scale e Aqui é padronização aqui porque se eu coloco tudo entre 0 e 1 pode ser pode ser um problema com outlanders né que eu tenho sei lá todos

- *Corpus ID:* 2571
- *Score:* 0.8731560707092285
- *URL:* oculto
- *Início:* 00:44:53
- *Fim:* 00:47:10
- *Transcrição:* então você chega lá por favor uma coluna dessa que tem dependesse Yes aí ele tem menos 065 e tenho 1.52 como é que eu vou saber o que é que sim ou não aí tem dependente sim ou não Qual que é assim aí eu não sei aqui a gente acaba perdendo quando a gente faz essa questão do de normalização geral né a gente perde um pouco essa essa Claro se eu tivesse fazendo em Max normalização em Max eu não precisaria transformar os binários eu transformaria só os numéricos para eles estarem quiserem um então a gente acaba perdendo aqui essa essa interpretabilidade tá Por uma questão é por uma questão lógica esses aqui os negativos ele seriam esses aqui é negativo para essa classe uma questão que a gente está falando de média e desvio padrão Acima da Média Tá mas assim a gente tá falando isso uma variável e vim aqui categórica mas de uma forma geral se torna difícil é a gente ter essa interpretação bem direta tá [Música] a gente tinha usado primeiro a gente tá no nosso data 7 a esse mesmo tá a gente tinha usado no nosso trabalho a primeira vez o stander né para padronizar a ele ficou bem a gente achou bem estranho né sim ou não os outros valores ficaram estranhos aí a gente usou o mi Max aí ele não mexeu no zero e nenhum né como a senhora falou nesse caso a gente tem que usar essa padronização pessoal tudo aqui ou algum Max resolve Por que que a gente tá usando isqueiro por causa do PCA se vocês não aplicar em PSA não precisa a gente tá usando standar scanner porque o PCA ele precisa dessa transformação para centralizar essa Distribuição e evitar efeito indesejado né dessas

- *Corpus ID:* 1704
- *Score:* 0.8730915188789368
- *URL:* oculto
- *Início:* 00:16:24
- *Fim:* 00:18:38
- *Transcrição:* valores a menos de C porque isso já me reduzem tantas combinações Ao Total para que o meu notebook Rode mais rápido e aí depois tem funções do próprio site tem possibilidade é de fazer isso com funções que são mais eficientes nessa busca nessa otimização de se preparar metros Tá mas o svm ele envolve esse custo de experimentação que ele é relativamente alto ficou Claro mais uma que é complementar mais alguma pergunta Então tá chama atenção para vocês da importância dos dados normalizados tá quando a gente usa o svm o svm assim como KNN a gente precisa normalizar os dados tá E aí é comum a gente usar um tipo de normalização não é que eu posso dizer para vocês não é a única que pode ser utilizado mas existe um tipo de normalização que é chamada de padronização tá quem já usa o secplane e a gente também vai vai trabalhar em alguns notebooks É o stander que a gente tinha usado nem Max scaler e é um tipo de padronização que ela basicamente faz com que a distribuição do meu dado numérico ele tenha média zero desvio padrão então ele calcula o que a gente chama de um z score tá é comum ser utilizado isso provavelmente se deve ao fato de que artigos por exemplo foi avaliado e isso e percebido que para esse algoritmo geral o standar scaler que é essa padronização funciona melhor tá então isso Às vezes a gente acaba tendo essas essas Como é que vão ser essas orientações em relação que é mais recomendado é muito em função do que se observa experimentalmente tá E aí é só isso é importante que vocês tenham em mente porque quando a gente tem essas diferenças de escala como no fim das contas o que o algoritmo tá fazendo também tem relação com aquela ideia de distância entre entre instâncias porque eu tô vendo os vetores de suporte esses vetores suportes são

- *Corpus ID:* 3652
- *Score:* 0.8712642192840576
- *URL:* oculto
- *Início:* 00:05:48
- *Fim:* 00:08:20
- *Transcrição:* próximo digamos assim mais mais uma escala mais parecida E aí eles puxam um pouco as vantagens vantagens desse método e também do método que até foi Foi questionado sugerido um dos colegas que seria homem mais também tá colocado lá então é basicamente a gente pode talvez talvez contrastaram os dois né E resumir as diferenças através dessa tabela que essa tabela também tá bem colocada pontuando os elementos principais né Então aqui tem o qual é o funcionamento ou a forma método que é usado para fazer a padronização e e quando quando quando você indica que se usaram um deles ou outro né então ele tem escala diferente quando a gente quer garantir que tem uma mesma uma média zero padrão padronizado pela pela Unidade de padrão as faixas de valores que ficam E aí quer talvez importante não é para costelização isso pode fazer uma grande diferença que essa questão dos outlaios né então mas a gente nós vamos com isso né mas existem aqueles pontos permitir fora da curva né então chamados formalmente de outlast fogem da curva de distribuição atual fora lá dos extremos E aí justamente seu uso método de mim Max e esses elementos Então são fora da curva estão eventualmente distantes né mais próximo do máximo um domingo é justamente vão afetar esse valor mais com valor mínimo que acaba não sendo um valor real em relação à distribuição original dos elementos né comuns Aquela aquele convite dados então usando mimax a gente tem um efeito de que esses Outlets podem afetar essa Distribuição e talvez [Música] transformar de maneira não muito correta enquanto que a padronização que nós vimos ela não tem esse problema aí tem outros

- *Corpus ID:* 1705
- *Score:* 0.8704444766044617
- *URL:* oculto
- *Início:* 00:18:08
- *Fim:* 00:20:14
- *Transcrição:* tendo essas essas Como é que vão ser essas orientações em relação que é mais recomendado é muito em função do que se observa experimentalmente tá E aí é só isso é importante que vocês tenham em mente porque quando a gente tem essas diferenças de escala como no fim das contas o que o algoritmo tá fazendo também tem relação com aquela ideia de distância entre entre instâncias porque eu tô vendo os vetores de suporte esses vetores suportes são vetores de atributos de entrada eu quero vetores que estejam os mais próximos entre si né classes diferentes mais próximos entre si eu quero calcular uma fronteira de decisão que seja aqui distante esses pontos Então sempre que eu tô falando de distância escalas diferentes de dados impactam muito tá então é para o SPM a ideia de dados normalizados E aí esse exemplo aqui que eu tirei da referência do livro mostra claramente a diferença né se eu tenho dados que não estão padronizados tá então aqui eu tenho os vetores de suporte e dados que estão padronizados a gente percebe que mesmo mesmo que eu tenha uma boa separabilidade aqui entre esses exemplos tá que eu tenho uma classe quadrada classe círculo a gente percebe que ainda assim o fato de eu não ter usados padronizados gera uma fronteira de decisão que não é interessante nesse caso né porque ela é muito envezada por essa diferença de escala que tá falando de 0 a 80 que de 0 a 6 E aí percebam como que fica essa fronteira com a simples transformação com base nesse nessa estratégia de padronização tá então enfim poderia usar o Max também a gente nesse momento dessa disciplina a gente também não vai discutir tão a fundo todos essas possibilidades para processamento Mas a questão é vocês garantirem que as escalas aqui são comparáveis tá então uma das formas é usar o Max que transforma entre zero e um ou esse de padronização que calcula

- *Corpus ID:* 3614
- *Score:* 0.8696786165237427
- *URL:* oculto
- *Início:* 00:41:45
- *Fim:* 00:44:12
- *Transcrição:* plotei aqui né esse algoritmo ele faz isso ele cria duas figuras duas sub figuras uma figura maior numa delas eu coloco as os valores originais né dimensões originais e no outro eu coloco as escalonadas e peço para eles mostrar as categorias né resultantes Então se vocês compararem um com outro é claro que eles foram escalonados para ficarem no mesmo tamanho Mas dá para ver aqui que no original variam né entre menos 11 e 16 no y e menos menos 11 e 11 no x e aqui no escalou nada ele variou entre menos dois e dois menos dois e dois as duas dimensões mas especialmente não a configuração ela fica muito próxima da original a distribuição dos elementos ela continua igual Então esse tipo de escalonamento não vai não vai fazer não vai causar nenhum prejuízo né para a identificação das distâncias entre os elementos ela ela permanece né Ela é mantida então eu vou conseguir ter um intervalo que é que é mais adequado para fazer as medições e eu não deformei fez uma deformação mas Eles continuam seguindo as mesmas proporções né essa esse exercício essa figura foi só para mostrar isso e bom agora nós vamos tentar aplicar o algoritmo de camélias por favor deles tem uma dúvida então né padronização a gente Manteve o formato original dos dados a pergunta é e com outras formas de padronização sei lá uma mimax da vida ela Faria deformação ou manteria essa constante é como em Max ele vê ele provavelmente ele vai deformar um pouco né porque se uma dimensão a gente pode experimentar agora aqui mas se uma dimensão varia entre 0 e 100 e a outra entre 0 e 10 ele vai provavelmente ele vai aproximar mais essa dimensão que é maior ela vai ficar mais próxima né então provavelmente os elementos ficaram um

- *Corpus ID:* 3613
- *Score:* 0.8693009614944458
- *URL:* oculto
- *Início:* 00:39:43
- *Fim:* 00:42:27
- *Transcrição:* reajuste de maneira que todos eles acabam tendo a mesma média no ponto zero e no máximo dizia o padrão de um então eles vão acabar ficando aí entre vai fazer um ajuste né Considerando o desvio padrão específico de cada dimensão e colocando a média o centro no ponto zero mesmo que eles não tenham então com isso eles acabam ficando dentro de um de um padrão né que fica mais fácil de eu aplicar alguma métrica de distância e essa e esse essa padronização é interessante porque independente da distribuição que tem uma quantidade ele vai para cada uma delas avaliar o desvio padrão específico de cada de cada dimensão e considerando esses desvio colocar o centro no ponto zero então aqui em seguida né só para nós podemos fazer uma comparação eu criei um uma tabela um Data Frame não é nesse data foi eu coloquei tanto as dimensões originais quanto as escalonadas E agreguei também o resultado a qual cluster pertence e aí mostra quem não dá para ver os 200 pontos qual era o x e o Y original e qual é o x e y ajustado e a etiqueta né o câncer correspondente a categoria correspondente que eles que eles ficam dentro do quantidades que foi gerado por aquela função nós vamos trabalhar em cima dele então primeira coisa que eu que eu que eu quero mostrar para vocês é que esse escalonador aí o padronizador ele ele parece no servir bem porque ele não não deforma o conjunto de dados né então eu plotei aqui né esse algoritmo ele faz isso ele cria duas figuras duas sub figuras uma figura maior numa delas eu coloco as os valores originais né dimensões originais e no outro eu coloco as escalonadas e peço para eles mostrar as categorias né resultantes Então se vocês compararem um com outro é claro que eles foram escalonados para ficarem no mesmo tamanho Mas dá para ver aqui que no original variam né entre menos 11 e 16

- *Corpus ID:* 2572
- *Score:* 0.8667601943016052
- *URL:* oculto
- *Início:* 00:46:37
- *Fim:* 00:48:50
- *Transcrição:* ele não mexeu no zero e nenhum né como a senhora falou nesse caso a gente tem que usar essa padronização pessoal tudo aqui ou algum Max resolve Por que que a gente tá usando isqueiro por causa do PCA se vocês não aplicar em PSA não precisa a gente tá usando standar scanner porque o PCA ele precisa dessa transformação para centralizar essa Distribuição e evitar efeito indesejado né dessas diferenças de escala da distribuição Então se vocês não estão usando o PCA vocês podem usar o Mi maxcaler tá isso é importante ficar claro aqui E no caso assim por exemplo só 01 e outras colunas ele varia zero um e dois até 01 de Se não me engano são três colunas ou quatro colunas de variação no máximo isso afetaria muito o resultado do PSA do PC porque a gente tem aqui a gente na verdade o problema aqui isso poderia afetar assim qualquer algoritmo só PSA mas o problema é que a gente tem várias que são numéricas né então a gente precisa normalizar porque que os valores Sei lá posso ter milhares aqui né então a gente precisa normalizar tá mas se eu tiver uma variável categórica que umas variam 01 a outra varia de 0 a 4 0 1 2 3 4 eu também tenho que normalizar porque se eu usar um KNN por exemplo isso faz diferença ou um svm enfim Ok vamos adiante o pessoal para a gente chegar na parte aqui do PCA aqui que eu queria aqui é a parte nova que eu queria que a gente mostrar para vocês né discutir um pouquinho como é que funciona vocês vão ver que a estrutura do PCA é muito parecido com tudo que a gente vem usando do ponto de vista das funções para processamento das funções de treinamento de modelos enfim tá a gente vai primeiro trabalhar com PCA tá análise componentes principais distração de atributos faz combinação dos atributos em novas dimensões através


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to the prompt and answer the question posed. Supporting Evidence: The RESPONSE states that it was not possible to answer with the context provided, which indicates a complete lack of relevance to the PROMPT. It does not address the question about how data scaling (normalization or standardization) affects the accuracy of predictive models for time series, nor does it provide any information or context related to the topic. Therefore, it fails to meet any of the criteria for relevance.  Score: 0


---
