**Pergunta 21**: Como você poderia coletar e analisar dados para usar as redes sociais na previsão do tempo?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8981
- *Score:* 0.8761529326438904
- *URL:* oculto
- *Início:* 02:38:12
- *Fim:* 02:40:58
- *Transcrição:* só pros dados Então qual que é o primeiro problema onde eu vou buscar esses dados e onde eu vou guardar então aqui na aquisição e armazenamento né tenho dados de sensores que eles são coletados eh em séries temporais né então a gente tem eh os dados sendo atualizados constantemente a gente tinha os mapas que eram estáticos né E esse era um problema aí a gente atualizou Maps bacias pluviais a gente tinha mapas bem antigos E aí ele fez né um coletor aí para extrair os dados da previsão do tempo então a primeiro o que que eu tenho eu vou ter que ir lá coletar os dados e vou ter que colocar em algum lugar então Aqui começa uma primeir um primeiro olhar pros dados né onde eu vou guardar ou se vai ser só arquivos e como eu vou fazer né esse tratamento então aqui eh a gente também tem no momento eh que a gente faz análise de dados é bem importante a gente guardar a proveniência dos dados de onde eles vieram de que datas eles foram coletados pra gente poder fazer a consulta para trás né caso de algum problema ou a gente queira fazer alguma outra análise então eu vou coletar os dados né E aí e o que vai acontecer é que a gente vai passar paraa parte do tratamento dos dados Então eu tenho dados eh de vários tipos diferentes né tenho imagem tenho foto de satélite tenho dados textuais que são sensores tenho dados históricos tenho dados georreferenciados tenho dados de previsão do tempo né que eu tenho atributo e valor então o primeiro é o que fica desses dados o que sai que tipo de limpeza a gente faz né então aqui pro nosso caso é onde a gente vai colocar né Então aqui tem só um exemplo rápido né Assim que não é bem o nosso foco mas limpeza de dados né então aqui ele fiz lá o coletor e daí ele tava gerando erro que eu precisava saber a previsão do tempo numa determinada data que tá aqui em cima e às vezes

- *Corpus ID:* 5853
- *Score:* 0.8695755004882812
- *URL:* oculto
- *Início:* 00:39:56
- *Fim:* 00:42:01
- *Transcrição:* monitoramento elas medem lá algumas métricas né Por exemplo temperatura pressão enfim chuva e tal e aí eh eles basicamente criaram um dataset que agrega do mundo inteiro então vocês Imaginem a quantidade de informação a quantidade de coisa que dá para tu concluir né ou ou ou enfim tentar concluir a partir disso né em vista do aquecimento global esse tipo de coisa né da do do que a gente tá vivenciando no hoje né tentar explicar o hoje a partir disso aqui então Eh o volume de dados ele é um pouco grande mas a grande vantagem de das pessoas que organizaram esses dados vocês podem ir aqui diretamente nessa URL aqui eh deixa eu abrir aqui nova Tab aqui é que eles tiveram a preocupação de criar um arquivo Onde está organizado por ano já tá vendo Então já vem organizado por ano então até at tem 2023 notem que o tamanho de 2023 é menor eles devem ter ó a última atualização foi no dia 19 de outubro às 12:27 então bom o ano não tá completo né então assim olhar com cuidado demais né mas eh permite então a gente já eh olhar digamos assim e selecionar anos que a gente quer tá Então nesse caso aqui ó eu ainda assim tem um outro detalhe que quando a gente descompacta um desses arquivos eu vou pegar um dos arquivos menores aqui pegar esse de 1929 aqui ó quando a gente descompacta esses arquivos Eles foram completados a gente tem um monte de arquiv minho dentro e aqui no caso tem até pouco de 2929 tá são não sei quantos tem aqui não sei não vou contar quantos mas são poucos né Eh cada um desses arquivos é de uma estação meteorológica entendeu então eu vou abrir ele aqui no meu libreoffice para vocês verem Então como que o data é organizado então tem o código da estação a data qual é a localização geográfica dessa dessa Estação Qual é a altura altitude dela em qual município ela tá né E aí depois tem os dados que foram coletados e lá em em primeo de outubro

- *Corpus ID:* 8982
- *Score:* 0.8690258264541626
- *URL:* oculto
- *Início:* 02:40:13
- *Fim:* 02:42:58
- *Transcrição:* previsão do tempo né que eu tenho atributo e valor então o primeiro é o que fica desses dados o que sai que tipo de limpeza a gente faz né então aqui pro nosso caso é onde a gente vai colocar né Então aqui tem só um exemplo rápido né Assim que não é bem o nosso foco mas limpeza de dados né então aqui ele fiz lá o coletor e daí ele tava gerando erro que eu precisava saber a previsão do tempo numa determinada data que tá aqui em cima e às vezes né extrator de dados pegava a data da atualização ali do site que é diferente da data da previsão né se eu quero saber daqui 10 dias a data aqui vai ser daqui 10 dias em cima aqui embaixo vai ser a data de hoje então esse olhar que a gente tem que explorar os dados para trazer eles corretamente para análise que a gente vai fazer né outra parte a gente pode fazer uma anotação né Por exemplo dados de imagem eu posso fazer uma anotação textual e gravar né no arquivo ou no banco de dados junto com a imagem né O que é importante é coletar de onde vieram esses dados eu posso ter num base de dados junto ou separado né então todas essas informações que a gente vai usar depois na análise de dados elas vão est em algum lugar e daí é isso que a gente trata aqui também né aí depois que a gente coletou os dados né fez a limpeza Então a gente vai fazer a integração e a maioria das disciplinas que a gente viu porque o foco era em machine learning a gente foi da Integração de né então a gente considerou que esses outros casos já estavam resolvidos mas que Possivelmente né TCC a gente vai precisar tratar isso até o fim então aqui numas né terceira etapa ainda de preparação para fazer as análises já tem que saber né O que que eu vou fazer de análise o que que eu vou usar de machine learning ou de de dados ou de processamento né de texto mas eu

- *Corpus ID:* 5852
- *Score:* 0.8687242865562439
- *URL:* oculto
- *Início:* 00:38:23
- *Fim:* 00:40:28
- *Transcrição:* um sistema que coleta informação registra ex dado né Tu tem uma imaginação assim de que tu sabe Ah não ele coleta os dados dessa forma achar que os dados depois vão estar exatamente como a gente imaginou é uma ilusão porque pode acontecer um monte de coisa errada entendeu E esse monte de coisa errado acaba influenciando sujando o dado e tudo mais então a gente tem que sempre ter usar o pandas para isso para verificar que o dado é consistente aí depois a gente daí fez essa verificação né daí a gente chega às conclusões e isso é especialmente importante quando a gente tá fazendo as médias e né gerando estatística né porque se a gente não olhar a qualidade do dado estatística não vai fazer sentido algum a gente toma decisão depois um monte de coisa que não tem nada a ver Entendeu então é bem delicado e E aí sim isso é uma questão assim de da responsabilidade sabe mas fechando parênteses Então tá Eh vamos olhar rapidamente então o ad09 vocês estão comigo então migramos agora pro ad09 Ah o ad09 o ad9 é um dado bem legal assim que de novo eu coletei ele assim tipo quando eu tava montando o curso eu gosto de sempre ir atrás de dados que eu tenho interesse assim né Tipo depois né mas enfim eh são os dados desse gsod aqui né E O legal é que eles T dados de 1929 até hoje certo e são dados do quê vocês Imaginem que tem várias estações de de de monitoramento do tempo espalhados pelo mundo inteiro tá pelo mundo inteiro e essas estações de monitoramento elas medem lá algumas métricas né Por exemplo temperatura pressão enfim chuva e tal e aí eh eles basicamente criaram um dataset que agrega do mundo inteiro então vocês Imaginem a quantidade de informação a quantidade de coisa que dá para tu concluir né ou ou ou enfim tentar concluir a partir disso né em vista do aquecimento global esse tipo de coisa né da do do que a gente tá vivenciando no hoje né tentar explicar o

- *Corpus ID:* 5086
- *Score:* 0.8662419319152832
- *URL:* oculto
- *Início:* 00:27:52
- *Fim:* 00:30:34
- *Transcrição:* eh separar Ou pelo menos identificar quando tá de vazio para muito cheio passando por uma ocup média então esses digamos assim os opostos são são aquelas próximos de situações que a gente tem que ter alertas né então qu tá próximo diga uma dúvida só e não existe só essa empresa que fornece bicicleta lá certamente né Deve ter algumas né foi levado em conta essa concorrência ou só se tratou porque você pode ter uma vazia mas o outro lado tá todo cheio e vice-versa né eh eu na época que a gente trabalhou só tinha essa essa essa empresa ah da da cidade eh eu acredito que isso se mantém até hoje mas eu não não posso ser categórico não não sei a situação atual mas eu acho foão com uma só né porque aí facilita né com uma só sim sim não não a os dados coletados eram de uma de uma base só então aqui Ah o que o que a gente então aqui a gente tá usando uma escala de cor divergente onde azul é vazio ou vermelho é cheio e branco é é essa parte do meio aqui são é onde você tá com um pouco mais de folga para poder mostrar E aí você pega e mostra por exemplo quatro estações diferentes aqui e você tem uma uma ordem que tá por exemplo sendo definida pelo número do o ID da da estação a estação um estação do Estação 3 Estação quro e e a gente já tinha experimentado com com técnicas de reordenamento quando a gente olhou para os dados de futebol poder ordenar por eh durante o jogo pela pela distância que o jogador tava do Gol ou o e isso mexia com com a com a com as linhas né E então aqui o que a gente pensou é que talvez fosse interessante Porque como você quer olhar para para partes específicas do do tempo eh desculpa ah alguém botou um p aqui apagou eh você pode querer por exemplo analisar horário de R no começo da manhã final do dia ou em algum outro momento o que a gente Pensou

- *Corpus ID:* 4906
- *Score:* 0.8660764098167419
- *URL:* oculto
- *Início:* 00:46:03
- *Fim:* 00:48:19
- *Transcrição:* embora você você também possa fazer diferentes filtragens no Google e dizer eh eh qual tipo de texto livre que você tá procurando mas aqui você já tem categorias pré-definidas que você tá querendo buscar então se você tá querendo uma coisa específica que por exemplo com dados de clima né aí você clica aqui e aí ele vai fazer uma filtragem e vai acesso a dados de né E aqui ele vai dizer qual é o órgão e quais são as tags que são os textos lives que são colocados Associados Então esse tipo de de interface é bastante interessante né para ser feita também parecido com teu diga pode falar Alexandre Desculpa eu não tinha visto sua mão levantada não eu levantei agora eh como eu sou Lego eu não tenho ideia da quantidade né Essa e o senhor falou que uma uma planilha na nuvem já poderia ser utilizada para para montar um um quadro desse então assim e qual é a ideia mais ou menos de quantitativo assim que é válido para para esse tipo de visualização né Eu perguntei na aula passada sobre gráficos O senhor falou entre 5.000 e 20.000 registros não foi isso Eu não lembro assim exatamente mas especificamente sim sim sim ó só para vocês terem uma ideia esse aqui ele ele tá um pouquinho lento vocês viram que ele demorou um pouco para carregar Mas ele tem 100.000 registros tá vendo aqui aqui em cima né esse é o que tá rodando aqui né Eu não sei se é porque essa é a versão a versão que ele ele tem um serviço que ele vende né Aí você basicamente você tem que fazer uma um pagar pagar mensal para para poder usar eh dados de planilhas que são que são protegidas né a versão gratuita que eu uso é só quando você tem um Google sheets que que é público né você pode compartilhar o link com pessoas e as pessoas podem pelo menos ver né mas não é um caso corporativo né no caso corporativo onde vocês têm dados que são sigilosos vocês não podem usar ou

- *Corpus ID:* 9052
- *Score:* 0.8659065961837769
- *URL:* oculto
- *Início:* 00:48:00
- *Fim:* 00:50:36
- *Transcrição:* importante eh é o propósito né O que vai ser feito isso aí bem mas de qualquer maneira eu acho interessante entender melhor a diferença até para porque pelo menos na comparação aqui no slide né tem aquela escala de Rigor científico né para entender um pouco melhor a diferença tá ótimo eh e aí o principal já Aproveitando né a pergunta do Ant que a que é né o resultado aqui dessa disciplina é a proposta então é que que vai ter na proposta vou ter Qual que é o meu objetivo Qual que é o problema né social que eu resolvo qual que técnicas eu vou usar como eu vou comparar né então pode cair em um daqueles tipos pode eh contemplar mais de um assim né então aqui é só paraa gente ver essa diferença mas na hora h de propor o trabalho aí né Eh a gente vai focar no objetivo do trabalho Ok obrigado aqui só daí para mostrar para vocês né então esse aqui do César ele ele era economista né então ele trabalhava na da prefeitura pegou dados de acidentes de carro de trânsito e daí ele fez eh vários cruzamentos né então ele pegou eh tinha os dados da localização dos acidentes aí ele cruzou eh num período acho que de 2is anos então ele cruzou com a a os dados de previsão de tempo né para ir com dados de calendários então ele queria analisar Por exemplo quando tá chovendo quando tá sol se quando tem mais acidentes né então sei lá quando tem tá chovendo tem mais acidentes mas em muitos pontos tinha mais acidentes Quando tava só porque as pessoas corriam mais Então essas análises ali que ele fez né E também cruzou com dados de do calendário para olhar assim feriados quando tinha um feriado mais estendido se tinha mais acidentes ou não então ele fez o cruzamento dessas três bases de dados e daí o usou né algumas técnicas de mineração de dados Então como regras de associação daí para

- *Corpus ID:* 4699
- *Score:* 0.86588454246521
- *URL:* oculto
- *Início:* 00:30:31
- *Fim:* 00:32:55
- *Transcrição:* ao invés de sempre comparar com os mesmos dias a gente permitia com que essa janela de comparação de 41 dias ela deslizasse ou para cima ou para baixo e aí a gente conseguia achar os anos mais próximos usando essa janela deslizante e nesse caso aqui Ah o mais próximo que eles que que a gente achou foi 2011 com c dias após Então você conseguia achar informações diferentes a gente não nenhuma digamos assim aplicação dessas análises para nenhum contexto agora falando para vocês eu fico pensando uma possível aplicação disso seria por exemplo para para vindima de de de vinho quando você tem que fazer a coleta de uvas se você tem uma monitoramento da tua região e você CONSEG entendeu os padrões Talvez isso Possa possa te dar um indicativo de qual seja o melhor dia de você fazer a a colheita por exemplo você a cada ano você tem condições climáticas diferentes você tem né níveis de chuva e coisas do gênero de repente eh você teve uma safra muito boa de um determinado ano e essa safra muito boa ela ela tem vários fatores que influ cam né os fatores climáticos mas também tem essa questão de quando que você faz o processo de coleta das uvas né não tinha pensado nisso Olha que eu tô trabalhando nesse negócio há muito tempo a sempre quis fazer um projeto na área de vind aqui eu vou ver se vou tentar entrar com em contato com o pessoal aqui no Rio Grande do Sul tem muitas muitas vinículas aqui para saber se eles teria o interesse em algo parecido como com isso aqui bom eh agradeço vocês estão por falar sobre o projeto e mostrar Bom enfim Ahã Então a nossa interface ela ela era interativa né permitia você poder trocar os anos e essa parte de combinar a interação com a visualização é bastante importante Principalmente quando você faz também eh configurações do algoritmo você você faz filtragens você quer

- *Corpus ID:* 4685
- *Score:* 0.8657735586166382
- *URL:* oculto
- *Início:* 00:03:45
- *Fim:* 00:06:02
- *Transcrição:* dados ou alguma técnica de de de Inteligência Artificial onde você cria modelos que podem eh a a auxiliar nessa análise de dados e esses modelos eles podem estar interagir com a visualização podem ser refinados pelo usuário muitos desses modelos eles exigem parâmetros pro para você poder configurar des conhecimento para chegar nesse no conhecimento final e esse processo se repete Então você volta esse esse diagrama foi proposto em 2005 que foi quando essa área de vis analític surgiu E como eu disse para vocês surgiu numa área numa época em que estava tentando se dar uma resposta para 11 de setembro tentar entender eh como é que a gente podia poder monitorar os dados nos Estados Unidos no caso para prevenir ataques e eu tenho trabalhado em vários projetos eu vou falar sobre alguns deles hoje principalmente esses três primeiros aqui e depois eu vou falar sobre os de covid esses outros eu vou falar mais à frente outras ideias mas aí eh O que é interessante é que cada um tem uma história cada um tem um tem um um problema que veio por trás e tem uma solução que a gente teve que pensar que não era trivial e que permitiu com que a gente chegasse A análise interessante dos dados e o primeiro deles é sobre eh monitoramento de clima esse projeto ele surgiu quando eu terminei aquele trabalho de corridas e eu tava apresentando esse trabalho num num evento em São Paulo e aí um colega meu que era professor da Unicamp e ele veio falar comigo e disse assim cumba Eu tenho um projeto que eu tô trabalhando na área de biologia que talvez a gente pudesse fazer algo parecido os asos são diferentes mas a gente talvez pudesse fazer algo parecido E aí Não vamos pensar vamos ver o que que o que que se pode fazer e aí a gente começou a se debruçar sobre o problema em si o que

- *Corpus ID:* 5078
- *Score:* 0.8650552034378052
- *URL:* oculto
- *Início:* 00:13:07
- *Fim:* 00:15:38
- *Transcrição:* domingo né você vê que tem Ah Picos específicos né manhã almoço e na do dia né normalmente tem três Picos você consegue ver claramente tem partes menores aqui né onde você tem a eh e depois você vê que também no final de semana não tem tanta movimentação como tem durante a semana embora que nesse dia específico terça-feira teve um um um pouco menos movimentação e isso aqui é uma agregação de todas as estações você tem eh dados separados por Estações E aí você soma todas e e e faz esse gráfico gráfico agregado claro que você também pode fazer gráficos específicos para uma determinada Estação Então essa que Estação 47 nesse lugar aqui né E você pode por exemplo fazer eh ao longo de mais tempo fazer médias eh eh ao longo de nesse caso aqui ele tá fazendo uma média para todos os dias da semana né pro para nesse caso o eixo X agora é o horário né E aqui também tinha o essa informação só que tava tava não tava detalhada mas aqui a gente tem um detalhamento maior para 24 horas do dia aí você vê que tem esses três Picos acontecendo também aqui né E aí no final de semana você tem um comportamento que é diferente ao longo do do dos outros dias onde você tem um pico maior entre 8 e e digamos assim 2as da tarde aqui você tem um pico maior no final do dia as pessoas vêm pra cidade e depois pareiam no final do dia né Possivelmente deixa eu ver quando é que foi isso aqui não tá dizendo mas Possivelmente foi no no verão quando os dia são mais longos né bom então eu tinha um aluno doutorado na época ele ele tava fazendo doutorado comigo e tem um colega que que é professor na na New York University no Nova York e E aí ele foi passar um ano com ele lá e a gente pensou bom como que a gente continua o trabalho de doutorado dele ele é o mesmo aluno que trabalhou com com a visualização de dados de corrida e a gente queria continuar esse trabalho então a gente pensou bom vamos vamos olhar para esses dados então a


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should provide relevant context and answer the question completely. Supporting Evidence: The RESPONSE states that it was not possible to answer the question, which indicates a complete lack of relevance to the PROMPT. It does not provide any information or context related to collecting and analyzing data for weather forecasting using social media, nor does it attempt to address any part of the question.  Score: 0


---
