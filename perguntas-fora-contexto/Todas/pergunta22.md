**Pergunta 22**: Como o deep learning supera os modelos tradicionais de aprendizado de máquina na análise de séries temporais?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8463
- *Score:* 0.8787685632705688
- *URL:* oculto
- *Início:* 01:13:59
- *Fim:* 01:16:21
- *Transcrição:* Teoricamente eles aprendem mais eles têm mais capacidade de identificar padrões eh complexos esse esse eh dimensionality do do modelo não é simplesmente o número a quantidade de estados de pesos Não é só isso porque ele tem cabeças de atenção então tem uma série de de coisas dentro dessa arquitetura Tá mas entendam que é um uma um vetor de 768 que vai sendo levado em cada de de uma camada para outra e que também depois nós vamos usar para isso que é AD beding que cada um desses modelos e provê tá então aqui tem mais eh tem aqui o o trabalho ó que se chama o artigo que foi publicado isso aqui foi tudo aberto então eles de fato revelaram como como treinaram explicaram então o nome era language models are un supervised multitask learners né então eles já viram a capacidade dele fazer várias tarefas aqui né E esses modelos a gente tem formas então de avaliar ele a forma mais utilizada são essas avaliações extrinsic então no momento que a gente tem um GPT treinado né como é que a gente diz que ele Ah ele para tarefa tal ele ele ele atinge um um determinado score né então o Bert por exemplo então eu vou fazer uma tarefa de classificação uma tarefa de eh geração de texto né então ele essa avaliação extrínseca ela mede o quanto a execução dessa tarefa melhora então o melhor exemplo aqui eu o do Bert nas usando classificação ou ou até a sumarização né uma tarefa bem específica na avaliação intrínseca ela já é mais difícil da gente compreender porque ela mede a qualidade de um modelo Independente de qualquer aplicação ou tarefa Então como é que se faria isso então tem uma métrica que é usada que é a perplexidade ou perplexity é usada nessa avaliação intrínseca e ela é uma métrica assim bem complexa ela tá relacionada à entropia

- *Corpus ID:* 6687
- *Score:* 0.8740004301071167
- *URL:* oculto
- *Início:* 00:17:49
- *Fim:* 00:19:55
- *Transcrição:* naqueles dados pra gente poder criar esse melhor modelo possível né ã a gente não precisa usar também a prisada fundo quando os métodos tradicionais eh funcionam bem e aí aqui eh eu coloquei métodos tradicionais de aidade máquina mas a gente poderia tornar isso aqui mais amplo onde os métodos estatísticos já funcionam bem né então por exemplo se eu quero fazer uma regressão linear esse tipo de coisa tem métodos que já funcionam super bem e que já geram um um feit eh suficientemente adequado né E que a gente não precisa daí empregar precisar profunda né até a gente brinca de vez em quando a a febre tá tão grande em relação a esse assunto né o Hype o pessoal fala né que por vezes o pessoal tem usado Prado Prof fundo em meso em situações assim que que realmente claramente não precisa mas é mais assim para Ok estou usando aprendizado profunda eh Tem situações também onde os dados estão bem estruturados onde eu tenho bastante conhecimento dos dados já estão limpos esse tipo de de coisa então daí não precisa usar também né Porque existe uma estrutura já bem clara né E tem uma outra situação que é quando não usar é quando o modelo que vai ser criado ele precisa ser explicável né então por exemplo eu crio um modelo isso tem muita gente trabalhando né quando eu crio um modelo de aprendizado profundo né que é uma rede neural dificilmente eu consigo explicar os valores dela uma vez ela tenha sido treinada né Tem gente trabalhando nisso né explicar assim ah como por que que ela funciona né Por que que essa rede neural consegue identificar os objetos ou classificar os objetos por né a pessoa tenta olhar visualizar os valores dessa rede neural muito difícil explicar né então nessas situações não se a gente precisa que o modelo seja explicável né daí a gente não vale a pena usar PR profundo porque isso é uma coisa que tá ninguém sabe

- *Corpus ID:* 7170
- *Score:* 0.870777428150177
- *URL:* oculto
- *Início:* 01:28:32
- *Fim:* 01:30:43
- *Transcrição:* Deep learning isso era bem Inicial né então vocês podem esperar muito mais do que isso para hoje tá aqui a gente tem um outro sistema do metp que é um sistema comercial e eles T outros sistemas outros dispositivos de captura né eventualmente alguns dispositivos capturam informação tridimensional também outros capturam só cor e e a partir das informações de cor a gente pode reconstruir né uma ou fazer uma reconstrução tridimensional do ambiente também e Navegar eventualmente né Por esse ambiente virtual eh usando esse tipo de ferramenta tá isso aqui se tornou bastante Popular especialmente fora do Brasil não sei como é que tá aqui dentro para eh em Sistemas de Imobiliária né Então a gente tem uma imobiliária tem um apartamento que a gente quer alugar Quer comprar uma casa e ao invés de me deslocar ir lá e e ver que eu não gostei né antes de tudo dá para fazer uma navegação virtual e bom se eu achei que é legalzinho de repente eu eu pago né o Uber pago o ônibus pago a gasolina para ir até lá ver tá mas mas fica um sistema de screening assim né para determinar se eu devo ou não devo investir naquela compra naquele aluguel tá eh e hoje em dia a gente tem soluções aqui é um dispositivo específico aqui é outro dispositivo específico para captura eh mas a gente tem soluções que tão mais portáteis né mais na palma da mão do que do que nunca então sei lá eu eu vou falar do iPhone embora não tenha um iPhone eu vou falar do iPhone porque eu sei que tem aplicativo para isso nele eh em que a gente de fato faz esse tipo de reconstrução eventualmente não é tão boa quanto né desses exemplos que foram dados mas ele consegue de uma forma ou de outra fazer um mapeamento de ambientes pequenos e e e te ajudam né a interagir com esses ambientes tá Ahã Professor

- *Corpus ID:* 6688
- *Score:* 0.8691415786743164
- *URL:* oculto
- *Início:* 00:19:24
- *Fim:* 00:21:39
- *Transcrição:* tenha sido treinada né Tem gente trabalhando nisso né explicar assim ah como por que que ela funciona né Por que que essa rede neural consegue identificar os objetos ou classificar os objetos por né a pessoa tenta olhar visualizar os valores dessa rede neural muito difícil explicar né então nessas situações não se a gente precisa que o modelo seja explicável né daí a gente não vale a pena usar PR profundo porque isso é uma coisa que tá ninguém sabe explicar ainda né Tem gente estudando isso aí mas isso ainda é um um processo tá acontecendo digamos assim bom então falando um pouco sobre o aprendizado profundo né Eh a gente sempre tem essas duas fases uma fase de treinamento e uma fase de inferência né então normalmente né em situações eh corriqueiras digamos assim o treinamento é uma parte que deve acontecer junto com a massa de dados que a gente tem tem PR processar esses dados e esse treinamento normalmente ele é lento né então ele envolve a gente ler todos os dados várias vezes que é o que a gente chama de épocas né e em cada uma dessas épocas a gente vai embaralhar os dados né para que eles fiquem bem misturados para que não se crie vícios né E a gente vai empregar então uma estratégia de distribuição desses dados de maneira que a gente possa treinar usando gpus tpus cpus então uma vez que esse treinamento Se conclui e a gente vai ter um modelo e esse modelo a gente pode salvar é um arquivo e esse modelo depois a gente pode usar no que a gente chama de eh no deployment né tipo assim que seria o uso do modelo e essa segunda parte que é a parte de inferência ela é considerada rápida sobretudo em relação à Ótica do treinamento ela sempre vai ser mais rápida que o treinamento porque ela envolve somente a gente fazer fornecer um dado na entrada e fazer aqueles dados estimular a rede Até chegar na saída né então isso é rápido né comparado com o tempo de treinamento

- *Corpus ID:* 1435
- *Score:* 0.8678132891654968
- *URL:* oculto
- *Início:* 00:01:47
- *Fim:* 00:03:46
- *Transcrição:* principalmente aprendizado profundo se usa muito então ela é uma estratégia útil Tá mas depois a gente vai motivar porque que a gente precisa outras estratégias e a gente vai explorar essas outras estratégias na disciplina seguinte lembre quando a gente falou da base de aprendizado de máquina né a gente discutiu o processo é o processo não Desculpem a característica de desculpe a característica de generalização tá generalização Seria isso quão bem esse modelo se sai na classificação na regressão enfim para novas instâncias instâncias que ele nunca viu que não estão na base de treinamento tá esse é o grande objetivo do aprendizado de máquina eu quero modelar bem os dados que eu tenho mas o objetivo em fazer isso a ser capaz de tomar decisões sobre novos dados e eu preciso fazer ter formas de avaliar essa capacidade de anenalização seja tão bem esse modelo vai sair quando ele for confrontado com dados que ele não viu que não estava na base de treinamento Quando eu digo em dados que ele não viu tem toda uma questão muito importante a gente citar que a gente está discutindo a respeito de um problema específico de uma distribuição de dados né então esses dados que ele não viu não quer dizer que são Dados totalmente diferentes né Se forem totalmente diferentes ele não vai sair bem mas são Dados que a gente assume que surge na mesma distribuição da mesma população então se eu tô tentando analisar aquela questão de probabilidade de uma pessoa vira fazer compras em uma loja né É porque eu tô lidando com conjunto de dados uma população Ou seja a base que eu tô usando para treinamento são de características que essa nova pessoa esse novo cliente sim vai se integrar ali na distribuição de alguma forma tá é uma coisa totalmente diferente se eu tô falando sobre a questão de

- *Corpus ID:* 8686
- *Score:* 0.8670104146003723
- *URL:* oculto
- *Início:* 01:55:49
- *Fim:* 01:57:36
- *Transcrição:* precisou fazer a a transformação aí da dos rótulos né né para para numéricos para poder eh continuar com o treino e e e a gente utilizou textos um pouquinho diferentes lá dos machine learning né a gente treinou reex com mantendo seus copy Worlds tá e não utilizamos nenhum lization nemum stem a gente entendeu que não não faz muito sentido Tá mas eh ambos foram utilizados com só com reex e e mantendo as Stop WS a gente também não removeu Stop WS para esses dois caras aí e da mesma forma que nos modelos de GP learning de machine learning perdão eh nós nós também Cap amos as métricas né durante treinamentos para fazer o comparativo aí desses cinco modelos né desses dois com os três de de machine learning daí Vamos aos resultados né a gente acaba pulando como alguns colegas falaram que o tempo Realmente é muito curto acho que deve est estourando aí eh depois dos treinamentos aí dos cinco modelos eh a gente percebeu aí que a o que apresentou o melhor resultado foi regressão logística e paraa nossa frustração todo o trabalho que nós tivemos aí com com os modelos de Deep learning que eh reconhecidamente eles eles têm técnicas mais recentes mais boa parte dos casos mais avançadas eh e que também levou muito de processamento nosso né exigiu muito processamento nós tivemos que abandonar o colab aí para treiná-los e e treinar aí usando máquina local eh botando a máquina placa de vídeo aí para esquentar um pouquinho né mas eh eles tiveram um desempenho assim relativamente pior em relação aos de aos de Machine lear né apesar de todo o trabalho que a gente teve a gente algumas configurações tá eh isso aí também a gente acredita que se deve ao ao fato de gente não ter tido eh tanto tempo quanto poder de processamento para variar de forma adequada esses dois últimos modelos aí que são J learning né então a gente acredita que eles podem eles tem um potencial muito bacana ainda

- *Corpus ID:* 2626
- *Score:* 0.8669999241828918
- *URL:* oculto
- *Início:* 00:18:47
- *Fim:* 00:20:41
- *Transcrição:* representado profunda era muito mais complexo não é interpretável então basicamente o que eles observaram isso é que um modelo interpretável aqui representado pela regressão era tão bom quanto esse modelo não interpretável e ele se depararam a situação que é ou eu sigo as regras do desafio e apresento o meu modelo de aprendizado profundo e apresenta uma estratégia para tentar explicar o que esse modelo tá prevendo que tem técnicas para isso Ou eu apresento meu modelo de agressão linear e mostro olha por que que eu vou fazer um modelo não interpretável se eu consigo resolver com um desempenho tão bom quanto usando um modelo interpretável então basicamente o que eles fizeram eles submeteram esse modelo baseado na regressão linear eles fizeram um site interativo que mostrava né basicamente o que que tava contribuindo para esse risco de nardimplência para um determinado cliente tá através da análise desses coeficientes e eles não ganharam o desafio mas eles ganharam esse porque eles basicamente né trouxeram esse modelo totalmente transparente em uma dashboard friendly então não só o modelo era transparente Mas eles permitiu que as pessoas interagir com isso e na realidade o maior ganho desse desse dessa equipe pesquisadora foi trazer né para discussão do momento de Por que que a gente precisa começar já determido que o problema tem que resolver tem que ser resolvido com o modelo que não é interpretável tá porque se tem essa falsa ideia de que modelos não interpretáveis são esses modelos mais complexos são os únicos possíveis de resolver um problema aparentemente complexo com desempenho razoável tá ou satisfatório Então existe essa discussão e aí essa pesquisadora ela falou essa internet essa escolha imposta entre a curasse a interpretabilidade é de modelos é uma falsa dicotomia né

- *Corpus ID:* 3021
- *Score:* 0.8665845990180969
- *URL:* oculto
- *Início:* 00:35:01
- *Fim:* 00:37:21
- *Transcrição:* partir do momento que você ganha uma camada oculta você sai do reino linear e ao sair do reino linear você né pode chamar de rede neural profunda Claro tem algumas pesquisadores mas puristas que vão defender esse ponto de vista né de que ah não tem que ter dezenas de camadas se não não aceito que chamam de profunda mas em termos de poder matemático se saiu do reino linear já pode chamar de rede profunda então para o nosso propostas aqui nós vamos adotar essa convenção se tem uma camada oculta ou seja não é só um neurônio de sair daqui mas tem né um processo neurônios intermediários aqui que fazem processamento Então já pode chamar de rede neural profunda ok pessoal Então tá visualizado aqui né o nosso poderio já de um e é uma região até simples né Tem uma camada oculta com dois neurônios e um neurônio de saída e já conseguiu esse efeito aqui né para o nosso propósitos ficou tá bem melhor né do que antes que estava errando praticamente todos os pontos ok tá aí vocês vão poder brincar aqui à vontade tá o link é esse aqui ó playground tá vai poder brincar à vontade e só né um pequeno parentes né o exercício dessa semana é implementar um neurônio linear no colab e a planilha de classificação né preencher de classificação agora com duas entradas né eu fiz para uma entrada vocês vão fazer com duas né que em vez de S é o z tá é só assim ajuste aí e aqui é mais ou menos no mesmo mesmo paradigma né do exercício anterior aí escreve a programação aqui que implementa faz um neurônio calcular corretamente para uma entrada né e o teste e aqui para regressão né para essa implementar essa função encontrar os pesos e o baias né Vocês vão meio que fazer o não precisa implementar o algoritmo tá vocês podem chutar aqui na mão mesmo e ver né Qual o peso qual baias faz o negócio funcionar aí daqui para baixo vocês

- *Corpus ID:* 7618
- *Score:* 0.8659316301345825
- *URL:* oculto
- *Início:* 00:02:39
- *Fim:* 00:05:03
- *Transcrição:* eh acho que as aulas mais importantes né para para entender essa base foi foram a a de sexta-feira com Cláudio né E essa primeira metade aí tá é claro que a gente não quer necessariamente que vocês saiam né aplicando wavelets e entendendo exatamente como é que esse negócio funciona e tudo mais E por que que tem os subespaços vetoriais e blá blá blá Não nada disso né o que a gente quer com esse monte de coisa é mostrar que existem informações que são melhor percebidas na eh num nível de de resolução do que em outro tá eh e no final das contas a ferramenta pré Deep learning que melhor fazia isso até então era essa de Tá além disso a ideia também era mostrar que a noção de frequência né Ela é importante eh paraa filtragem e para outras tarefas né como reconhecimento de de partes ali né como a gente viu no no no último exemplo do da imagem com ruído né h e que essa noção de frequência ela é compartilhada querendo ou não com a parte de fourrier alguma coisa aqui de wavelets também então a gente tem a ideia de cada nível de decomposição captura uma frequência diferente né e por sua vez frequência filtragem de por frequência no domínio fourrier pode ser também né feita de uma forma um pouco menos eh inteligível digamos assim no domínio espacial original né que foi alguma coisa que o Claudio deve ter mostrado lá para vocês do na galciana no espaço no e no e no espectro né Tá então acho que essas aulas são as mais importantes definitivamente pré eh até agora pelo menos tá pra gente entender o resto mas pra gente entender essas aulas a gente também precisava das aulas anteriores né então assim eh de novo é uma disciplina introdução programa introdução a apressamento de imagens É uma disciplina de 60 horas introdução a visão computacional É uma disciplina de 60 horas a gente a gente junta as duas

- *Corpus ID:* 1327
- *Score:* 0.8654552698135376
- *URL:* oculto
- *Início:* 00:10:46
- *Fim:* 00:12:54
- *Transcrição:* eu tivesse esse modelo modelo linear ele modela bem assim não perfeitamente mas ele ele consegue recuperar um pouquinho a tendência dos dados tá eu tenho esse modelo aqui em verde agora então perceba que esse modelo ele já não é linear e ele tem ele consegue então não vou eu tô com a caneta vermelha aqui mas ele consegue modelar melhor esses dados fica um dado aqui que a gente não sabe se é um alto layer é um dado fora da curva um erro de medição ou é um dado real a gente não sabe isso ainda tá mas ele melhor ele é um pouquinho melhor que o outro né Porque ele consegue modelar quatro e cinco pontos de dados cinco instâncias que são esses x aqui né esses x que a gente tá vendo aqui tá E aí a gente pode aumentando a complexidade do nosso modelo da nossa função Agora tem essa função que ela modela bem todos os dados Então ela é capaz de acertar a saída para toda entrada desses dados usados né para modelagem então a gente assume que para novos dados ela vai conseguir prever melhor a saída de forma mais precisa também posso ter essa função laranja certo essa função em laranja ela é uma função extremamente complexa muito mais do que necessário certo então esse esses exemplos Ele eles têm o motivo de mostrar para vocês alguns aspectos específicos né primeiro então eu quero sempre esse modelo que melhor se ajuste aos dados entre modelos que se ajustam aos dados eu posso ter múltiplos modelos que são consistentes alguns cometendo um pouco de erro outros nenhum erro mas mesmo entre os modelos que são que parecem bons que eles aprendem todos os tipos de dados a gente pode ter diferentes modelos por exemplo nesse caso aqui tanto a laranja Quanto é essa azul são modelos que se ajustaram perfeitamente


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: A resposta deve ser relevante para todo o PROMPT para obter a pontuação máxima de 3. Respostas que não respondem à pergunta ou que não fornecem informações relevantes devem receber a pontuação mais baixa. Supporting Evidence: A resposta "Não foi possível responder com o contexto fornecido" não aborda a questão de como o deep learning supera os modelos tradicionais de aprendizado de máquina na análise de séries temporais. Ela não fornece informações, explicações ou contextos relevantes relacionados ao PROMPT, e não tenta responder à pergunta de forma alguma. Portanto, a resposta é irrelevante.  Score: 0


---
