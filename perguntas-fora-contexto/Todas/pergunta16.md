**Pergunta 16**: Entre filtragem colaborativa e filtragem baseada em conteúdo, qual é considerada melhor e por quê?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2488
- *Score:* 0.8520557880401611
- *URL:* oculto
- *Início:* 00:10:37
- *Fim:* 00:12:40
- *Transcrição:* ao longo dessas abordagens que a gente vai discutir ok bom vamos começar a falar da seleção baseada em filtros tá essa seleção ela é a mais simples que a gente tem Tá mas a gente vai ver que ela é igual ela é muito útil principalmente em casos como a gente comentou brevemente né em outros momentos em que eu tenho por exemplo grandes volumes de dados porque cada etapa de transformação de dados dependendo do que o tecido que eu decido fazer por exemplo vou fazer uma imputação simples uma imputação por KNN se eu faço uma seleção baseada em filtros ou uma seleção baseada em breve porque a gente ainda vai ver é cada uma delas envolve um custo computacional diferente né então muitas vezes dependendo do volume de dados a gente também tem que levar em consideração o volume de dados para decidir qual a estratégia que a gente vai usar é e essa abordagem baseada em filtros ela tem algumas vantagens e tal realmente tudo tem vantagens e vantagens mas ela tem algumas vantagens que é o seguinte ela faz a seleção de atributos de uma forma independente do algoritmo de aprendizado de máquina isso quer dizer que eu poderia por exemplo usar essa seleção de atributos e o resultado dela poderia ser usado como entrada para diferentes algoritmos se eu quiser porque a decisão de como ela de como ela estabelece os mais relevantes não depende de um algoritmo específico o que ela faz é ela usa uma médica uma estratégia que avalia a cada tributo em relação a sua a sua importância para determinar que ele é tributo alvo então eu coloquei que correlação Associação porque no fundo as métricas dependendo são atributos numéricos categóricos as métricas estão avaliando esse tipo de questão tá é associação daquele atributo matribual você seja o quanto que ele contribui aquele atributo né um preditor

- *Corpus ID:* 850
- *Score:* 0.8451981544494629
- *URL:* oculto
- *Início:* 00:58:03
- *Fim:* 01:00:27
- *Transcrição:* menos regras ou a gente não põe nada de restrição aqui sai muitas regras e aí a gente tem um esquema que vai me ajudar a selecionar aquilo que é interessante e desinteressante no século passado quando a gente não tinha muito poder computacional a gente jogava valores de suporte confiança lá em cima para que saíssem menos regras e fosse mais rápido tá nos dias de hoje que a gente tem poder computacional a gente trabalha muito mais com esse tipo de abordagem joga tudo lá embaixo e depois a gente filtra externamente as regras para não precisar rodar de novo mas ambas as abordagens são válidas se vocês vão ver que no estudo dirigido que eu vou dar para vocês Vocês usam uma e outra no processo que são essas métricas de interesse aqui tem um monte gente um monte a gente já falou de suporte a gente já falou de confiança agora a gente vai rolar de lift então o suporte é algo que tem que estar presente no mínimo x% de registros para eu considerar interessante a confiança eu tenho uma força naquela implicação confiança alta é um bom sinal mas às vezes simplesmente pode ser porque o valor é prevalente ali a nossa cebola né vai dar muitos valores que que a cebola vai determinar outras coisas mas é porque a cebola sempre tá presente E aí é uma outra coisa bem interessante é o lift o lift que também é chamado de alavanca vai verificar se a presença do antecedente aumenta ou diminui a probabilidade da presença do consequente se ela aumenta essa probabilidade a gente diz que essa correlação é positiva e se ela diminui a gente diz que a correlação é negativa e se o lixo é igual a um essas valores são estatisticamente independentes quer dizer que pode ser que sim pode ser que não eles ocorrem tanto naquele caso como eles também ocorrem no outro

- *Corpus ID:* 7522
- *Score:* 0.8451364636421204
- *URL:* oculto
- *Início:* 00:18:34
- *Fim:* 00:21:01
- *Transcrição:* atualização da posição desse Pixel vermelho tá e se eu aumentar tanto Sigma D quanto Sigma R bom aí eu digo que todo mundo fala com todo mundo e eu relaxo essa restrição de cor aí eu tenho de fato um borramento maior que é o que a gente tá vendo nessa imagem Aqui de baixo tá de novo pessoal eu não sei se ficou Claro para vocês eu acho que é legal experimentar tá tem ali no num dos scripts do colab tem um uma uma rotininha para vocês brincarem com isso tá mas o filtro bilateral é um filtro não linear e é um filtro bem interessante para meio que fazer o meio-termo entre e remoção de ruído em região que é mais ou menos homogênea e tentar evitar o borramento de bordas Ok vezes é chamado que o pessoal chama de aware Filter é um filtro que consegue respeitar bordas B outra que é important gente eu perguntei um tempo para trás para vocês tá vamos supor que eu tenho uma imagem que é imagem ouro a imagem ideal eu tenho uma versão dela corrompida com ruído OK e eu tenho duas técnicas de filtragem um com filtro galso outro com filto bilateral eu vou rodar filtragem nas duas imagens E aí eu pergunto para vocês qual das duas é melhor quando eu pergunto para vocês qual das duas é melhor vocês vão vão ter obviamente um baias de vocês para responder a pergunta eventualmente alguns acham que o filtro da média é melhor outros acham que o filtro bilateral É melhor então quando a avaliação envolve a percepção humana eu Gero uma subjetividade muito grande então é importante que a gente tenha métricas objetivas de qualidade da imagem então eu tenho uma imagem ideal e eu tenho duas versões alteradas dela eu quero saber qual delas tá mais parecida com a original certo bom dito isso que tipo de métrica

- *Corpus ID:* 3574
- *Score:* 0.844243586063385
- *URL:* oculto
- *Início:* 00:55:42
- *Fim:* 00:57:54
- *Transcrição:* resultados e o primeiro em teoria é o mais relevante só que se eu se eu fizer é uma castelização do resultado e como eu disse algumas ferramentas foram criadas para isso mas pessoal achou que isso fosse ser interessante no fim não sei se por questões de tempo as pessoas acabaram não não usando muito essas ferramentas então elas já nem existem mais ou existem muito poucas elas pegam o resultado analisam se se as páginas seus recursos Têm alguma relação e criam essa taxonomia então a ideia delas é que ao invés de receber uma lista de páginas eu recebo uma árvore eu vou navegando por essa árvore ele justamente ao Navegar eu teria uma noção da relação das páginas então eu consigo filtrar melhor o resultado aí eu vejo eu quero essas que são que tem esse conteúdo da esquerda ou da direita a escolha da direita então eu vou filtrando melhor o resultado até que eu acho mas no fim Isso acabou não ajudando as pessoas acabam usando as uma das 10 primeiras páginas e ignoram o resto né então não vale a pena uma ferramenta dessas e outra situação outro conjunto outra família de algoritmos eles não criam hierarquias Então são esses sistemas de partitivos né eles dividem com dados e os elementos são isolados até pode haver situações em que um elemento pode estar em mais de um cluster mas mas a princípio Eles não têm uma hierarquia podem ter sobreposição mas não hierarquia e os hierárquicos eles podem funcionar de cima para baixo ou de baixo para cima quer dizer eu começo com conjuntividades vou dividindo esse quantidades em dois ou eu posso começar com cada um cada considerando né que cada elemento é um câncer e eu vou agrupando eles dois a dois de acordo com a semelhanças

- *Corpus ID:* 2530
- *Score:* 0.842319905757904
- *URL:* oculto
- *Início:* 01:17:44
- *Fim:* 01:19:45
- *Transcrição:* desse atributo em relação àquela classe Então se nessas instâncias estão nos dados de teste ou de validação a gente acaba tendo vazamento de dados mas o efeito não é tão grande porque como você está comentando a gente não faz avaliação e termos do desempenho né então o desempenho atrelado a um modelo então a técnica de baseado em filtro eu diria que aquela mais tolerável da gente fazer fora da validação cruzada porque ela seria menos sensível a esse tipo de tá E até porque ela também é uma técnica a gente usa muito com grandes volumes de dados E aí fazer Deixa eu só voltar aqui fazer esse tipo de abordagem com grande volume de dados a gente mesmo que obviamente não vou usar a livonaldo eu vou usar por exemplo tendo folder validation mas acaba sendo muitas vezes inviável em praticável assim porque exige muito tempo né computacional então às vezes a gente não consegue arcar com esses custos ou com o tempo né para fazer esse tipo de experimentos então por isso que às vezes a gente acaba muitas vezes simplificando algumas coisas da metodologia que a gente sabe que não é a melhor forma de fazer mas a gente não consegue arcar com o curso de fazer tudo da forma correta ou né do ponto de vista sem vazamento de dados então por exemplo do ponto de vista de pesquisa quando a gente está fazendo a gente tenta ver assim o que que vai minimizar o meu Impacto o que que vai ter o menor impacto em relação a pergunta de pesquisa que eu tô querendo fazer né então se eu tô querendo comparar a questão de algoritmos por exemplo bom então vou fazer uma baseada em filtro antes em todos os meus dados e eu vou dizer Olha foi feita essa forma porque meus dados eram muito volumosos e porque o nosso objetivo era comprar outros aspectos e não a sensibilidade

- *Corpus ID:* 7554
- *Score:* 0.8418689370155334
- *URL:* oculto
- *Início:* 01:20:11
- *Fim:* 01:22:25
- *Transcrição:* tempo no colab é muito fake né porque eu não sei como é que ele aloca tempo de máquina mas pelo menos dá para ter uma noção né E vocês vão ver que a distância é brutal tá e então recapitulando a se nós a a aplicação da transformada discreta de furir ela vai implicar em uma redução do custo computacional mas em termos de qualidade do do do do do resultado isso não se altera correto el são equivalentes matematicamente pode ter algum pequeno alguma perturbação numérica Tá mas em tese o resultado matemático é idêntico tá vocês podem até comprovar isso no próprio notebook que tá lá na no Moodle para vocês tá então só para mostrar aqui ó como é que funcionaria a filtragem dessa imagem ã com filtro da Média 5 por5 então eu pego a imagem Gero as versões expandidas que eu não tô mostrando aqui mas é só fazer o pé colar as duas e o que eu já tô mostrando é a transformada de fourrier desses caras aí então aqui em cima tá transformada de furria do filtro olhando para esse cara vocês concordam que esse cara é um filtro passa baixa centro do domínio de frequência tá no meio da imagem Então esse cara tem cara de filtro passa baixa tem tem uma coisa meio bizarra né que tem essas bolinhas aqui né ele e que são os ripples do filtro da Média Então na verdade ele tem uma aba principal e ele tem uns carocinhos que deixa passar algumas frequências um pouco maior Tá mas é uma é uma característica do filtro da Média o filtro da galciana não tem isso aqui é multiplicação elemento a elemento tá eu tô visualizando esses caras em escala logarítmica porque a transformada de fourrier normalmente o elemento zero zer ele é muito maior que os outros muito maior que os outros se eu só fizesse uma escala linear do maior pro banco o resto ia ficar preto e hav um pontinho branco na origem tá então é meio ilusório esse gráfico esse gráfico aqui

- *Corpus ID:* 2506
- *Score:* 0.8412638306617737
- *URL:* oculto
- *Início:* 00:39:32
- *Fim:* 00:41:36
- *Transcrição:* todos mas eu acho que a parte revisorais eu acho que sim SPM eu tenho a impressão que sim mas não tenho certeza absoluta a gente pode confirmar depois então o único cuidado que a gente tem que ter o seguinte quando a gente usa o método de seleção de atributos que envolve de alguma forma um algoritmo de aprendizado aquela escolha tá sendo feito do ponto de vista daquele algoritmo de aprendizado e ela pode não ser a melhor decisão para um outro algoritmo que não foi avaliado muitas vezes o pessoal faz isso Tá Mas não é garantido então é então o pessoal faz mas é só lembrar isso né garantida é como se a gente fizesse assim eu digo que é mais importante na minha opinião e eu estabeleço que tu vai usar só esses mais importantes mas a tua opinião é outra sobre isso né então enfim só uma analogia errada tá por isso que talvez assim do ponto de vista de selecionar algo de forma rápida com um critério para usar diferentes algoritmos seja mais interessante usar o baseado em filtro porque o baseado em filtro ele vai usar alguns critérios Claro tem inclusive o ganho de informação mas tem outros critérios estatísticos que a gente vai ver E aí basicamente é útil porque eu não tô levando em consideração né todo viesse do tipo de um modelo específico porque é diferente é usar um ganho de informação em todos os dados e eu usar uma relevância que vem da análise do ganho de informação e interativamente dentro de uma árvore de decisão ela tem uma pequena diferença porque o segundo depende da construção do modelo e o primeiro depende só de um critério de avaliação claro essa parte tô pensando aqui professora no seguinte cenário se eu cheguei no Spot Check e os meus dois algoritmos mais promissores um é a árvore de decisão e outro sei lá svm

- *Corpus ID:* 1295
- *Score:* 0.8412322998046875
- *URL:* oculto
- *Início:* 01:08:06
- *Fim:* 01:10:04
- *Transcrição:* que é o ponto central da tua pergunta sim isso a gente tem que ter muito cuidado muito cuidado mesmo tá E isso é bem importante assim essas questões essa essa personalização de conteúdo é isso ele é busca por associações ou padrões no consumo dos clientes ou no comportamento dos clientes Então tá a hora do dia por exemplo no iPhone novo tem algumas questões às vezes eu abro o iPhone e diz olha normalmente você tem um aplicativo ali destacado você abre esse aplicativo nessa hora do dia e é interessante porque a maioria das vezes funciona um padrão de comportamento meu né da Mariana então mas quando a gente pega isso com muitos consumidores ele consegue identificar algumas características talvez socioeconômicas né dependendo dependendo dos casos eles têm essas informações é homem ou mulher faixa etária qual é a região em que mora enfim mas também essa questão de interação com os s tá inclusive uma das áreas por exemplo de pesquisa que algumas empresas apostam é justamente isso é determinar a tomar decisão sobre que anúncio vai colocar ali né considerando aquele que tem maior chance de reverter em uma compra Então isso é um problema de pressão de detecção de padrão né a pessoa tá navegando Qual é o anúncio que vai ali que vai ter maior chance de reverter em Compra mesmo né Então já tem uma comentário ali do Dênis no chat então isso aí essa é uma das questões né de recomendação de informações toda parte de detecção de padrões e classificação de imagens então vocês vão ter uma disciplina de divisão computacional onde vocês vão ver que todo que a gente está discutindo de aprendizado de máquina a forma mais geral aprendizado profundo que vocês vão ver vai chegar a hora que vocês vão trabalhar com visão

- *Corpus ID:* 3254
- *Score:* 0.8411858081817627
- *URL:* oculto
- *Início:* 01:12:51
- *Fim:* 01:15:23
- *Transcrição:* eu não lembro qual vem primeiro acho que é largura versus altura largura vê essa altura então eu posso determinar lá eu quero três filtros três por três três por três eu quero 20 filtros 5 por 5 não tem problema tudo é permitido especificar tá E aí eu posso ter mais uma camada né eu posso ter a primeira camada com 10 filtros 3 por 5 por 5 5 por 5 a segunda camada e ativação reluque a segunda camada cinco filtros 3 por 3 e ativação sigmoide e aqui também enfim ao rego tá aqui né tá aqui pode ser sigmoide e olha só essa primeira não tem maxpring essa aqui tem eu posso também né não é obrigatório colocar um Max plugin depois de uma convolução tá é opcional é tem um benefício de de resumir né os mapas de características mas não é obrigatório É sempre bom colocar um antes né de passar aqui para rede clássica por causa dessa ideia de tornar mais robusto né o local exato ali da característica mas enfim eu posso organizar né E aí por isso que cria esse redes né com certeza de camadas com dezenas com centenas de camadas eu posso ir adiciodo quanto mais complexa for a minha aplicação E aí então o que que acaba acontecendo né os filtros iniciais aqui ó começam a detectar coisas mais baixo nível coisa simples primitivas né linhas em diagonal ou horizontais enfim retas né de maneira geral ou curvas né com diferentes orientações ou inclinações ou curvaturas são as coisas da primeira camada camada Inicial aqui aí depois né camadas mais à frente começam a combinar que nas círculos diferentes que nas com diferentes inclinações círculos com diferentes curvaturas e aí outras né já começam a combinar essas características mais avançadas em outras ainda mais avançadas tipo olhos né combina várias coisas

- *Corpus ID:* 3791
- *Score:* 0.8404346108436584
- *URL:* oculto
- *Início:* 01:23:44
- *Fim:* 01:25:59
- *Transcrição:* Cristiana enfim mas vocês podem para ele outra função uma melhor pode passar também o outro outra Matriz que represente esse esse gráfico de vizinhança e nós podiam calcular antes usando alguma biblioteca qualquer e depois passar para ele ele não vai só analisar essas quebras Então aí tem todo uma pessoal que trabalha com redes sociais e enfim mesmo com a web que a gente pode pode criar um gráfico da web do acoplamento entre as páginas é o que o Google faz mas basicamente no algoritmo dele de ranqueamento né E aí poderiam usar alguma técnica dessas para para o algoritmo de vocês e aí isso é interessante porque Justamente esse acoplamento aqui o que que é vizinhança pode dependendo do contexto né então se vocês criarem algum algoritmo que no contexto de vocês usa só alguns atributos alguma maneira específica de combinar esses atributos vocês vão dizer qual é a forçar digamos assim uma estrutura de Vizinhança naquele conjuntinho dados E aí o algoritmo e agrupamento ele usa essa estrutura que vocês passaram para ele como base e aí só separa aqueles que que tem essa essa distância maior então isso pode ser uma vantagem e é uma vantagem desde que você saibam usufruir dessa dessa vantagem E aí então eu trouxe aqui para vocês também para vocês verem que também não é difícil não é um exemplo dele dele funciodo então aqui embaixo tem um clancing espectral tem aqui que é interessante de vocês olharem funcionamento com um pouquinho mais de detalhe né dessa questão de análise espectável como é que ela funciona tão basicamente como eu disse a gente tem uma matriz celularidade nós precisamos fazer aplicar alguma técnica de redução de dimensionalidade


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should provide a clear answer to the question posed in the PROMPT. Supporting Evidence: The RESPONSE states that it was not possible to answer the question, which indicates a complete lack of relevance to the PROMPT. It does not address the comparison between collaborative filtering and content-based filtering, nor does it provide any reasoning or context regarding which might be considered better and why. Therefore, it fails to engage with any part of the PROMPT.  Score: 0


---
