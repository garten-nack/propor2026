**Pergunta 29**: Quais abordagens de machine learning são mais eficazes para a previsão de séries temporais de alta frequência e como lidar com o problema de ruído nos dados?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 1848
- *Score:* 0.8643913865089417
- *URL:* oculto
- *Início:* 01:07:51
- *Fim:* 01:09:52
- *Transcrição:* decisão baseada em proporções de exemplos por classe né então isso impacta menos né do que do que outros algoritmos que a gente discutiu ele impacta menos mas sempre que a gente vai trabalhar com aprendizado de máquinas de ruídos e outlanders que são pontos que vale a pena a gente pensar em tratar como pré-processamento tá mas ele é menos sensível a isso e essa questão da flexibilidade Eu já comentei né que ele é um algoritmo muito flexível em relação a suposições então a gente diz que ele é um método não paramétrico ele não tem nenhuma suposição sobre a natureza dos dados tipo de distribuição essa questão da comparação dos valores com escalas diferentes isso não impacta a árvore de decisão tá então ele é um algoritmo que a gente pode dizer assim é poucos requisitos em relação a pré-processamento algumas outras questões enfim alguns problemas né replicação de estruturas tá então pode acontecer de eu ter subarvores ou seja são conjuntos de testes encadeados que se repetem no modelo tá isso pode acontecer ou seja até mesmo sequência de testes mais em cima da árvore e depois mais abaixo da árvore isso pode acontecer por conta né da forma que ele olha só o que tá o que é melhor naquele momento não reviso que já foi feito nem o que vai ser feito no futuro elas são bastante propensas a over fiting pessoalmente quando tem uma árvore profunda Ou seja quando eu tô gerando partições super pequenas e aí a gente vai ver que algumas regras são geradas para se adaptar aquele ruído lá né do que tá no meu dado de Treinamento E aí isso gera um overflite no outro material que a gente não vai discutir hoje até posso mostrar rapidinho vai gerar um overfite tá ela não consegue detectar linearidade entre atributos e classes ou entradas e saídas tá justamente porque essas saídas sempre

- *Corpus ID:* 2176
- *Score:* 0.8576920032501221
- *URL:* oculto
- *Início:* 00:21:37
- *Fim:* 00:23:35
- *Transcrição:* vezes os dados são bons só que estão cheio de ruídos precisam de tratamento né tem que falar tem alto lá ele tem valores faltantes enfim tem que passar por um pré-processamento de quantificação categórico numérico numérica categórico tem uma série de questões ali normalização então às vezes não é só o dado não quer dizer que ele seja o suficiente para aquela tarefa ele só precisa desse refinamento digamos assim tá então isso é um é digamos assim senso comum e aprendizado de máquina vocês provavelmente Vão ouvir falar várias vezes que até garganta de alta né então por isso que a gente tem que evitar alimentar esse processo de alimentos modelos com dados que são lixo que são ruins Ok E aí tem duas questões aqui importantes né Se eu tiver dados muito bons né que representam esse diamante Se eu tivesse dados muito bons tiveram um processo de modelagem de treinamentos de modelos ruins certo então por exemplo eu não cuidei overfiting eu não tô avaliando bem o meu modelo enfim o meu resultado sim pode ser ruim eu posso estragar digamos aquele potencial dos dados em função de não explorar bem essa modelagem na parte de Treinamento avaliação e seleção dos modelos agora quando eu tenho dados ruins mesmo que eu faço essa metodologia de treinamento do modelo né mas da parte já é essa divisão dos dados dados Independentes treinamento a validação enfim se eu fizer isso de forma Impecável se os meus dados forem muito ruins essa modelagem Impecável do ponto de vista de metodologia ela não salva esses dados ela dificilmente salva esses dados certo então a gente tem que cuidar a gente tem que entender o que que a gente precisa trabalhar em nível de dado né para que a

- *Corpus ID:* 1999
- *Score:* 0.855917751789093
- *URL:* oculto
- *Início:* 00:20:45
- *Fim:* 00:23:02
- *Transcrição:* conseguia acertar porque todo mundo aqui disse que era zero né todos os modelos classificaram como zero mas ela era um então eu não acertei tá uma das coisas que eu queria perguntar para vocês olhando para esses dados né o que que vocês acham que é a principal característica que faz com que eu consiga nesse modelo un samba que é esse e de fato melhorar o meu desempenho em relação a esses individuais em relação tomadas de decisão desses modelos né O que que vocês observam como característica que faz com que ao fazer uma votação majoritária eu consiga né Por exemplo o que tá acontecendo aqui melhorar substancialmente o desempenho alguma intuição a respeito dos modelos ele gerar as mesmas saídas [Música] sim a pergunta era o seguinte o que que a gente o que que vocês conseguem observar a respeito desses modelos e das suas predições que faz com que né uma característica que faz com que ao agregar essas decisões por uma votação majoritária eu consiga acertar a maioria das instâncias e principalmente aumentar o desempenho em relação a eles né então característica em relação as predições que eles estão fazendo diminuição das variâncias composição a diminuição das variâncias de ruídos e hotlines suavização de ruídos Mas alguma coisa depois eu vou comentar quero que vocês tragam a intuição de vocês e alguém Se alguém quiser comentar mais alguma coisa deixa eu ver que tem no chat então vocês falaram coisas bem interessantes assim nesse caso de variância como eu tô falando de três modelos diferentes não dá para dizer necessariamente que eu tenho uma variância de modelo né porque a variância Seria tipo o mesmo o mesmo se eu tivesse o mesmo algoritmo com dados diferentes aí eu poderia ter

- *Corpus ID:* 7618
- *Score:* 0.8546535968780518
- *URL:* oculto
- *Início:* 00:02:39
- *Fim:* 00:05:03
- *Transcrição:* eh acho que as aulas mais importantes né para para entender essa base foi foram a a de sexta-feira com Cláudio né E essa primeira metade aí tá é claro que a gente não quer necessariamente que vocês saiam né aplicando wavelets e entendendo exatamente como é que esse negócio funciona e tudo mais E por que que tem os subespaços vetoriais e blá blá blá Não nada disso né o que a gente quer com esse monte de coisa é mostrar que existem informações que são melhor percebidas na eh num nível de de resolução do que em outro tá eh e no final das contas a ferramenta pré Deep learning que melhor fazia isso até então era essa de Tá além disso a ideia também era mostrar que a noção de frequência né Ela é importante eh paraa filtragem e para outras tarefas né como reconhecimento de de partes ali né como a gente viu no no no último exemplo do da imagem com ruído né h e que essa noção de frequência ela é compartilhada querendo ou não com a parte de fourrier alguma coisa aqui de wavelets também então a gente tem a ideia de cada nível de decomposição captura uma frequência diferente né e por sua vez frequência filtragem de por frequência no domínio fourrier pode ser também né feita de uma forma um pouco menos eh inteligível digamos assim no domínio espacial original né que foi alguma coisa que o Claudio deve ter mostrado lá para vocês do na galciana no espaço no e no e no espectro né Tá então acho que essas aulas são as mais importantes definitivamente pré eh até agora pelo menos tá pra gente entender o resto mas pra gente entender essas aulas a gente também precisava das aulas anteriores né então assim eh de novo é uma disciplina introdução programa introdução a apressamento de imagens É uma disciplina de 60 horas introdução a visão computacional É uma disciplina de 60 horas a gente a gente junta as duas

- *Corpus ID:* 1871
- *Score:* 0.8545252084732056
- *URL:* oculto
- *Início:* 00:12:57
- *Fim:* 00:15:14
- *Transcrição:* esses aqui o que que acontece os dados de teste não tem esse ruído porque ele é um porque Justamente eu não falei ele é uma questão aleatória ele pode ocorrer Como pode não ocorrer mas como usar os treinamentos possuem o ruído e o modelo foi treinado né para aprender digamos seu melhor possível usar de Treinamento ele também se ajustou aquele ruído E aí essa esse detalhe que ele criou na fronteira de decisão Para incorporar aquele ruído do lado correto da Fronteira de decisão acabou fazendo com que ele erra e para novos dados Porque de fato essa região aqui ela é uma região que tende a ser só botar aqui ela é uma região que tem de ser de dados azuis aqui certo Essa região é que ela tem de ser de dados azuis só que agora essa região tá sendo classificada como laranja porque ela se especializou se ajustou né aquele ruído Então esse é o problema de overflite né ruído e de outro item o que acontece é que os modelos eles se sobre ajustam aos dados de Treinamento Então aqui estão sobre ajuste né ele ele se ajusta tão bem que ele incorpora inclusive o ruído e isso faz com que ele perca o poder de generalização nos dados de teste porque Todas aquelas instâncias que estão caindo nesse espaço de entrada que é esse retângulo aqui né do dado de teste que deveriam ser instâncias classificadas como azul porque é a tendência dos dados elas não vão mais ser classificadas como azul por causa de um ruído que tinha nos dados que fez com que o modelo ajustasse a fronteira de decisão Então esse aqui é um caso Claro é um exemplo bastante simples mas eu acho que ele mostra bem o problema da gente ter ruído E aí isso se reflete não só em mais erros mas também Vejam só numa árvore de decisão bem mais complexa aqui né então por causa de um ruído eu tive que aumentar especializar a fronteiras decisão E isso acaba refletindo no modelo mais complexo

- *Corpus ID:* 1718
- *Score:* 0.8543407320976257
- *URL:* oculto
- *Início:* 00:38:30
- *Fim:* 00:40:36
- *Transcrição:* tiver ruído nos dados porque quando eu tenho ruído seu modela aquele ruído eu tô me sobre ajustando aquela característica particular do dado que por ser um ruído é aleatório não necessariamente vai acontecer nos dados de teste então svm ele funciona bem a gente tem também a opção de trabalhar com ele para regressão tá E aí só falando um pouquinho assim de algumas vantagens desvantagens né claro se a gente for pensar em problemas de classificação que tem uma margem uma separação muito clara ou seja se a gente for olhar projetar esses dados a gente percebe que as classes elas estão bem desjuntas assim bem separadas é claro que o svm vai funcionar muito bem né mesmo que essa Fronteira seja não não linear ou seja Às vezes a gente tem alguma sobreposição em uma parte né menos densa e a parte mais densa das classes elas estão mais distantes ele vai funcionar bem certo E mesmo quando ele tem muitos atributos em relação ao número de instâncias né problema de auto dimensionalidade ele também costuma ser efetivo ufpm e aqui eu comentei já vocês que ele é super versátil porque com um algoritmo a gente consegue gerar muitos modelos né variando a função de quermel variando se preparamos de termos de regularização Gama enfim Tá além do que ele também permite que a gente aplica problemas multiclasse embora isso gera um custo maior no sentido de que a gente vai estar sempre treido mais classificadores do que um único classificador tá então a gente vai estar treido por exemplo cinco classes no mínimo cinco classificadores se eu usar a estratégia um ano versus Existem algumas desvantagens tá que na prática algumas a gente já são contornados para vocês saberem o algoritmo ele naturalmente não estima probabilidade para a classificação embora eu tenha dito para vocês aquela ideia de que qualquer

- *Corpus ID:* 1847
- *Score:* 0.8539491891860962
- *URL:* oculto
- *Início:* 01:06:13
- *Fim:* 01:08:20
- *Transcrição:* corte idade menor ou igual a 35 esse 35 ele não chegou aleatoriamente ele testou valores de 35 ele tem uma estratégia interna para pegar alguns valores que são mais promissores e entre esses mais promissores e a qual divide melhor as classes então ele lida automaticamente com isso Só que essa parte dos atributos numéricos ele é um custo computacional mais alto para o algoritmo então não quer dizer que ele não se beneficia de um pré-processamento se a gente tiver um atributo numérico né e através do mérito discretizar esses dados né enfim de acordo com o valor sol em categorias para árvore de decisão é bom no sentido de rodar mais rápido tá o algoritmo vai ser mais rápido no treinamento Beleza então a gente não precisa se preocupar com normalização assim não a normalização não interfere não vai prejudicar mas ela não é necessária dele que o volume ele tem realmente é muito bom para tudo isso né a gente tem que fazer menos tratamento Porém quando o volume de dados é muito grande ele não é muito bom exatamente essa questão do volume ele pode ser um pouquinho mais pela questão né do Justamente que ele vai fazendo essas divisões sucessivas né ele é um atributo que ele é um pouco menos sensível Auto lion não quer dizer que não seja sensível tá por isso que eu botei menos tá mas ele é um pouco menos sensível alto lá ele porque ele faz essa divisão para determinar Fronteira de decisão baseada em proporções de exemplos por classe né então isso impacta menos né do que do que outros algoritmos que a gente discutiu ele impacta menos mas sempre que a gente vai trabalhar com aprendizado de máquinas de ruídos e outlanders que são pontos que vale a pena a gente pensar em tratar como pré-processamento tá mas ele é menos sensível a isso e essa questão da flexibilidade Eu já comentei né que ele é um algoritmo muito flexível em relação

- *Corpus ID:* 8553
- *Score:* 0.8538880348205566
- *URL:* oculto
- *Início:* 00:54:37
- *Fim:* 00:56:56
- *Transcrição:* e vai dividindo interativamente então eu parto aqui do grandão e vou vou estabelecendo os menores ah depois ou Bottom up onde eu começo das instâncias né E aí cada Instância pertenceria a um a um grupo de distinto e vou e vou combido esses grupos de maneira interativa então tem esse agglomerative clustering isso aqui tem os links vocês podem ver as um exemplo de implementação desses algoritmos e depois tem um que eu acho que vocês não viram mas devem ter ouvido falar ouvido o conceito talvez não viram na prática que é algoritmos baseados em densidade né então eles são grupos com uma ou poucas instâncias eh são formados inicialmente E aí de maneira interativa vão recebendo mais estâncias localizadas em sua vizinhança sempre tentando gerar clusters densos o que que ele faz ele separa por regiões de baixa densidade então que esses algoritmos Eles encontram formas como essa bem diferentes eh e eles também são muito usados para identificar pontos de ruído porque nem todos os pontos eles classificam nesse algoritmo por partição aqui especialmente todos os pontos vão estar assignados né vão estar eh classificados em um cluster né e acredito no hierárquico também talvez ten alguma variação aí que que possa permitir ficar fora nesse não se ele tiver numa área de baixa densidade ele não considera ele tem que estar num mínimo lá H dentro de um grupo para fazer parte então por isso esse tipo de algoritmo é muito usado para identificar outliers ou ruídos que são aquelas coisas que às vezes não deveriam nem estar ali entendeu que é uma um documento uma sentença que tá fora assim do é um exemplo é esse DB Scan e tem o hierarchical DB Scan que também tem uma questão hierárquica que a gente vai ver no no exemplo do ber top que ele usa como é o top hoje o algoritmo de cluster é o aid de bisca a gente vai ver mais dele

- *Corpus ID:* 1776
- *Score:* 0.8536843061447144
- *URL:* oculto
- *Início:* 00:42:52
- *Fim:* 00:45:11
- *Transcrição:* beleza certo então esse não tem aquele problema de falta de mineralização né ele vai realmente vai estar naquele quadradinho de acordo com a regra né só que folha né e não tem problema de falta de generalização né na realidade a gente vai ver que esse algoritmo ele até pode chegar nesse problema de falta generalização para novos casos quando ele tenta se especializar muito tá vou dar um exemplo vamos supor que eu tivesse aqui um círculo tá vamos supor que tem um círculo isso aí eu tô talvez adiantando alguma coisa que a gente vai discutir mas se eu tivesse um círculo aqui essa região do lado esquerdo inferior ela não tá 100% ocupada por uma classe ela tem uma ainda um nível de heterogeneidade tá E meu coisa tá termido aqui a bateria e aí o que acontece é que eu poderia por exemplo o meu modelo poderia tentar fazer essa divisão ainda certo então quando o modelo começa a se especializar E aí vamos supor que eu tivesse aqui um triângulo tá isso meus atos de Treinamento E aí meu modelo de fazer isso aqui então desculpe o que acontece é que meu modelo ele foi criando partições muito específicas para um tipo de dado aqui que pode ser um ruído que é esse aqui E aí aqui pode gerar um overfito em que seria dificuldade de analisar para novos casos tá bom demais porque nas métricas de treinamento de teste ele Nossa tal Sei lá 99% só que na hora de ver dados novos ele vai exatamente aí o que vai acontecer nesse caso aqui né Se fosse caso é que um novo dado que cai aqui por exemplo aqui ficou meio confuso porque eu botei o Deixa eu só trocar aqui vamos supor que

- *Corpus ID:* 1474
- *Score:* 0.8532277941703796
- *URL:* oculto
- *Início:* 01:04:14
- *Fim:* 01:06:16
- *Transcrição:* ver o quanto ele se sai bem para generaliza né esse modelo quando eu pego dados que ele nunca viu e usa esse modelo e consigo aí eu consigo avaliar ele tem esse conjunto de treinamento e a partir desse treinamento ele gerou um modelo esse modelo sai dessa forma para dados que ele nunca viu então a gente precisa realmente avaliar com instâncias nunca vistas tá quando a gente tem modelos que falham que seja mais que estejam com os labels né eu sei que você não falou tá implícito mas ele não viu e tem que estar classificado e tem que ter já o lei bom lá para a gente ver essas métricas isso exatamente e quando esse modelo falha em generalizar essa falha de generalizar ela vai se ela vai ter um desempenho ruim né E aí a gente tem duas origens principais né relacionadas ou um modelo que a gente tá usando ele é muito simples ele não é suficiente para estranho aqueles padrões nos dados né então ele não consegue lidar com essa complexidade do padrão de associação entre saída ou esse modelo se especializou muito bem nos dados de Treinamento então ele quando ele sai muito bem nos dados de Treinamento às vezes ele não consegue prever novos dados porque ele incorpora inclusive ruídos ou particularidade esses dados outlanders Enfim então ele acaba gerando uma fronteira de dizer Fronteira de confusão uma fronteiras de decisão e se especializa na característica da realidade de treinamento e isso pode fazer com que ele erra em novas novos dados no conjunto de teste porque esses dados não tem desculpem essas mesmas ruídos os dados de Treinamento tá é só para vocês entenderem essas duas estratégias essas duas pessoas causas né de falha e generalizar elas têm dois nomes bem conhecidos bem específicos assim que a gente que vocês vão ouvir muitas vezes que é o Wander fiting E o


**Resposta gerada pelo LLM**: Não foi possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to the approaches of machine learning for high-frequency time series forecasting and how to handle noise in the data. Supporting Evidence: The RESPONSE states that it was not possible to answer with the context provided, which indicates a complete lack of relevance to the PROMPT. It does not address any part of the question regarding machine learning approaches or noise handling, and it does not provide any information or insights related to the topic.  Score: 0


---
