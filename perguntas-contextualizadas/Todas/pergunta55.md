**Pergunta 55**: O que é fine-tuning?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 3460
- *Score:* 0.8537889719009399
- *URL:* oculto
- *Início:* 00:16:28
- *Fim:* 00:19:09
- *Transcrição:* tá bom obrigado Ok E aí assim ó se aí vai eu vou o próximo assunto também vai abordar um pouco disso que é o fine tudo aqui é o transferir Ou seja eu adiciono né uma camada nova na rede né eu pego o conhecimento da rede anterior e adicionar novas camadas aí depois o finnetone pode ajudar um pouco nisso nessa questão tá então só voltando aqui ó agora o finitune ou seja o ajuste fino né eu posso eu posso fazer dessa forma aqui que aí é o transferir Então vou colocar aqui ó e em conjunto ou né de maneira isolada a gente pode fazer o Fini só que no fine tuning a diferença principal é que a rede não é congelada toda né então eu não congelo não congela E aí eu posso nem precisar de nem adicionar novas camadas porque pode ser Aquele caso por exemplo do data 7 né de treinou em rostos ocidentais E aí tá ruim em rostos orientais então eu coleto imagens de rostos orientais né e agora eu vou aproveitar todo o treinamento que foi feito antes né então carrego com os pesos tudo e aqui eu posso inclusive manter pode ser né que eu pego toda a rede que tava antes pronta né com as suas camadas o mlp o classificador dela já original e aí o que que vai acontecer eu vou é como se fosse um pós treino né a rede já tava treinada eu vou fazer um treino adicional com meu data 7 pequeno né E aí ela vai se ganhar mais conhecimento em cima né Desse data 7 das imagens que estão contidas aqui né Isso é o fine tune o ajuste fino fine ok aqui da forma que eu apresentei né eu tô fazendo outras perlane ou fine tune mas dá para fazer as duas coisas as duas coisas vamos lá deixa eu voltar aqui no blog rapidinho não tá não precisa botar nesse momento tá já vou voltar lá deixa eu adicionar uma nova página

- *Corpus ID:* 6933
- *Score:* 0.842645525932312
- *URL:* oculto
- *Início:* 00:51:15
- *Fim:* 00:53:22
- *Transcrição:* esses ajustes Normalmente eles têm a ver com a limpeza dos dados esse tipo de coisa ou eventualmente com a filtragem né tipo ah eu quero remover algum dado que tá ruim então eh tem outros ajustes também dependendo do que pode ser mais custosos né então por exemplo se a gente tá trabalhando com imagens e a gente quer detectar se uma imagem ela está num terada orientação por exemplo a gente tá trabalhando com imagens eh o Wide screen e vamos supor que a nossa rede precise de imagens que sejam h mais de pé assim então tem que fazer a rotação notem que rotacional a imagem não é uma uma operação barata tem que mudar a a base de de eh a escala né Eh eventualmente talvez remover as cores eh né tipo transformar de RGB para Tons de Cinza isso é um é um processo que tem que aplicar para cada pixxel tu tem que aplicar uma função tu quer normalizar depois de transformar para números de 0 a 255 porque são tonalidades de Cinza do que é normalizar entre zero e um então todas essas operações elas são prévias ao treinamento e elas têm que fazer então de alguma forma né então o tensor falou permite a gente criar Essas funções de usuário para fazer esse tipo de operação e tendo em vista que é bem Custoso a gente pode usar então de novo esse parâmetro num paralel caos para fazer quantos itens a gente vai fazer em paralelo sempre na ideia na hora de fazer o fine tuning ali do nosso treinamento né especificar uma quantidade de cores aqui para fazer com que a g né que é o nosso objetivo principal fique o 100% tempo sendo usada né a gente não quer osidade na GPU Então a gente vai usar vários cores aqui para preparar esses esses aplicar Essas funções de limpeza né enfim de de transformação dos dados de maneira eficiente em paralelo então para fazer isso a gente usa o método map a gente fornece então uma função

- *Corpus ID:* 3461
- *Score:* 0.8417847156524658
- *URL:* oculto
- *Início:* 00:18:30
- *Fim:* 00:20:54
- *Transcrição:* cima né Desse data 7 das imagens que estão contidas aqui né Isso é o fine tune o ajuste fino fine ok aqui da forma que eu apresentei né eu tô fazendo outras perlane ou fine tune mas dá para fazer as duas coisas as duas coisas vamos lá deixa eu voltar aqui no blog rapidinho não tá não precisa botar nesse momento tá já vou voltar lá deixa eu adicionar uma nova página que agora a gente vai fazer transferir e farne tune né então aqui ó como é que vai ser os dois deixa eu recapitular primeiro transferência de aprendizado transferência de aprendizado eu carrego a rede e adiciono novas camadas e vou treinar só essas novas camadas isso é transferência de aprendizado transformando no contexto de redes neurais tá existe outro significado para transferir no contexto de redes neurais é isso eu adiciono novas camadas que aí essas novas camadas né vão aqui ó se especializar ou seja não tô usando a rede na aplicação original dela né eu tô aproveitando que ela sabe mas eu tô usando aqui no meu né na minha aplicação tá agora o fine tune eu aproveito tudo que a rede já sabe e eu vou só adicionar digamos adicionar conhecimento a ela né então eu pego novos dados e É como se eu continuasse né o treinamento da rede né Essa rede que foi treinada no mês NET então eu a partir dali eu continuo né então eu não congelo a rede original eu vou ela vai ter todos os pesos ajustados em cima do meu j7 pequeno isso é o ajuste Fino que é bem nesse contexto né um tipo de imagem ou de classe que a rede estava tendo dificuldade então eu vou dar uma fazer a rede né ficar um pouquinho mais especializada naquele naquele novo conceito e agora combinar isso é eu não eu posso não congelar eu posso não congelar aqui ó não congelar não congelar e aí e ao mesmo tempo que

- *Corpus ID:* 8535
- *Score:* 0.8393320441246033
- *URL:* oculto
- *Início:* 00:21:59
- *Fim:* 00:24:22
- *Transcrição:* esse modelo de fato estabeleceu o que a gente chama o paradigma do pré-treino e do fine tuning que que é o pré-treino o PR treino é Aquela fase que o modelo ele é então tem essa arquitetura de Transformers no caso do Bert base o base né Ele tem 12 camadas de Transformers e ele é trinado de uma maneira que a gente chama autos supervisionada que nada mais é do que ele usa o próprio texto como rótulo Então como é que ele faz isso ele tem dois objetivos de Treinamento um é a linguagem mascarada que é ele ele ele mascara alguns tokens em torno de 15% dos tokens do treinamento e ele tenta predizer esse esses tokens à medida que ele vai otimizando né e ele também tem o Next sentence prediction que é ele tenta prever a qual seria a próxima sentença mas de verdade para as edens a a o objetivo que mais contribui é essa linguagem mascarada e o fato dele ser um modelo bidirecional O que que significa isso que ele olha nas palavras ele ele olha tanto paraa direita como paraa esquerda tá então ele aprende o contexto muito bem em que uma palavra tá inserida e ele então foi isso foi muito importante porque que gerou com com essa fase de pré-treino gerou um modelo excepcional em entender o contexto né e gerar representações que a gente já vai ver como muito úteis E aí tem a segunda fase que é Tá bom agora eu pego esse modelinho lembra que eu expliquei lá o o Bert a gente coloca uma substitui uma head lá né que a gente chamou de uma uma cabeça do modelo que seria uma camada ou n camadas para uma tarefa específica que a gente quer realizar e aí ele nesse esse pré-treino ele aprendeu a estrutura da língua aprendeu esses significados essa relação das palavras e nessa outra parte do treino que seria então o fine tuning que que nós fizemos fine Tuning para Americanas classificação eu especializo esse modelo então para tarefa o Squad aqui é uma tarefa de

- *Corpus ID:* 3462
- *Score:* 0.8390911817550659
- *URL:* oculto
- *Início:* 00:20:13
- *Fim:* 00:22:47
- *Transcrição:* congelo a rede original eu vou ela vai ter todos os pesos ajustados em cima do meu j7 pequeno isso é o ajuste Fino que é bem nesse contexto né um tipo de imagem ou de classe que a rede estava tendo dificuldade então eu vou dar uma fazer a rede né ficar um pouquinho mais especializada naquele naquele novo conceito e agora combinar isso é eu não eu posso não congelar eu posso não congelar aqui ó não congelar não congelar e aí e ao mesmo tempo que eu adiciono novas camadas né então aqui é transferir Vou colocar aqui ó transferir Tony mas ajuste fino porque eu tô adiciodo novas camadas Então vou meio que aproveitar o que a rede já sabe mas eu vou treinar ela eu vou permitir que toda ela seja treinada né então a parte de adicionar novas camadas é o transferir e a parte de permitir que todo o restante dela seja treinada é o fine tunen Então posso fazer os dois ao mesmo tempo essa que é a grande sacada aqui de qualquer forma né a gente tá usando redes pré treinadas Ok meus dois as duas modalidades e aqui o como que se faz isso né a ideia de congelar as camadas né o se ele quiser adicionar uma nova camada em cima da rede ele aquele tá criando uma camada adicional né E aqui inicializa os pesos e aqui um Beth não pode deixar eu queria ver se tem um exemplo completo aqui do da rede funciona [Música] aquilo tá criando um novo modelo e aquilo só tá mostrando congelar tá ele não tá pegando a rede para treinar daqui eu queria pegar a rede pré-treino aqui agora sim transferir ó carregou o modelo treinado no Imaginext que aceita esse formato de entrada e não não incluiu o topo né então é transferir tá no transferir aqui ó deixa eu voltar no slide tá aqui no

- *Corpus ID:* 7737
- *Score:* 0.8384712338447571
- *URL:* oculto
- *Início:* 00:13:01
- *Fim:* 00:15:28
- *Transcrição:* treinado numa tarefa de classificação usando Esse imaginet tá nos últimos anos tem surgidos outros datasets ainda maiores com outras tarefas que podem ser usados para fazer esse pré-treino tá mas assim essa ideia de transfer learning ela é crucial Tá o que que já adiantando é fundamental quando eu tenho um backbone tá eu eu forneço arquitetura tá então essa arquitetura eventualmente vai ter uma resolução de entrada Tá mas mais do que isso provavelmente durante o treinamento desse backbone eu tenho algum processo de normalização dos dados né tipo entre 0 e 1 entre 0 255 Então se o meu backbone foi prét treinado com imagens entre zero e 1 e eu quiser usar um novo dataset eu tenho que usar o mesmo tipo de pré-processamento tá Ah o outra coisa é um detalhe muito pequeno Tá mas assim no opencv o defo é lei imagem nos canais BG R se eu usar o matplot Lib eu eu eu leio no RGB tá então se eu treinei assumindo a ordem RGB e depois quiser rodar com bgr não vai funcionar direito tá então é importante entender como o backbone foi pré treinado para reproduzir essas características na hora de reaproveitar Ok a outra questão importante é bom às vezes eu tenho back Bones que são gigantescos tá sei lá daqui a pouco eu tenho 100 camadas quais dessas camadas eu vou aproveitar e quais que eu vou deixar livre pro método treinar tá E aí novamente não tem uma uma resposta óbvia para isso eventualmente eu posso ah deixar todo o backbone congelado não preciso nem mexer nada eu só altero lá lá na parte final da rede tá Talvez isso funcione eventualmente eu tenho que descongelar as camadas finais tá Ah e uma outra estratégia que é intimamente relacionada ao transfer learning é o fine tuning Tá o que que o fine tuning faz então eu pego um backbone deixo ele congelado aí eu treino a Red final aí pro meu dado e meu

- *Corpus ID:* 7738
- *Score:* 0.8383221626281738
- *URL:* oculto
- *Início:* 00:14:49
- *Fim:* 00:17:15
- *Transcrição:* para isso eventualmente eu posso ah deixar todo o backbone congelado não preciso nem mexer nada eu só altero lá lá na parte final da rede tá Talvez isso funcione eventualmente eu tenho que descongelar as camadas finais tá Ah e uma outra estratégia que é intimamente relacionada ao transfer learning é o fine tuning Tá o que que o fine tuning faz então eu pego um backbone deixo ele congelado aí eu treino a Red final aí pro meu dado e meu problema vou chegar num resultado que satura beleza feito isso agora eu descongelo todo mundo diminuo o learning rate né só para dar uma uma pequena alterada nos pesos e treino tudo de novo tá então esse tipo de estratégia que eu começo com um transfer learning e depois faço um fine tuning em geral ele acelera o treinamento e gera bons resultados beleza bom como todas as minhas aulas a parte dois Começa depois da metade do intervalo Mas não tem problema porque ela é menor mesmo tá ã agora a gente vai tentar fazer um pacote completo tá tá o tudo que a gente viu até agora ele tem uma imagem com entrada e gera um feature map gera atributos como é que eu vou ah bolar projetar as camadas finais da minha da minha rede tá E aí obviamente que a maneira na arquitetura dessas camadas finais é altamente dependente do tipo de dado que a rede vai produzir Tá e isso impacta também a função de ativação que vai ser usada na última camada né como eu falei para vocês a função de ativação a relu por exemplo ela é muito utilizada para facilitar a propagação da informação nas camadas intermediárias da rede mas na camada final é fundamental que a minha função de ativação mapeia os valores da rede para algo que eu consiga interpretar no meu problema tá por exemplo

- *Corpus ID:* 8539
- *Score:* 0.8372142314910889
- *URL:* oculto
- *Início:* 00:29:14
- *Fim:* 00:31:46
- *Transcrição:* coisa a gente ele descobriu uma né fez um modelo bom fez F tuning mostrou que o modelo dele era super bom E aqui tá os números ó do Bert lard que são 24 Transformers e aqui o base é 12 então ele chegou a valores enormes aqui pra época Acho que até hoje os modelos derivados da arquitetura aber são os estado da arte para essa tarefa aí ele fez o que ele chamou de um ablation study tipo um Abalo no que ele fez onde ele disz sabe o que eu vou tentar eu vou usar uma abordagem que ele chamou de feature based approach isso aqui nada mais é do que usar embeddings então ele extraiu as badings desse desse Bert aqui que ele tinha treinado não do fine tuning o que ele tinha treinado né pré-treino E aí ele usou uma série de estratégias de pulling onde ele começou a fazer a média então desses layers todos eh e aí os resultados são surpreendentes porque ele fez várias estratégias aqui por exemplo do segundo até o último layer o somente o último layer e a melhor maor delas é concatenar os quatro últimos layers quatro últimos layers eh concatenar essas edens aí fica um né um tamanho grande ele chegou aqui 96.1 mas poderia usar aqui ó somar de maneira né ponderada os 12 leeves então tem várias estratégias O que que a gente denota daqui chegou muito perto do fine tuning tá claro que nesse tipo de trabalho o o estado da arte assim quando o cara dá um pulinho de de 96.2 para 96.4 isso é comemorado isso é mostrando que isso tem significância estatística né que de fato ele vai ele é 0.2 maior Isso é muito bom até o pessoal fala assim antigamente tu subia tu tu vir com um modelo de 70% de acurácia por exemplo e subir isso para 90 isso é fácil o que é difícil quando tu chegou nesse limar acima do dos 90 daí para tu te diferenciar é difícil então aqui é só mostrando que ele mesmo nessa época Já disse olha Essas edens são muito boas como é que ele usou ela

- *Corpus ID:* 7776
- *Score:* 0.8355535268783569
- *URL:* oculto
- *Início:* 01:25:48
- *Fim:* 01:28:22
- *Transcrição:* Range é quanto eu vou girar n é quanto eu eu vou shiar na na escala na horizontal ou na vertical tá E aí de novo não tem não tem uma estratégia muito clara de saber o quanto eu vou mexer em cada um desses caras aí tá E aqui tem algumas versões aumentadas de uma única imagem então na prática o que que eu faço eu multiplico artificialmente os meus dados de entrada e o que que eu espero que aconteça com data augmentation eu espero que a minha rede consiga generalizar melhor tá então na hora de treinar com data augmentation eu chamo o Model Fit também com data com meu data generator certo e aqui tá o resultado o que que aconteceu com acurácia de treino parada com anterior Caiu um pouquinho ela caiu ela não saturou Em Um tá na verdade eu acho que até tem o potencial de deixar treido por mais tempo tá mas por outro lado o que que acontece com GAP entre acurácia de treino e de validação diminuiu ela também diminuiu então parece que ela está generalizando melhor ou over fitando menos tá e uma outra coisa que a gente ainda pode fazer é feito esse treinamento Inicial Agora eu vou fazer o fine tuning que é eu vou agora descongelar o backbone e vou treinar tudo do zero com learning rate menor tá E aí o que que acontece agora com a curá de treino ela aumentou né ela ficou mesmo com data augmentation ela chega perto de de um né e acurácia de validação ela deu uma boa aumentada tá lá perto de 0.875 tá então no final das contas neste nessa nesse treinamento é isso que a gente chegou tá acurácia de teste sem augmentation deu 80% com augmentation subiu para 81% Olha o ganho foi pequeno tá e com augmentation e fine tuning subiu para 88% nos dados de teste tá se eu fizer sem augmentation e f tuning como é que vai ficar vocês

- *Corpus ID:* 3465
- *Score:* 0.8272849321365356
- *URL:* oculto
- *Início:* 00:25:39
- *Fim:* 00:27:55
- *Transcrição:* de base né esse esse jeito aqui né o primeiro caso o primeiro caso ele congela adiciona é uma nova camada e aqui ó em vez de adicionar sua mãe poderia adicionar um perceptro mais com camadas ocultas também né mas ele quis condicionar só uma sem camadas ocultas quer dizer né então antes dessa densa de sair daqui Vocês poderiam adicionar outras também tá mas enfim esse é o pcpton desse cara que em especial E aí ele treinou né fez o feed E aí depois de fazer o feed ele quis fazer o fine tuneing então ele descongelou o modelo base e mandou treinar de novo bem nessa modalidade aqui ok Opa tô salvando aqui mesmo não precisa ok pessoal dúvidas esse Fire também com os dados né para treinados não corre o risco de ficar muito lento assim porque é uma rede é a Rede Inteira né que pode ser treinado E aí então tem como fazer né menos agressivo digamos assim porque aquele código não mostra isso né mas você pode escolher quais camadas você congela né então eu congelo só a parte inicial da rede então congela aqui e não congela a parte final né então isso você pode ajustar isso para de acordo com o seu poder computacional porque realmente fazer o propagation lá até o começo a rede pode ter centenas de camadas né Isso pode demorar muito E aí você pode definir né tem como pegar frações da rede aqui e congelar só descongelar né só essa fração aqui deixar o resto congelado no código tem essa página o link lá que eu abri tem isso tem como que você escolhe camadas para congelar ou não eu só eu pulei essa parte fui lá para baixo para mostrar a parte mais conceitual ali mais completa Ok mas se eu não tivesse a rede pré-treinada totalmente congelada do início ao fim mesmo assim eu eu teria resultados razoáveis né no caso aqui de transferir eu manter ela


**Resposta gerada pelo LLM**: Fine-tuning, ou ajuste fino, é um processo onde se aproveita todo o conhecimento prévio de uma rede neural já treinada e adiciona-se conhecimento adicional a ela, utilizando novos dados.  É como se continuasse o treinamento da rede, sem congelar a rede original, ajustando todos os pesos com base nos novos dados.  Pode ser usado para especializar a rede em um tipo específico de imagem ou classe onde ela tinha dificuldades.  Também pode ser combinado com a adição de novas camadas (transfer learning).

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the PROMPT by defining fine-tuning and explaining its process and applications. It provides a clear and comprehensive explanation of how fine-tuning works, including the concept of leveraging pre-trained neural networks and the potential for specialization. This relevance to the entire PROMPT justifies a high score.  Score: 3


---
