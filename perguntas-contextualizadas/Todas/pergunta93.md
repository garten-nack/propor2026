**Pergunta 93**: Qual a principal diferença entre o modelo de linguagem Elisa e o modelo Duplex Voice? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8021
- *Score:* 0.8460583686828613
- *URL:* oculto
- *Início:* 00:44:27
- *Fim:* 00:47:08
- *Transcrição:* serving for aut for Now okay we 10 Okay your the first is Lisa Okay Perfect so I will see Lisa at on mayd have a great day bye então aqui a gente já tem uma uma interação bem mais bem mais complexa né que que soa bem natural Ela não é uma não é uma assim ah eu quero marcar um um corte de cabelo paraas para meio-dia Ah tá marcado pronto não tem Idas e Voltas o horário não tá disponível H então foi já demonstra bem mais sofisticação e a gente tem Claro atualmente os assistentes de voz que são eh bem eh difundidos eu acho que a a Alexa da da Amazon é uma das das mais populares depois mais recentemente tem uma perguntinha professora Ah pode falar Opa V professora Então esse último exemplo bastante realista né já já se baseou em redes neurais eu fiquei com a curiosidade e os anteriores lá a Elisa e o e o eugene ah Quais as tecnologias ali envolvidas a Elisa era baseada em regras simplesmente então o desenvolvedor criou uma uma série de padrões ah se a pergunta diz eu estou assim ou eu gostaria de X aí ele responde Ah o que significaria para você se você tivesse x ou então tinha uma série de de padrões já definidos E aí por exemplo quando ele não sabe o que resolver então ele ele eh coloca a a resposta Vamos mudar de assunto fale mais de outra coisa então ele ele ele tinha esse conjunto já implementado o ean eu não lembro o que que h não acho que não era ainda não era nada de de Deep learning a mina sim a mina não desculpa a duplex Voice essa sim já era baseada em em redes neurais recorrentes e esse próximo que a gente vai ver que é o lambda então já é uma já era uma uma rede neural mais avançada treinada prét treinada com um conjunto bem grande aqui 1,5 5 trilhões de palavras e aí ela foi ajustada para tarefas de geração de texto e ela ganhou

- *Corpus ID:* 8266
- *Score:* 0.8334090709686279
- *URL:* oculto
- *Início:* 00:18:39
- *Fim:* 00:21:07
- *Transcrição:* espaços então ele é h mais adequado para idiomas que não têm espaços e e ele é uma generalização que permite lidar com esse tipo de idioma porque o wordpiece ele ele e o e o bpe ele supõe essa separação supõe que o espaço é um separador de palavras então ele adiciona o espaço à lista de caracteres e depois aplica o ppe E aí o Albert o Excel net e o T5 usam o o sentence pce e bom então tudo isso é pra gente conseguir entender eh o o funcionamento das edens contextuais então a gente tá entrando nessa quarta onda aqui que foi bem grande começou em 2018 que foi com os modelos baseados em Transformer cujo foco inicial eram tarefas de compreensão de linguagem natural language understanding e não tarefas de geração tarefas de geração a gente vai ver na na próxima semana então a gente vai falar um pouco da motivação um pouco de background vamos falar do Elmo do birth e diversões para português e tendências e e alguns algumas questões relacionadas também então problemas com as edins fixas a gente já viu que ã no que as embeddings fixas Elas têm as palavras são independentes do contexto Ou seja eu Gero um vetor para cada palavra separada então aquele exemplo do havia uma fila enorme no Banco por causa do pagamento Joana sentou no banco da praça e se você não ter não tiver dinheiro eu banco nossa viagem ao exterior então a frase que todo mundo quer ouvir aí todos esses bancos aqui vão ser representados pelo mesmo vetor no ort vc no Glove E aí isso não é desejável porque eu tô eu tô eles são palavras diferentes é como se fosse assim é por acaso que eles têm essa essa representação igual Mas eles são os significados são completamente diferentes então a solução apresentada para resolver esse problema da policemi é ah eu não vou representar

- *Corpus ID:* 8282
- *Score:* 0.8314973711967468
- *URL:* oculto
- *Início:* 00:48:35
- *Fim:* 00:51:18
- *Transcrição:* tá no no artigo do do birth então ele mostra que o birth é bidirecional quer dizer tá todo mundo conectado com todo mundo o GPT é unidirecional e o Elmo é uma concatenação de um lstm direita esquerda e outro esquerda direita Então esse tá indo para lá esse tá vindo para cá E aí Aqui um comparativo de alguns modelos qual que é que é a arquitetura básica então o Elmo é lstm os outros todos aqui são Transformers aqui são as funções do do pré-treino então tarefas de modelagem de linguagem que é diferente dessa mass language modeling então aqui é predizer a próxima palavra aqui é ocultar uma palavra e dizer o que que ela tem olhando o contexto o nsp aquele da da próxima sentença aí o modelo Roberta que veio depois ele viu olha eu consigo fazer até melhor se eu não usar esse next sentence prediction aqui e aí tem outras eh outras outros objetivos de Treinamento também e aqui um dos modelos que foram disponibilizados sobre o que que corpos que eles foram treinados e os tamanhos desses modelos bom a gente tem o o Bird quando foi lançado ele disponibilizou uma versão base e uma versão grande então base e o large E aí uncased quer dizer com com Ah todos no no no mesmo eh letra minúscula para todo mundo e o case que é mantendo a a a letra original maiúscula ou minúsculo ah a junto com e aqui Quantas camadas eles têm Quantas unidades ocultas cabeças de atenção e os parâmetros Então a gente tem o o birth base com 110 milhões o large era três vezes maior o multilíngue eh era do base é maior esse multilíngua aqui tem mais de 100 idiomas ele foi treinado com mais de 100 idiomas ã aí tem os os leves o os que tentam ã ser modelos mais enxutos o Albert por exemplo ele é 10 vezes menor do que o do que o birt o gpt2 já era maior o gpt2 large era maior o T5 e assim vai indo T5 11b tem 11

- *Corpus ID:* 8022
- *Score:* 0.8279922008514404
- *URL:* oculto
- *Início:* 00:46:22
- *Fim:* 00:49:02
- *Transcrição:* implementado o ean eu não lembro o que que h não acho que não era ainda não era nada de de Deep learning a mina sim a mina não desculpa a duplex Voice essa sim já era baseada em em redes neurais recorrentes e esse próximo que a gente vai ver que é o lambda então já é uma já era uma uma rede neural mais avançada treinada prét treinada com um conjunto bem grande aqui 1,5 5 trilhões de palavras e aí ela foi ajustada para tarefas de geração de texto e ela ganhou as as notícias de jornal porque o um um funcionário da Google disse que a tecnologia tinha ganhado vida que a tecnologia tinha assim ideias próprias e bom foi eh e aí saiu só passou um teste de de touring não não passou no teste de de T eh e bem mais recentemente a gente tem então o o o chat GPT que foi lançado no no fim de 2022 o chat GPT a primeira versão era baseada no GPT 3.5 que era o gpt3 só que ajustado para não gerar conteúdo tóxico Porque então foi passado por um um processo de reinforcement learning with Human feedback quer dizer uma realimentação humana para ele eh eh ficar seguro digamos assim para ele não gerar não gerar respostas para perguntas que poderiam H teri um potencial de de gerar algum algum dano então uma a as pessoas podem perguntar como é que eu faço para para matar alguém sem ser detectado ele Ah sim Faça sim então esse esse tipo de de pergunta foi foi eh ele ele tá treinado a não responder eh com base nessa realimentação humana aqui bom então teve o o chat GPT E aí mostra ah como o chat PT falhou o teste de T na verdade não falhou Porque ele nunca nunca se nunca se propôs a se passar por um humano Então quem interage com ele deve saber que que é uma que é uma tecnologia que quem tá gerando aquelas aqueles textos são são é uma máquina Então vem a pergunta Será que alguma

- *Corpus ID:* 8537
- *Score:* 0.8261807560920715
- *URL:* oculto
- *Início:* 00:25:40
- *Fim:* 00:28:03
- *Transcrição:* são elles então nos modelos Transformer elas correspondem a um vetor de estados que é transferido de uma camada para outra tá lembra que eu falei o Bert Bas tem 12 camadas então ele vai transferindo esse vetor de uma camada para outra no Bert base esse vetor tem um tamanho de 768 Isso Aqui varia mesmo em modelos Bert tem modelos com mais com menos tá E isso aqui também é algo que acontece com modelos eh eh decoder ou Encoder decoder que o Bert é só Encoder eles também TM esse vetor só que a coisa especial do Bert é que ele é Encoder bidirecional tá e usa a linguagem mascarada então ele edens muito boas a gente chama não só as do Bert mas essas edens todas dos Transformers como edens contextuais porque elas são sensíveis ao contexto onde uma palavra se encontra então isso quer dizer que palavras polissêmicas lembra a palavra banco a palavra manga que tem mais de um sentido Elas têm diferentes representações lembra lá na worden B até uma colega perguntou tô né lá não lá é feit uma média então eu não consigo não resolve o problema das palavras policas né eu confundo eu eu eu dou faço uma média de todos aqueles sentidos e as edens elas são de palavras também mesmo nos Transformers só que a gente fala muito e o que faz mais sentido da gente falar de edens de sentenças justamente porque elas são contextuais então a gente não fala de edens de palavra do Bet isso não isso não faz sentido mais porque a gente elas são contextuais Então as edens de sentença elas consideram as edens de cada toque que compõe né a sentença E aí são usadas estratégias que eu vou falar um pouquinho delas aqui de pulling que essas estratégias não são uma simples média dessas edens elas TM algumas abordagens um pouco mais complexas pra gente chegar num para fazer um um agrupamento desses

- *Corpus ID:* 8015
- *Score:* 0.8256867527961731
- *URL:* oculto
- *Início:* 00:31:52
- *Fim:* 00:34:39
- *Transcrição:* um Escreva um soneto E aí a resposta que a máquina que tá tentando se fazer passar por humano podia diz dizer ah não conte comigo eu não consigo escrever poesia ou então se ele fazia uma pergunta que era uma uma operação ã matemática eh que uma pessoa levaria alguns segundos para responder ele não poderia responder instantaneamente então eu teria que dar uma uma pausa de alguns segundos e aí eh gerar a resposta para simular que a pessoa tá tá fazendo a conta então essa esse foi o teste original e aí então vamos falar um pouco dos dos sistemas de de eh sistemas de eh agentes conversacionais que que já existiram o primeiro deles o sistema Pioneiro é o sistema elaiza que é de 1966 E aí limitava as respostas de um psicoterapeuta Por que um psicoterapeuta ah porque muitas vezes o que o psicoterapeuta outra fase é só fazer mais perguntas ah fale mais sobre isso explore melhor então ele ele responde com com perguntas sem sem eh dizer nada assim muito específico ou apresentar uma uma solução Então esse foi o sistema Pioneiro que era baseado em casamento de padrões apenas então ele dizia Ah eu preciso de X aí dizer o que significa para você se você tivesse X então ele tinha esse padrão E aí transformava a entrada do usuário em uma em uma pergunta não tinha nenhuma inteligência ali nenhum aprendizado de máquina por trás o que ele tinha era simplesmente o regras definidas quando quando um padrão casava na regra ele ele eh dava aquela resposta então tem um exemplo de um de um diálogo que eh que tá no Capítulo dois do livro do jurafsky que depois a gente vai falar um pouco mais desse livro eh mas eh alguém implementou ou tem uma uma implementação da da elaiza que a gente consegue testar deixa eu entrar aqui nela copiar aqui Laisa Vocês conseguem enxergar deixa eu aumentar um pouco aqui a tela melhorou

- *Corpus ID:* 8464
- *Score:* 0.8254854083061218
- *URL:* oculto
- *Início:* 01:15:49
- *Fim:* 01:18:03
- *Transcrição:* usando classificação ou ou até a sumarização né uma tarefa bem específica na avaliação intrínseca ela já é mais difícil da gente compreender porque ela mede a qualidade de um modelo Independente de qualquer aplicação ou tarefa Então como é que se faria isso então tem uma métrica que é usada que é a perplexidade ou perplexity é usada nessa avaliação intrínseca e ela é uma métrica assim bem complexa ela tá relacionada à entropia cruzada que a gente usa normalmente como loss né no no treinamento e e ah ela ela a a gente pode dizer que a perplexidade do modelo de linguagem é um conjunto de testes né a desculpa a perplexidade de um modelo de linguagem em um conjunto de testes é a probabilidade inversa tá desse eh conjunto de testes normalizado pelo número de palavras Então não é uma coisa tão simples mas a gente vê que aqui tem a entropia cruzada Então ela é exponencial da entropia cruzada e o que ela significa é isso então assim de maneira geral tanto a perplexidade como a entropia cruzada elas diminuem quando o modelo se ajusta melhor aos dados e faz previsões mais precisas em cima de teste então quando ele vai Ger gerando os o texto dos Testes de da maneira que mais de forma mais precisa esses valores eles diminuem como a gente vê no treinamento da rede o loss diminui né então diminuindo o validation loss a gente que o modelo tá melhorando e da mesma forma a perplexidade essa métrica tem uma uma explicação mais detalhada com várias referências lá dentro do materiais opcionais se ainda não tá disponível a gente já libera para vocês tá que ela não é tão simples de entender nesse momento assim a principal mensagem é quanto menor melhor mas entenda que ela tem problemas tá então alguns problemas dessa métrica ela é muito sensível ao tamanho do vocabulário então modelos treinados com vocabulários diferentes

- *Corpus ID:* 1999
- *Score:* 0.8248889446258545
- *URL:* oculto
- *Início:* 00:20:45
- *Fim:* 00:23:02
- *Transcrição:* conseguia acertar porque todo mundo aqui disse que era zero né todos os modelos classificaram como zero mas ela era um então eu não acertei tá uma das coisas que eu queria perguntar para vocês olhando para esses dados né o que que vocês acham que é a principal característica que faz com que eu consiga nesse modelo un samba que é esse e de fato melhorar o meu desempenho em relação a esses individuais em relação tomadas de decisão desses modelos né O que que vocês observam como característica que faz com que ao fazer uma votação majoritária eu consiga né Por exemplo o que tá acontecendo aqui melhorar substancialmente o desempenho alguma intuição a respeito dos modelos ele gerar as mesmas saídas [Música] sim a pergunta era o seguinte o que que a gente o que que vocês conseguem observar a respeito desses modelos e das suas predições que faz com que né uma característica que faz com que ao agregar essas decisões por uma votação majoritária eu consiga acertar a maioria das instâncias e principalmente aumentar o desempenho em relação a eles né então característica em relação as predições que eles estão fazendo diminuição das variâncias composição a diminuição das variâncias de ruídos e hotlines suavização de ruídos Mas alguma coisa depois eu vou comentar quero que vocês tragam a intuição de vocês e alguém Se alguém quiser comentar mais alguma coisa deixa eu ver que tem no chat então vocês falaram coisas bem interessantes assim nesse caso de variância como eu tô falando de três modelos diferentes não dá para dizer necessariamente que eu tenho uma variância de modelo né porque a variância Seria tipo o mesmo o mesmo se eu tivesse o mesmo algoritmo com dados diferentes aí eu poderia ter

- *Corpus ID:* 8020
- *Score:* 0.8244366645812988
- *URL:* oculto
- *Início:* 00:42:08
- *Fim:* 00:45:11
- *Transcrição:* robótica não tem a voz do do waz por exemplo e e ela Inclusive tem disfluências que é o que normalmente a gente tem numa fala real então a pessoa fala ã hum faz as pausas gagueja e ela consegue compreender falas rápidas e e com sotaque eh ele é foi já é um sistema então bem mais recente baseado em em aprendizado profundo em redes neurais recorrentes reconhecimento de fala porque ela tem que entender o que o que a gente tá falando com ela e depois a geração de fala que é o processo de texto Speech então reconhecimento de fala entra a fala humana e e isso é convertido em texto por exemplo e o texto speit é o caminho inverso entra o o texto e sai o o speit não tá disponível em português tem aqui Um um pedacinho do de um vídeo pra gente assistir também e ah temi essa propagand minha meu Deus we here GO I'm calling book a women's haircut for C I'm looking for something Third sure One M What time are you looking for at 12 PM We do not have a PM available the closest we have to that is a 115 you have anything between 10. and 12m depending on What service she would serving for aut for Now okay we 10 Okay your the first is Lisa Okay Perfect so I will see Lisa at on mayd have a great day bye então aqui a gente já tem uma uma interação bem mais bem mais complexa né que que soa bem natural Ela não é uma não é uma assim ah eu quero marcar um um corte de cabelo paraas para meio-dia Ah

- *Corpus ID:* 8463
- *Score:* 0.8242398500442505
- *URL:* oculto
- *Início:* 01:13:59
- *Fim:* 01:16:21
- *Transcrição:* Teoricamente eles aprendem mais eles têm mais capacidade de identificar padrões eh complexos esse esse eh dimensionality do do modelo não é simplesmente o número a quantidade de estados de pesos Não é só isso porque ele tem cabeças de atenção então tem uma série de de coisas dentro dessa arquitetura Tá mas entendam que é um uma um vetor de 768 que vai sendo levado em cada de de uma camada para outra e que também depois nós vamos usar para isso que é AD beding que cada um desses modelos e provê tá então aqui tem mais eh tem aqui o o trabalho ó que se chama o artigo que foi publicado isso aqui foi tudo aberto então eles de fato revelaram como como treinaram explicaram então o nome era language models are un supervised multitask learners né então eles já viram a capacidade dele fazer várias tarefas aqui né E esses modelos a gente tem formas então de avaliar ele a forma mais utilizada são essas avaliações extrinsic então no momento que a gente tem um GPT treinado né como é que a gente diz que ele Ah ele para tarefa tal ele ele ele atinge um um determinado score né então o Bert por exemplo então eu vou fazer uma tarefa de classificação uma tarefa de eh geração de texto né então ele essa avaliação extrínseca ela mede o quanto a execução dessa tarefa melhora então o melhor exemplo aqui eu o do Bert nas usando classificação ou ou até a sumarização né uma tarefa bem específica na avaliação intrínseca ela já é mais difícil da gente compreender porque ela mede a qualidade de um modelo Independente de qualquer aplicação ou tarefa Então como é que se faria isso então tem uma métrica que é usada que é a perplexidade ou perplexity é usada nessa avaliação intrínseca e ela é uma métrica assim bem complexa ela tá relacionada à entropia


**Resposta gerada pelo LLM**: Elisa era baseado em regras, enquanto o Duplex Voice era baseado em redes neurais recorrentes.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to both models mentioned in the PROMPT. Supporting Evidence: The RESPONSE addresses the main difference between the two models, Elisa and Duplex Voice, by stating that Elisa was based on rules while Duplex Voice was based on recurrent neural networks. This directly answers the PROMPT and provides a clear comparison between the two models, fulfilling the requirement for relevance to the entire PROMPT.  Score: 3


---
