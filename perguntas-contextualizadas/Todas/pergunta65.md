**Pergunta 65**: Como o aprendizado por reforço com feedback humano foi usado para melhorar o desempenho do ChatGPT? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 4255
- *Score:* 0.8963887095451355
- *URL:* oculto
- *Início:* 00:55:17
- *Fim:* 00:57:34
- *Transcrição:* particular enfim daquele cenário que eu quero modelar né ã eu vou ter que definir e até cabe lembrar aqui tá aprendizado por reforço é um dos componentes do chat GPT né o o e aqui o paradigma que eles usaram o HF aqui é de Human feedback então reinforcement learning com Human feedback ou seja o pessoal da Open ai pegou um monte de gente de pessoas né E eles tinham um sistema lá gerador de texto um GPT não era o chat GPT ainda né e a partir um promp uma pergunta o GPT gerava várias respostas e aí as pessoas avaliavam ranavat resposta eh melhor né um botava no rank lá qual era melhor Qual era pior e aí eles obtiveram um modelo de recompensa para essas respostas e aí treinaram lá né qual era o número que valia cada uma dessas respostas aqui Inventaram um modelo para isso né Inventaram um mdp né um processo de decisão de marcov para isso E aí treinaram o GPT né foi refinado para maximizar aquela recompensa treinada com o feedback humano e Então esse é um componente né treinado com aprendizado por reforço E aí com isso ele foi melhorando cada vez mais as respostas dele né esse essa ideia foi repetida algumas vezes esse ciclo aqui e aí com isso né ele foi evoluindo até virar o chat GPT ou seja foi modelado um mundo no qual mundo era processamento de linguagem natural né então gerar textos né foi o mundo aqui e aí se definiu todas essas grandezas aqui pro mundo né O que que que que é cada uma delas e eu vou explicar aqui pra gente entender de uma forma geral deixa eu ver aqui a mensagem se Apareceu alguma mensagem no chat Achei que tinha aparecido apareceu notificação aqui mas ok então tá aí o que que caracteriza tá quando a gente for definir né Um Um cenário um mundo lá para nossos agentes de aprendizado por reforço a gente precisa

- *Corpus ID:* 3484
- *Score:* 0.8820343613624573
- *URL:* oculto
- *Início:* 00:25:31
- *Fim:* 00:27:35
- *Transcrição:* inclusive né o g chat GPT a questão das perguntas e respostas ele foi treinado um pouco com aprendizado por reforço reforço muito learning né Não só aprendizado supervisionado tá o chat GPT não só aprendizado supervisionado Porque como O que é no aprendizado de supervisionado você apresenta a entrada e a saída esperada então para essa pergunta Tem que ser essa resposta aqui né então ele vai começar a mapear isso mas no aprendizado por reforço no aprendizado por reforço você não apresenta a saída esperada no aprendizado por reforço é reforço muito a gente vai ver um pouco disso na aula sobre isso né vai ter um curso sobre isso curto aqui nossa mas vai ter se apresenta entrada ele gera a saída que ele acha que é E você só avalia ah essa saída foi nota 10 então joinha né joinha ou dislike aqui é o joinha para cima ou para baixo só isso né E aí ele entende Ah beleza nessa aqui eu mandei bem então valeu Aí ele anota isso lá aí quando vier a mesma pergunta a tendência é que ele gera é aquela mesma resposta que foi bem avaliada né E se ele por acaso é para uma pergunta ele gerou uma resposta que foi mal avaliada aí na próxima vez na próxima vez ele vai tentar outra coisa é porque ele sabe que aquela anterior não foi boa é isso é aprendizado um jeito né de gerar um chat GPT assim também dúvida Alexandre depois Carolina Bom dia professor Bom dia eu tava não é nenhuma dúvida é um observação assim né Essa é a peça etapa 1 do gráfico aí né que é os tokens né A gente trabalha acho que muitos colegas e devem trabalhar também com ela que né Eu acho que essa parte de token de linguística digamos assim eu acho que é uma coisa até já que já tem uma técnica né própria né acho que ela nem chega a c&a mesmo né eu tô falando essa parte de linguística né de fixo prefixo tanto que a gente consegue né ter a gente tem

- *Corpus ID:* 8022
- *Score:* 0.8795686364173889
- *URL:* oculto
- *Início:* 00:46:22
- *Fim:* 00:49:02
- *Transcrição:* implementado o ean eu não lembro o que que h não acho que não era ainda não era nada de de Deep learning a mina sim a mina não desculpa a duplex Voice essa sim já era baseada em em redes neurais recorrentes e esse próximo que a gente vai ver que é o lambda então já é uma já era uma uma rede neural mais avançada treinada prét treinada com um conjunto bem grande aqui 1,5 5 trilhões de palavras e aí ela foi ajustada para tarefas de geração de texto e ela ganhou as as notícias de jornal porque o um um funcionário da Google disse que a tecnologia tinha ganhado vida que a tecnologia tinha assim ideias próprias e bom foi eh e aí saiu só passou um teste de de touring não não passou no teste de de T eh e bem mais recentemente a gente tem então o o o chat GPT que foi lançado no no fim de 2022 o chat GPT a primeira versão era baseada no GPT 3.5 que era o gpt3 só que ajustado para não gerar conteúdo tóxico Porque então foi passado por um um processo de reinforcement learning with Human feedback quer dizer uma realimentação humana para ele eh eh ficar seguro digamos assim para ele não gerar não gerar respostas para perguntas que poderiam H teri um potencial de de gerar algum algum dano então uma a as pessoas podem perguntar como é que eu faço para para matar alguém sem ser detectado ele Ah sim Faça sim então esse esse tipo de de pergunta foi foi eh ele ele tá treinado a não responder eh com base nessa realimentação humana aqui bom então teve o o chat GPT E aí mostra ah como o chat PT falhou o teste de T na verdade não falhou Porque ele nunca nunca se nunca se propôs a se passar por um humano Então quem interage com ele deve saber que que é uma que é uma tecnologia que quem tá gerando aquelas aqueles textos são são é uma máquina Então vem a pergunta Será que alguma

- *Corpus ID:* 8494
- *Score:* 0.8724369406700134
- *URL:* oculto
- *Início:* 00:13:55
- *Fim:* 00:16:11
- *Transcrição:* ã ele foi baseado Claro no no no GPT modelo GBT 3.5 E aí esses chats às vezes eles têm um treinamento adicional às vezes eles só usam um modelo lembram lá daquela figura que a professora mostrou no início do do coiso então às vezes eles envolvem várias coisas né a consulta pode uma ferramenta de recuperação para melhorar a resposta então tem várias coisas aqui que não é aberto a gente não sabe Exatamente tudo que tem ali mas tem muitas habilidades a gente já viu né isso isso aqui era esse slide é do ano passado então cada vez mais a gente até não se surpreende mais com a capacidade dele né aqui é uma perguntinha que foi feito lá atrás ó da usuário da professora Viviane O que é o chat GPT aí ele respondeu tudo bonitinho ó eu fui treinado e uma grande quantidade de dados aí aqui agora o que que ele responde agora ó se eu fiz ontem então assim ele já dá um monte de coisa já começa a falar de segurança e ética atualiza já dá uma explicação mais estruturada aqui né e os dados estão atualizados até 2023 então aqui era ó aqui é a primeira fase era 2021 E ontem ele me respondeu que é Até abril de 2023 algumas coisas ele não conhece né tu vai perguntar de coisas depois disso ele não não conhece Claro tem aquelas opções que tu pode pesquisar na internet né não vou entrar nisso aqui tô falando só do modelo em si e aqui a gente perguntou na época assim como é que tu tu tu lida com com que as pessoas te respondem e tu usa isso tu aprende aí olha só ele respondi aqui ó quando as pessoas falam comigo eu analiso as entradas uso modelos estatísticos para gerar resposta e se eu eu eu uso o feedback tá na minha resposta então ó adicionalmente a o meu meus dados de treino são atualizados Aí depois ele mudou dizia não eu não uso sabe então hoje ele diz o seguinte ele diz que sim ele usa né mas isso é de uma

- *Corpus ID:* 8487
- *Score:* 0.8703314065933228
- *URL:* oculto
- *Início:* 00:02:00
- *Fim:* 00:04:25
- *Transcrição:* gerando saídas falsas né Essa a Alucinação que a gente vai falar um pouco mais depois ou então saídas tóxicas né ofensivas ou simplesmente saídas que não eram úteis pro usuário então mesmo eles sendo grandes modelos e tendo ótimos desempenh eles tinham esse comportamento Então nesse paper eh nesse modelo 3.5 eles buscaram alinhar Então os modelos com a intenção do usuário e aí eles fizeram uma um ajuste para uma gama de tarefas usando o reinforcement learning eh com a feedback do ser humano né E E com isso eles chegaram a modelos que eles cham de instru GPT porque vocês vão ver aqui que no na estratégia de reinforcement de aprendizado por reforço deles eles usaram instruções de humanos então na primeira fase aqui ele tem uma supervisionado então aqui por exemplo a o promt era eh Explique a a lua h o pouso na lua para uma criança de 6 anos aí elas escreviam Então tinha um grupo de pessoas que escreveram resposta para essa pergunta como o modelo deveria se comportar e esse dado foi usado para fazer o fine tuning então depois disso eles compararam fizeram uma segunda rodada aonde usaram Então esse modelo fado aqui né para para para responder algumas perguntas e aí de novo um grupo de pessoas classificava essas perguntas em ordem de qualidade né De acordo com o julgamento humano então com isso ele aprendeu Como que o ser humano né como que um um um uma pessoa que atribui Rot para aquilo considera o que que é melhor o que que é pior dentro aquelas respostas esse D esses dados todos foram usados então para calcular esse modelo de recompensa E aí entra mais na questão do do reinforcement learning mesmo né de aí ele otimiza isso de forma a ter uma política né que vai sendo atualizada então com isso ele consegue modelos altamente capazes de segir a instrução do usuário né então a pessoa fala a

- *Corpus ID:* 8429
- *Score:* 0.8697179555892944
- *URL:* oculto
- *Início:* 00:10:02
- *Fim:* 00:12:50
- *Transcrição:* a a resposta do do chatbot ela é convertida para paraa fala só para ter uma ideia de um monte e tem banco de dados aqui ó tem um banco de dados relacional que tá aqui nessa nesse Rosinha que tem opiniões então tem tem muitos muitos muitos componentes e outra questão importante é que h o chatbot ele pode ir aprendendo Ou seja a ideia é que ele consiga aprender a partir das respostas e que ele possa de alguma forma ter um um uma ideia de como é que se ele tá indo bem ou tá indo mal eh pode por exemplo ã ele usa as respostas para melhorar usa as interações com os humanos para melhorar o resultado que ele tá gerando e pode usar evidências por exemplo ah a pessoa tá demonstrando irritação então a pode usar análise de sentimento para ver se o usuário tá ficando irritado ele tá respondendo de um jeito assim não tão cordial então Eh essa conversa não tá indo bem então pode usar as evidências tanto o texto mesmo que a pessoa fornece como ã características que são extraídas do texto como o análise de sentimento então tem como me di se a conversa tá indo tá indo bem ou não aqui tá um componente de de satisfação aqui é importante também e agora sobre a geração agora é contigo né Luciana isso tu tu me permite compartilhar que não tá aparecendo aqui s tá eu fui a a primeira a entrar aqui no no chat deixa eu recuperar aqui então paro e já tá aparecendo para ti não pera aí que eu vou vou ver como é que nas opções da reunião opções da reunião vou botar que todos podem apresentar mais fácil agora sim foi estão vendo apresentação no modo correto sim sim sim então então tá pessoal vamos falar então de geração de texto Então a a geração de texto como a

- *Corpus ID:* 1287
- *Score:* 0.8691956996917725
- *URL:* oculto
- *Início:* 00:56:03
- *Fim:* 00:57:56
- *Transcrição:* de máquina que não tem nenhuma interferência de humano então a pessoa basicamente ela programa lá para o modelo conseguir aprendido né mas essa questão do humano no loop que aí uma nindelupe justamente está se discutindo bastante por isso que as pessoas como é que eu coloco o feedback né do meu humano se eu tenho uma pessoa com conhecimento muito rico a respeito daquele domínio como é que eu posso usar isso para aprimorar o meu modelo então existem formas de fazer isso por exemplo a pessoa enfim vamos supor que é uma tarefa que você classificar pontos entre a e b e aí você olha o modelo com as coisas que como a E aí alguém né humano vai lá e disse não isso aqui não é isso é b e é isso retorna para o modelo para que ele aprenda a partir disso então a parte também de por exemplo bom eu tenho uma série de características que eu utilizo para tomar minha decisão né E aí meu modelo diz que tais características são mais relevantes mas vem um humano e disso tudo normalmente de uma forma programada digamos assim para ter essa interação e salvar esses resultados humanos diz olha não mas essa característica ela é enfezada e essa daqui ela é relevante não foi incluída então existe sim toda uma linha de eu diria de pesquisa e de pensamento dentro de aprendizado de máquina que é colocar o humano de volta Nesse loop dando feedbacks que possa melhorar né esse processo de treinamento dos modelos tá outra forma por exemplo se a gente sabe que quando a gente trabalha com probabilidade a gente sabe né alguns Eventos São mais raros então a gente ajusta as probabilidades Com base no conhecimento prévio não Com base no conhecimento dos dados porque às vezes a coleta de dados pode ter alguns artefatos amostrais Enfim então existem sim formas de fazer isso aí e são diversas assim tá mas acho que isso é um

- *Corpus ID:* 1277
- *Score:* 0.8667476773262024
- *URL:* oculto
- *Início:* 00:40:14
- *Fim:* 00:42:23
- *Transcrição:* aprendizado por reforço né que basicamente vai avaliar decisões sucessivas e ajudar criar uma política que nos leve a um a digamos assim o desfecho que a gente quer né Por Exemplo né um agente se mover pelo pelo ambiente conseguir chegar sei lá por exemplo Aquele caso do robô né onde tá mais sujo Enfim então é uma decisões excessivas que ele vai tendo feedback né se aquela Decisão foi boa se aquela ação foi boa ou não então tem várias formas a gente fazer né um computador Aprender Pensando no computador de forma geral então a área de aprendizado de máquina são esses sistemas que são treinados para realizar uma tarefa né ao invés de terem explicitamente programados para tal então assim eu não programo passo a passo daquela tarefa eu treino ele para que ele aprenda a realizar uma tarefa específica para que ele aprenda a detectar o diagnóstico para que ele aprenda a prever o valor das ações para que ele aprenda determinasse se um cliente vai parar de comprar na minha loja para eu poder enviar algumas promoções para esse cliente então ele consegue aprender a realizar aquela tarefa né eu não quando a gente fala explicitamente programados eu não estou programando passo a passo as regras eu estou treido ele para aprender essas regras ou esses padrões certo E aí aqui vem esse treinados é justamente isso ele tem a habilidade de aprender a partir de exemplos passados por isso que quando a gente fala de aprendizado de máquina e é especificamente falando de aprendizados nossos pressionado aprendizado profundo a gente tá falando de ter dados porque a gente precisa ter dados para permitir que aquele algoritmo aprenda os padrões que estão subjacentes aqueles dados certo e para a gente dar um exemplo né sobre isso essa habilidade de aprender a

- *Corpus ID:* 8504
- *Score:* 0.862455427646637
- *URL:* oculto
- *Início:* 00:31:56
- *Fim:* 00:34:22
- *Transcrição:* porque eu dei o o o enter aqui tá Ah para não fazer assim a gente bota pode colocar aqui barra n barra desculpa barra n eh às vezes duas vezes separando porque o modelo ele identifica essas coisas principalmente o o GPT tá então cada modelo Às vezes tem uma forma da gente reforçar o que que é a a instrução que que é o contexto que que é a pergunta e o que que a gente quer dele né então aqui é sim ah a acho que a comida estava boa e o sentimento ele classificou como positivo aqui outra brincadeira né desempenhando um papel então aqui a gente diz para ele Ó o que se segue isso aqui tudo foi passado no prompt para ele é uma conversa com o assistente de pesquisa de a o tom do assistente deixa eu puxar isso aqui é técnico e científico humo olá quem é você aí o a gente já dá o exemplo aqui para ele dentro do Pr a gente tá dizendo Olha só como tu tem que te comportar saudações eu sou um assistente de pesquisa de a como posso te ajudar aí oo Você pode me falar sobre a criação de buracos negros e olha a resposta dele Claro os buracos negros são formados a partir de colapso gravitacional então bem técnica a gente poderia mudar aqui dizer Te comporta de uma maneira bem formal como se tu tivesse explicando para uma criança e ele a princípio tende a seguir a nossa instrução lembra lá do instruct dos instruct models do do primeiro gpt3 lá que já começou a se perceber que a gente eles fazendo reinforcement learning ensido ele a a seguir ele segue a instrução então ele é bom em seguir a instrução que a gente dá E aí aqui mais abaixo coisas bem simples gerando um código aí bota a tabela as colunas né outra tabela as colunas e diz me cria um uma co SQL que pegue todos os estudantes E aí eu poderia fazer mais complexo aqui né E aí ele faz mas isso aqui é uma coisinha fácil né pessoal se a gente fazia se a gente faz lá com o próprio

- *Corpus ID:* 1285
- *Score:* 0.8604303598403931
- *URL:* oculto
- *Início:* 00:53:05
- *Fim:* 00:55:11
- *Transcrição:* supervisionado e profundo a gente precisa de dados o aprendizado por reforço ele vai ter uma forma diferente de modelar esse processo de aprendizado tá que não é que justamente é usado quando eu não tenho dados assim históricos né ou uma base prévia Tá mas tem um entendimento tá correto só vou fazer essa essa observação em relação ao aprendizado por reforço por reforço seria então tipo tentativa e erro é exatamente é por tentativa e eu e a gente tem formas de penalizar os erros ou da Recompensas pelos acertos para que o próprio Agente né às vezes um robô consiga ajustar sua política de ação olha não posso fazer isso porque eu não posso botar minha mão no fogo porque queima né Então aí entende assim essa de ele ir aprendendo a partir desses feedbacks que a gente dá mas a gente não diz explicitamente o que ele vai fazer Beleza obrigada nada temos mais uma pergunta Deixa eu só ver aqui a mão levantada o salvador alguns slides atrás me mostrou uma distinção entre dois paradigmas o mais antigo onde tinha as regras eram aplicadas em cima de um conjunto de dados né para obter respostas e esse o do aprendizado de máquina onde a gente tem os dados e respostas e para aí a gente conseguir extrair disso daí as regras na prática você pode existir situações onde você tem uma tipo uma mistura por exemplo você já tem algumas regras óbvia já conhecidas pelo especialista e submeter aplicar em cima desses dados e a partir do aprendizado de máquina tentar descobrir novas regras que eram desconhe cidas ou as coisas são distintas na realidade Existem várias formas a gente embutir conhecimento prévio tá então por exemplo quando a gente fala de regras a gente vai ver modelos de aprendizado de máquina que de fato são baseado em


**Resposta gerada pelo LLM**: O ChatGPT foi treinado com aprendizado por reforço com feedback humano (RLHF).  Um sistema gerador de texto (ainda não o ChatGPT) gerava várias respostas a partir de prompts. Pessoas avaliavam essas respostas, criando um modelo de recompensa.  O modelo foi então refinado para maximizar essa recompensa, melhorando as respostas gradualmente até se tornar o ChatGPT.  Esse ciclo foi repetido várias vezes.  No caso do GPT-3.5, base do primeiro ChatGPT, o RLHF foi usado para alinhar o modelo com a intenção do usuário e evitar saídas falsas, tóxicas ou inúteis, usando instruções humanas e classificação de respostas por qualidade.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining how reinforcement learning with human feedback (RLHF) was utilized to enhance the performance of ChatGPT. It details the process of generating responses, human evaluation, and the refinement of the model, which aligns with the inquiry about the use of RLHF in improving ChatGPT. The explanation is comprehensive and covers the key aspects of the PROMPT, including the alignment of the model with user intent and the avoidance of undesirable outputs.  Score: 3


---
