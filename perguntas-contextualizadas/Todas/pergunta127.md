**Pergunta 127**: Quais são os tipos de dados anotados necessários para o desenvolvimento de sistemas de classificação de notícias e reconhecimento de entidades nomeadas? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8681
- *Score:* 0.8527938723564148
- *URL:* oculto
- *Início:* 01:48:24
- *Fim:* 01:50:26
- *Transcrição:* então eh qual que é o nosso problema né de negócio né O que que a gente tá buscando atender né com essa crescente crescente aumento de do volume de dados digitais né Na maioria deles e são Dados não estruturados como nós viemos aí algumas aulas eh Nossa tarefa seria eh criar mecanismos que facilitem né a busca e análise desses dados né Eh e abordade seria em cima de de notícias tá eh para ver se auxilia um pouco mais nessa tarefa né eh o nosso dat set foi a folha de São Paulo eh extraída da do portal do kegle né e ele tem notícias publicadas aí de janeiro de 2015 até setembro de 2017 eh fazar de dois anos né Mais ou menos de de Notícias tem coisa para caramba o dataset tem aí um pouco mais de 167.000 eh notícias publicadas eh distribuída de forma bem mal como vocês podem ver na imagem aí né em 44 categorias e tem seis colunas né dentre elas aí o título O texto a data da da publicação a URL a categoria E a subcategoria da notícia e a gente já vai ver um pouco mais detalhadamente aí no no na etapa de pré-processamento eh nós acabamos passando aí pela pela etapa de exploração dos dados né Inicial E acabamos tendo que remover eh quase todas essas colunas e mantivemos só o o texto da Notícia o texto bruto da notícia e a categoria né que seri nossos rótulos né Eh e o motivo disso eh é que a gente vai ver aqui já na etapa de pré-processamento eh com a análise da eh exploratória dos dados que a gente fez no início a gente percebeu aí que tinham muitas notícias com uma frequência muito baixa dentro da da do corpo do do texto da notícia né então a gente acabou tendo que limitar aí né vamos vamos pegar só o que tem acima de 500 registros tá eh quanto as colunas que nós precisamos remover o RL data da publicação a gente Inicialmente nós consideramos elas eh sem uma desutilidade muito assim importante para paraa nossa análise né pra nossa tarefa aí de classificação tá Ah uma mais mais interessante Zinha que

- *Corpus ID:* 8040
- *Score:* 0.8487967252731323
- *URL:* oculto
- *Início:* 01:20:20
- *Fim:* 01:22:35
- *Transcrição:* disponibilizados e seja mais fácil paraas para outras pessoas usarem tem muitos milhares de modelos Transformers pré-treinos e A gente vai usar também esses esses modelos aqui da da huging face ã em Maio de 2021 as bibliotecas de Transformers deles já faz bastante tempo né já tinam mais de 1 milhão de downloads é bastante coisa então aqui a gente tá vendo o que Hã o que [Música] eh ajuda o desenvolvimento são esses sete fatores se alguém lembrar de mais um podem falar por favor que a gente pode enriquecer aqui eh por Ops por outro lado a gente tem algumas coisas a que que ainda limitam o desenvolvimento e o que que são esses esses limitadores do do desenvolvimento ã a gente precisa de dados anotados mas para que não é não é só o texto não tem muito as coisas que a gente precisa de dados anotados por exemplo se eu quero fazer um sistema de classificação para ã esse de Notícias que eu que eu tenho falado então eu vou precisar Que alguém tenha anotado um conjunto de notícias para dizer qual que é a classe delas ou então ah eu preciso fazer um reconhecedor de entidades nomeadas eu preciso que alguém olhe para um para um conjunto de sentenças um conjunto de frases tá pessoal eu falo muito Muitas vezes sentença sentença sentença não é a lá que o juiz escreve é uma frase é o que uma tem que termina por um por um ponto final um ponto são são as frases do texto bom então eu preciso de um conjunto de sentenças onde tá onde tá cada elemento anotado para para essas tarefas de rotulação de sentenças essas esses rótulos que a gente precisa essas anotações é o que a gente chama de Ground truth eu preciso para treinar algoritmos de aprendizado supervisionado né os os algoritmos de aprendizado

- *Corpus ID:* 8310
- *Score:* 0.8483485579490662
- *URL:* oculto
- *Início:* 01:41:47
- *Fim:* 01:44:05
- *Transcrição:* porém para determinadas tecnologias não mas por exemplo citação quântica eu fiz no levantamento aí tem quase 2.000 textos né então assim se eu pudesse né Eh eh preparar alguma coisa que me ajudasse a selecionar né dentro do assunto que eu tô pesquisando seria interessante Então a gente tava imagido em usar né um modelo pré-treino né alguma coisa assim a gente começou eh eh Na verdade queria né conversar com com as professoras né isso daí se se teria tempo hábil se Seria interessante se talvez a gente eh eh conseguisse fazer um recorte disso né para ajudar né E que eh eh eh fosse apresentável né fosse eh digerível n digestivo dentro da da disciplina tá é a dificuldade aqui Luiz Cláudio é como é que tu vai avaliar então tu fez lá tu encontrou 10.000 artigos fez lá encontrou 10.000 artigos como é que tu vai saber se os artigos pertencem a ao Dom que tu quer se se os artigos TM informação ou vão ser relevantes depois como é que tu vai fazer isso sem avaliar manualmente esses então como é que tu vai calcular uma métrica de sucesso se não tem essa anotação por isso que a gente tá recomendando usar um Data Set anotado porque aí esse trabalho manual Já tá feito já que aqui a gente não vai ter tempo de fazer essa anotação Então se tu quer trabalhar por exemplo com uma tarefa de recuperação de informação Então pega um de recuperação de informação que esteja pronto e aí o que tu aprende usando aquele dataset pronto o que que é um dataset de recuperação de informação é um dataset que tem consultas tem documentos e tem os julgamentos de relevância vai dizer ah para essa consulta aqui esse é relevante esse não é esse outro é e isso aqui é que que se não for um dataset pronto não não vai ter e leva assim milhares de horas para para anotar um dataset desse desse esse tipo então eu acharia melhor usar um dataset de recuperação de informação ou até mesmo de de extração

- *Corpus ID:* 8682
- *Score:* 0.8476492166519165
- *URL:* oculto
- *Início:* 01:49:56
- *Fim:* 01:51:50
- *Transcrição:* muitas notícias com uma frequência muito baixa dentro da da do corpo do do texto da notícia né então a gente acabou tendo que limitar aí né vamos vamos pegar só o que tem acima de 500 registros tá eh quanto as colunas que nós precisamos remover o RL data da publicação a gente Inicialmente nós consideramos elas eh sem uma desutilidade muito assim importante para paraa nossa análise né pra nossa tarefa aí de classificação tá Ah uma mais mais interessante Zinha que a gente entendeu seria de subcategoria me pareceu interessante mas ela tava com muitos valores nulos né então Eh acabar atrapalhando mais do que ajudando tá e o título também parece me pareceu interessante até a gente começar a ver os dados na exploração Inicial e descobrimos que a grande maioria dos títulos das notícias ele já tava eh embutido no no corpo do do texto da notícia então a gente acabou removendo também tá eh e por fim a gente aplicou aí um balanceamento de classes aí usando o and sample né para eh nivelar um pouquinho mais aí é e a distribuição e das classes da dos nossos x aí pelas pelos nossos rótulos né Eh e continuando com o pré-processamento a gente aplicou aí o reex para remover aqueles caracteres indesejados lá n que a gente já conhecidos aí URL tags etc eh e a gente nessa nesse pré-processamento é importante a gente eh salientar que Nós criamos quatro colunas novas no nosso dataset que vai ser modelado lá na frente né utilizado na modelagem lá na frente eh por quê porque cada modelo ele a gente entendeu também pelas aulas aí alguma pesquisa de que eles eles os modelos eles têm comportamentos diferentes de acordo com o tipo de texto que a gente tá usando Eh sei lá o o modelo x vai se comportar melhor com o texto com stopwords ou o outro vai se comportar pior então a gente acabou criando as quatro colunas de texto do pré-processamento para vamos dizer assim iterar o comportamento

- *Corpus ID:* 8421
- *Score:* 0.8475008010864258
- *URL:* oculto
- *Início:* 01:23:27
- *Fim:* 01:25:58
- *Transcrição:* fica bem livre né não precisa ser um um dataset anotado normalmente qualquer texto que você tiver né um conjunto de e-mails um conjunto de reclamações Você já consegue trabalhar né É mas para pro trabalho a gente precisa avaliar Então como é que como é que tu faria a avaliação por isso que aí precisa do dataset anotado da coleção de teste pod cal da modelagem de tópicos entendeu A modelagem de tópicos é um caso a parte cling é um caso a parte mas as as outras as as tarefas em que que a gente precisa calcular uma métrica que tem que ser baseada num num a gente chama de Ground truth que precisa ser precisa do resultado esperado tipo um gabarito preciso do gabarito para poder avaliar análise sentimento classificação e Recuperação de informação também mas existem coleções de teste que que podem ser usadas então tem a do Glasgow herold do La times em português tem essa da Folha de São Paulo é porque no caso o que vocês querem é que no final a gente faça um tipo de métrica mostrando o que isso o a técnica que a gente aplicou teve tanto por cento de acerto isso isso o teu trabalho é análise de sentimento né Carlos é como eu tá dizendo agora eu acho que talvez eu mude né Eu gostei dear esse tópico de hoje tá se tu quiser uma coleção em português tu me avisa ali me manda uma mensagem no no teams que eu que eu te encaminho a coleção do da Folha de São Paulo pronto tá ótimo obrigado que pode ser usada é que assim a a não ter um Data Set anotado uma coleção de teste daí implica muito trabalho braçal e que a gente não não precisa ter nesse nesse momento Aí sim o trabalho Fica dá muito trabalho bom dia professora aqui Bom dia Antônio alísio falando eh na nossa proposta né Nós eh é aquela proposta de fazer tradução usando um um modelo generativo né llm

- *Corpus ID:* 1243
- *Score:* 0.8462029695510864
- *URL:* oculto
- *Início:* 02:00:07
- *Fim:* 02:02:16
- *Transcrição:* a coisa toda começa de uma forma absolutamente geral que é uma lei é de fibo está aqui colocado né a circular 3461 do Banco Central é difícil entender o que está aqui colocado a partir disso aqui você precisa transformar em uma estrutura de dados é claro isso que tá fora do escopo do nosso trabalho que a gente só coloca a como item de atenção né nem sempre é fácil se ter essa compreensão dos dados é quem derivou essa estrutura for até um trabalho aqui de Mestrado do que do colega Ricardo lá de Curitiba onde ele fez uma proposta desse modelo preditivo o que tá em operação não é a proposta dele mas enfim já foi um trabalho nesse sentido pois bem avançando então na nossa proposta como está hoje nós temos Então essa preparação dos dados que é o seguinte os Coffee que recebe as comunicações naturalmente né ele tem uma estrutura transacional os dados todos normalizados e as diversas bases complementares existentes Então a primeira fase da preparação de dados é o que fazer a extração de comunicação enzima determinado período de fazer a desnorização de transformar aqui no tabelão Como existe texto também né começar a aplicar algumas técnicas aqui né de pré-proceamento de texto beleza e eu tiro também informações né que são de bases complementares como por exemplo CPF CNPJ dívida ativa da União registros de óbito registro de sistema de segurança Serasa e assim por diante isso tudo né vai compor aqui o tabelão faça um conjunto de tratamentos desse tabelão e aqui eu tenho o modelo analítico certinho né para o período isso aí eles rodam isso aqui Salvo engano de semana em semana obtive que essa basezona analítica o que eu faço passa pelo modelo preditivo e ele vai gerar o que o conjunto de comunicações é classificadas que classificação é essa tem fraude não tem fraude perfeito então isso aqui é o exista legal qual a

- *Corpus ID:* 8317
- *Score:* 0.8459610939025879
- *URL:* oculto
- *Início:* 01:54:01
- *Fim:* 01:56:10
- *Transcrição:* isso é modelagem de tópicos assim que aí tu tu tenta ter um Label tu vai fazer a o cluster dessas embeddings né claro tem outras formas o que nós fizemos foi usar o Grid Search para avaliar o svn e o Nav Bales tá Tu já fez uma comparação ali então tu vê o resultado com uns três ou quatro tipos de hiperparâmetros uhum para achar Qual o melhor modelo para comparar os modelos saber qual era o melhor tem que analisar os teus erros porque às vezes tu pode chegar à conclusão que o naive base erra num num determinada aquilo que a gente falou ali Ah tá errando só nos negativos e dentro dos negativos ele erra um assunto tal por exemplo sabe tu tu tem que analisar os erros ok e Ah então agora eu eu pulei alguém não então o grupo do Alexandre Anderson e Bruno categorização de notícias tá é uma uma tarefa bem definida então uma classificação multiclasse e tem o e tem o dataset e vocês já sabem que Que métodos que vão usar Oi professora tudo bem tá tá me ouvindo bem S Opa beleza Eh então professora nós pegamos esse dataset e ele é bem grandinho né por sinal tem tem 400 e 500 m e a gente passou por uma fase zinha de pré-processamento nele aí e e e precisamos fazer uma um balanceamento de classes né tinha tinha bastante notícia com categoria eh com uma frequência muito baixa né em categoria sim então a gente precisou também utilizar um pouco daquele método lá do stratify né e ele não funciona com com uma frequência de uma uma ocorrência só né eh ah sim não então a gente aproveitou e fez um um balanceamento de classe bem pesado para melhorar um pouquinho aí a a utilidade né do dos dados do dataset eh surgiu a primeira dúvida no início que era a questão de eh nosso tema não faz parte daquela lista de tópicos mas no próprio

- *Corpus ID:* 8041
- *Score:* 0.845857560634613
- *URL:* oculto
- *Início:* 01:22:00
- *Fim:* 01:24:21
- *Transcrição:* lá que o juiz escreve é uma frase é o que uma tem que termina por um por um ponto final um ponto são são as frases do texto bom então eu preciso de um conjunto de sentenças onde tá onde tá cada elemento anotado para para essas tarefas de rotulação de sentenças essas esses rótulos que a gente precisa essas anotações é o que a gente chama de Ground truth eu preciso para treinar algoritmos de aprendizado supervisionado né os os algoritmos de aprendizado supervisionado precisam aprender com exemplos e esses exemplos são os datasets anotados e também eu preciso eu tô para qualquer coisa que a gente vá fazer a gente tem que ter uma noção se Tá acertando ou se tá errando então a gente precisa ter essa essa avaliação ã essa rotulação pra gente poder calcular alguma métrica de sucesso tô acertando ou tô errando ah troquei isso aqui melhorou piorou então a gente precisa dos datasets anotados Por que que eles são caros porque precisa de gente para para anotar na maioria das vezes a gente precisa de gente a gente precisa de um volume grande de de anotações Então isso é o que fazem com que eles sejam eh Assim menos abundantes do que do que texto se a gente for pensar então idiomas menos h com menos recursos Como o português isso começa a ficar um problema mais sério e olha que português é um dos top seis idiomas mais falados do mundo seis ou sete se a gente for pensar num idioma pouco falado pouco representado algum idioma sei lá da falado em alguma tribo hã em alguma comunidade Menor da da África por exemplo então não não tem nada muito pouca coisa Ah então então data sets anotados são um dos principais limitadores a gente tem algumas algumas Fontes né que são por exemplo em português existe um aqui tem alguém reuniu alguns datasets eh sites de campanha de avaliação são também chamado

- *Corpus ID:* 8659
- *Score:* 0.8445346355438232
- *URL:* oculto
- *Início:* 01:05:58
- *Fim:* 01:08:30
- *Transcrição:* Eh eu acho que é é envolvido aí né um acrônimo que eu gosto muito é essa história do vuca né uma informação volátil inserta complexa e ambiguo né Tá além da nossa capacidade processar essa essa informação então aí o nosso trabalho né especificamente eu Guilherme né dentro do certo tá envolvido com projetos de Observatório de tecnologia incubadora né e prospecção de software e nesse momento varrer uma quantidade muito grande de informações resumir para saber aquilo que vai se investir né é muito importante e aí a gente escolheu esse dataset da BBC News né que ele tem eh notícias e resumos né e é é um um um dos processos lá na frente que atende né ao nosso trabalho seria Realmente você conseguir fazer uma catalogação né você avaliar a qualidade e fazer uma catalogação dessas informações aí no próximo slide a gente mostra as referências né foi a a o levantamento né que a gente fez essas que estão em destaque as principais E aí eh seguindo né o dataset pré-processamento né Eh seguinte né a gente vai dar uma ideia do do do dataset né Eh é um conjunto de 2.225 artigos né tirados da BBC News ele foi usado foi utilizado num trabalho eh eh publicado né pelo Dr de Green e pa né e 860.000 palavras né em torno disso e o mais interessante esse texto é bem comportado né porque a gente chegou a avaliar os caracteres né O que são letras O que são dígitos O que são espaços em branco então assim ele não tem um dos problemas que foi até abordado antes né de eh eh eh imagem né de equações tá que talvez seja uma coisa que eh eh no nosso mundo vai impactar seguindo né a gente fez uma análise né esse pré-processamento E aí você vê que a média de palavras nesses textos né Por sentença ela tá bem dentro do do nível né da quantidade de tokens né que a gente vai trabalhar e ele segue a lei de zic né Eh aí no slide seguin a gente dá uma olhada quer dizer a

- *Corpus ID:* 8003
- *Score:* 0.8432754278182983
- *URL:* oculto
- *Início:* 00:08:53
- *Fim:* 00:11:10
- *Transcrição:* tag que é rotulação morfossintática e reconhecimento de entidades nomeadas depois a gente tem uma aula sobre recuperação de informação a gente vai falar inevitavelmente né do dos modelos generativos dos gpts vamos falar de modelagem de tópicos E aí na nossa última aula que é no dia 12 de Abril a gente tem apresentação de trabalhos e e a prova final então uma introdução professora diga então a nota vai ser eh trabalho e não teremos eh só será somente esse trabalho final a gente vai ter trabalhos intermediários ao longo das aulas Sim nós temos trabalhos intermediários nós temos o trabalho final e nós temos prova então são são todos esses esses componentes tá jo eh então sobre sobre os tipos de dados que que a gente pode trabalhar então eles podem ser divididos em três categorias os estruturados os não estruturados e os semiestruturados e aqui do lado tem mais ou menos uma Estimativa de quanto por cento das informações de interesse para as organizações que eles representam o primeiro grupo os dados estruturados são aqueles que a gente consegue armazenar em bancos de dados relacionais em formato de tabelas sqr Então são informações que estão organizadas e que podem ser facilmente carregadas em tabelas E consultadas então existe uma linguagem de consulta que permite que a gente consiga selecionar esses dados de acordo com com critérios e essa linguagem é bem específica ela não é ambígua e hã e e e esses dados precisam de um esquema bem definido para que a gente consiga consultar Então aqui tem um exemplo tradicional então tenho uma tabela de alunos uma tabela de atividades e aí eu tenho uma tabela que relaciona não os alunos com as atividades e aí a gente tem os dados não estruturados que até podem ter alguma


**Resposta gerada pelo LLM**: Para sistemas de classificação de notícias, são necessários dados anotados com um conjunto de notícias e a classe de cada uma. Para o reconhecimento de entidades nomeadas, é preciso um conjunto de sentenças com cada elemento anotado para as tarefas de rotulação de sentenças.  Essas anotações são chamadas de Ground truth e são necessárias para treinar algoritmos de aprendizado supervisionado.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the types of annotated data needed for both news classification systems and named entity recognition, which are the two main components of the PROMPT. It specifies the requirement for annotated news data with classes for classification and annotated sentences for named entity recognition, providing a clear and relevant explanation. Additionally, it mentions the importance of these annotations for training supervised learning algorithms, which adds further context to the relevance of the response.  Score: 3


---
