**Pergunta 43**: O TensorFlow prioriza a execução de operações em qual dispositivo?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 7024
- *Score:* 0.8707904815673828
- *URL:* oculto
- *Início:* 00:18:08
- *Fim:* 00:20:12
- *Transcrição:* basicamente um arquivo csv que descreve cada modelo de alguma forma né E aí vai ter um mecanismo que vai pegar uma dessas dessas configurações e vai pegar Ah tá qual GPU tá livre vou treinar nessa GPU depois ele vai pegar a próxima configuração vai botar na outra GPU e vai fazendo isso enquanto tem GPU disponível e como é que tu força para que eh eh cada script veja só uma GPU daí tem uma variável de ambiente chamada kuda visible devices que ela é respeitada pelo tensor Flow E aí a gente diz no cuda visible devices Qual que é o device ID eh da GPU que a gente quer usar naquele naquela execução de tensor Flow né então com isso daí eles conseguem automatizar esse mecanismo de procura bem manual né a gente vê isso bem manual embora existam mecanismos mas eh eficientes para fazer isso talvez já mais prontos assim né mas essa seria a forma manual de fazer Então essa é uma alternativa né tu tem várias eh gpus tu testa um modelo GPU Ok ah uma outra uma outra possibilidade de paralelização né que essa Inclusive a gente já viu é usar o prefet né então a gente vai ao mesmo tempo que tá treido com uma GPU né a gente pode usar CPU pra gente fazer pré-processamento né então por exemplo aqui nessa nossa nesse nosso grafo de tarefas essas tarefas que aparecem aqui em CPU podem ser de pré-processamento né para isso a gente vai usar o dataset prefet que a gente viu na aula passada e aí a gente põe o paralelismo que a gente precisa ter nesse dataset esse paralelismo vai ser um paralelismo de CPU e a gente usa todas as cpus para preparar os dados para que a GPU possa fazer o treinamento de maneira eficiente então com isso a gente vai ter um paral ismo em CPU e um paralelismo em GPU Ok tem uma outra alternativa também que é a gente usar para aqueles modelos que são de

- *Corpus ID:* 6844
- *Score:* 0.8682695031166077
- *URL:* oculto
- *Início:* 00:01:50
- *Fim:* 00:03:59
- *Transcrição:* Quais são os devices os Physical devices que estão disponíveis Então nesse caso aqui a gente consegue observar que tem dois é uma lista né onde o primeiro é a CPU e o segundo é a GPU Então se no caso aqui a gente tivesse mais de uma GPU ou mais de uma CPU né a gente teria listado então várias vezes esses eh com com um identificador diferente aqui provavelmente esse número aqui seria diferente se tivesse mais de uma GPU por exemplo Então como que eh ele gerencia isso depois iso é bastante transparente né a gente vai realmente só instanciar os tensores dizer onde é que eles têm que ficar tá depois as operações acontecem de maneira transparente tá a gente tem como também aqui no caso get visible devices pega todos que estão disponíveis ã no caso que a gente tá no ambiente do collab então tem uma CPU e uma GPU mas a gente poderia por exemplo também listar os devices baseado numa string certo Seria tipo uma string de consulta como se ele estivesse fazendo uma filtragem com essa string Então se a gente quiser por exemplo somente os devices que são eh gpus a gente pode passar GPU tudo maiúsculo daí ele procura por GPU tudo maiúsculo no nome e lista somente aquelas que que estão nesse que onde essa essa verificação é é é é verdadeira então Eh nas situações onde a gente quer fazer uma operação onde tem uma implementação tanto em CPU e uma implementação em GPU se ambos os recursos estiverem disponíveis o tensor Flow sempre vai priorizar GPU Ok talvez porque operações de álgebra em geral são muito mais rápidas em GPU tem algumas operações que não estão portadas né a gente viu no exemplo da atividadea anterior o exemplo do cast né então Casting por exemplo só é só ocorre em CPU não tem como fazer mudança de tipo uma vez que os o dado já está na memória da GPU Então nesse caso a gente tem que sempre fazer os casts sabendo que sempre

- *Corpus ID:* 6709
- *Score:* 0.8678483366966248
- *URL:* oculto
- *Início:* 00:53:36
- *Fim:* 00:55:41
- *Transcrição:* Google e o tensor Flow é o é a única até onde eu conheço né o único que tem suporte para TPU entendeu então mas assim depois a gente na semana que vem a gente vai fazer uns experimentos comparativos tá e se vocês têm uma GPU tipo assim não sei se vale a pena tipo assim vocês vão ver que o desempenho de Treinamento paraas nossas para para alguns exemplos simples tá eh é bem equivalente o desempenho de treinamento em GPU e TPU né então não sei se vale a pena tu tu se preocupar com esse tipo de coisa tá ter o TPU lá específico para ti bom então indo adiante aqui a gente logo em seguida já vai passar pra parte de de eh do queras mais específico mas só para ter uma ideia geral né então o tensor Flow a gente vai importar como TF e do tensor Flow a gente vai importar o caras então com isso a gente vai estar usando a p keras implementada pelo tensor Flow e lá dentro daí a gente vai ter então a instanciação de um modelo então a gente pode instanciar um modelo sequencial dessa forma onde a gente vai especificar Então as camadas né através dessa uma lista é uma lista é um vetor de camadas então todas do mesmo tipo aqui né Então a primeira é uma é uma quer dizer assim do tipo Python né todas elas são queras layers mas a primeira é uma é uma de flaten para para pegar a entrada e tornar ela plana né porque a gente só tá trabalhando com tensores Então são vetores né embora a gente possa trabalhar com vetores multidimensionais a camada de entrada de uma rede neural é um é um não é um cubo ela é sempre um vetor Ok então essa fleten ela permite a gente pegar o dado seja ele qual for uma matriz uma imagem whatever transformar num num vetor e isso vai ser o estímulo inicial para minha rede neural E aí depois tem camadas aqui tem uma camada densa tem uma camada de drop Out para prevenir overfitting com 20% que ela vai meio que esquecer um pouco perder um

- *Corpus ID:* 7036
- *Score:* 0.8674595952033997
- *URL:* oculto
- *Início:* 00:36:35
- *Fim:* 00:39:00
- *Transcrição:* espelhamento ela realmente pega ela realmente faz isso que eu acabei de descrever ela ela replica ela espelha o modelo e faz com que cada placa atue sobre betes diferentes e depois tem uma uma uma fase de suavização né uma combinação Então essa é uma forma de implementar existe uma outra forma que são através de parâmetros centralizados a gente ter um servidor dedicado para gerenciar os parâmetros o que que são os parâmetros é o modelo em si com seus pesos né e Bis Ok E aí no caso dessa segunda estratégia a gente tem uma atualização síncrona ou aí então vamos pras perguntas que parece que apareceu bastante né Jan Carlo Pode falar a sua Av visação dos dados é pegar os modelos que estão nos devices e e ajustar os baias e pesos para todos ficem iguais exatamente tu vai pegar os os baias e os pesos das placas diferentes né que foram que divergiram né E tu vai fazer uma operação de suavização para que tu tenha só uma modelo como se tu estivesse fazendo uma combinação e essa combinação daí tem várias formas de fazer pode fazer uma média por exemplo entendeu seria Exatamente isso Antônio Fagner pode perguntar Bom dia Lucas Bom dia eh a minha pergunta é assim é uma questão técnica do seguinte a gente tá estudando essas técnicas aí eh um p tort o eh tensor Flow cu e eh dnn né todos esses frameworks esse eles oferecem maneiras de a gente fazer isso utilizar essas técnicas que a gente tá estudando na prática tá porque você vai vamos vamos trabalhar em cima disso mas qual desses te oferece uma melhor te oferece a os algoritmos essa divisão ah de ess ess quebrar esses dados né paralelizar por processo paralelizar por dados qual desses é melhor pra gente utilizar entendi ah não existe uma resposta definitiva para essa pergunta porque

- *Corpus ID:* 6845
- *Score:* 0.8671481013298035
- *URL:* oculto
- *Início:* 00:03:25
- *Fim:* 00:05:40
- *Transcrição:* recursos estiverem disponíveis o tensor Flow sempre vai priorizar GPU Ok talvez porque operações de álgebra em geral são muito mais rápidas em GPU tem algumas operações que não estão portadas né a gente viu no exemplo da atividadea anterior o exemplo do cast né então Casting por exemplo só é só ocorre em CPU não tem como fazer mudança de tipo uma vez que os o dado já está na memória da GPU Então nesse caso a gente tem que sempre fazer os casts sabendo que sempre os casts que a gente for fazer vai ser sempre em CPU com todas as consequências que isso traz por exemplo cópia de dados esse tipo de coisa Ok eh bom da parte de eh localização dos dados né é uma forma interessante da gente trabalhar com tensor Flow com gpus é a gente especificar eh a parte num módulo de debug depois vocês podem dar uma olhada na na multitude de parâmetros que tem esse módulo tá de métodos né mas tem um específico quero para vocês que é o setlog device placement que basicamente vai informar pro usuário uma vez que ele está usando os as os tensores esse tipo de coisa fazendo as operações vai dizer sempre onde é que está colocado aquele aquele tensor então com isso a gente consegue eh perceber eh durante a execução né quando a gente visualiza os dados por exemplo saber se eles estão nos locais que a gente imagina que eles querem que a gente gostaria que que eles estivessem tá então ativando isso nesse exemplo aqui ó a gente vai então instanciar ã uma const um tensor constante a partir de uma matriz e um segundo vetor constante um segunda Matriz constante a partir também dessa dessa Matriz Então a gente tem duas matrizes e vejam bem a gente faz uma avaliação gulosa né então é eager evaluation né e a gente consegue ver então que essa primeira operação ó V dier que tá só uma delas com aquela questão da priorização de colocar em GPU no momento que a gente faz essa execução dessa linha aqui ó ele

- *Corpus ID:* 6699
- *Score:* 0.8669005036354065
- *URL:* oculto
- *Início:* 00:37:27
- *Fim:* 00:39:35
- *Transcrição:* foco foco em cores de CPU em gpus E tpus então a gente tem esse esses diferentes backends de execução então provavelmente Quando vocês tiverem usando danor Flow eh a maior parte do código de vocês vai ser ou para api TF keras ou para api do TF data Talvez né para alguns cenários bem específicos talvez se queira programar com uma API mais de baixo nível do do tensor Flow Mas isso é bem rápido tá então assim a gente vai ficar só nesses dois universos aqui mais alto nível Lembrando que depois tem toda essa parte de A da do motor de execução local e distribuído né são esses diferentes kernels que vão estar vão estar sendo usados se eles estão disponíveis vão estar sendo usados claro que por exemplo para usar mais de uma máquina a gente precisa fazer um esforço a gente precisa instanciar o software que vai executar e vai coordenar aquelas várias máquinas Mas se a gente tiver trabalhando uma máquina só eh ela vai usar por default as cpus né E aí a gente precisa fazer um esforço um pouquinho assim mas é mínimo é uma linha de código para adicionar suporte GPU então Eh como a gente já viu lá na cd009 né que o o dask quebra as operações do npai ou do pandas operações em blocos vocês lembram né tipo assim quando eu quero fazer uma operação de join por exemplo de uma uma um dataframe ou um left join que esse left join esses dataframe já tá quebrado em pedaços existe todo um mecanismo um algoritmo em Blocos para fazer isso funcionar então aqui é exatamente o mesma raciocínio a gente vai ter os nossos dados Eles serão quebrados em pedaços em blocos e esses blocos vão executar em paralelo nas gpus como que a gente define o tamanho do bloco isso a gente vai ver depois né mas existe né como a gente vai fazer paraliso de dados a gente vai se focar muito mais em como definir o tamanho mínimo do dado que a gente sobre o qual a gente vai trabalhar

- *Corpus ID:* 6846
- *Score:* 0.8660457134246826
- *URL:* oculto
- *Início:* 00:05:05
- *Fim:* 00:07:15
- *Transcrição:* ã uma const um tensor constante a partir de uma matriz e um segundo vetor constante um segunda Matriz constante a partir também dessa dessa Matriz Então a gente tem duas matrizes e vejam bem a gente faz uma avaliação gulosa né então é eager evaluation né e a gente consegue ver então que essa primeira operação ó V dier que tá só uma delas com aquela questão da priorização de colocar em GPU no momento que a gente faz essa execução dessa linha aqui ó ele já está executando isso nesse device aqui ó e lá no final a gente consegue ver então que ela é uma GPU então ele prioriza posicionar esses tensores em GPU então isso significa o quê que se a gente olhar agora a ocupação de memória dessa GPU T4 que tá disponível aqui né A gente vai ver que essa GPU já teve um aumento da memória h dessa GPU né É bem pequeno né porque afal de contas a gente quase não instanci nenhum dados que é é muito pequeno né mas isso para ter uma noção que ele prioriza daí a locação em GPU e ele não só prioriza a locação desses tensores em GPU ele ele prioriza também eh o cálculo né uma vez que esses dados estão em GPU a multiplicação de matriz o Kernel matm aqui que tá sendo invocado ele vai ser executado lá na GPU também então ele retorna aqui já o valor né o c é uma avaliação gulosa né então C já vai ter o valor resultante dessa multiplicação a gente tá multiplicando nesse caso aqui 2 por 3 3 por 2 então a gente vai ter uma resposta 2 por 2 aqui eh se uma GPU estiver disponível eh a saída do código vai indicar a locação dos dados e operação na GPU caso contrário será na GPU Então dependendo né Eh se a GPU está disponível né pode acontecer né de de dar da operação acontecer em CPU igual né se por exemplo ela não estiver disponível naquele instante da execução do bloco ali a gente consegue forçar também a execução a a localização dos dados em um determinado local porque por defa T vez a gente não quer colocar uma

- *Corpus ID:* 7097
- *Score:* 0.8658984899520874
- *URL:* oculto
- *Início:* 00:58:30
- *Fim:* 01:00:49
- *Transcrição:* podem inclusive testar com TPU também mas antes de gente chegar lá vamos falar rapidamente is aqui de novo é uma é uma um assunto que eu não vou conseguir mostrar para vocês na prática tá porque envolve como executar o tensor Flow no cluster Mas se vocês têm um cluster de computadores o tensor Flow pode ser usado para vocês distribuírem a carga de Treinamento usando essa estratégia de espelhamento em várias máquinas Ok e supondo que as máquinas elas são homogêneas no sentido de que todas elas têm as mesmas placas aceleradoras normalmente é o caso né Tu compra um cluster de 10 nós Cada nó vem com sei lá duas ou oito quatro placas aceleradores e todas as placas aceleradores são iguais Ok então a gente viu essa questão do paraliso de dados né com processo único a gente fez multicore a gente fez GPU uma no caso mas a gente viu como é que funciona para mais de uma e a gente fez TPU Ok então ã para executar o tensor Flow de maneira distribuída né Cada processo eh vai ser um servidor tensorflow Então a gente vai ter um endereço IP uma porta Ok e os tipos possíveis de eh workers digamos assim de servidores seriam esses quatro a gente tem ã o worker que seria quem faz o o treinamento mesmo né Eh o Chief que seria quem eh gerencia a os workers digamos assim o parameter server que é o PS e o e o evaluator esse último aqui seria eh um um tipo possível para fazer eh inferência Ok então a gente pode ter vários workers né esses workers eles vão ser quem responsável por fazer o treinamento mesmo ã e eles vão ser equipos de gpus ou equipados de tpus esse Tif Ele trabalha como um worker A diferença é que esse Tif ele também alimenta o tensor board e ele é responsável por fazer o checkpoint do modelo que a gente tem que fazer sempre o checkpoint né Vocês lemam bem dos callbacks que a gente viu e nós temos o parameter server que o parameter server

- *Corpus ID:* 7017
- *Score:* 0.865485668182373
- *URL:* oculto
- *Início:* 00:06:58
- *Fim:* 00:09:08
- *Transcrição:* também Ok bom feito então Eh dito isso Eh vamos adiante então começar então com a parte efetivamente da aula então para isso a gente vai olhar Então esse primeiro conjunto de slides aqui que é usando gpus para serar acelerar cálculo é a é esse primeiro conjunto de slides aqui então tá eh então o objetivo Então desse conjunto de slides a gente entender como que o treinamento acontece né Eh com múltiplas gpus Então a gente vai começar falando assim de uma parte mais básica né de como que a gente pode usar uma GPU e depois a gente avança para mais de uma GPU então h o suporte a GPU no tensor Flow como a gente já viu né Ele é nativo e o próprio tensor Flow ele prioriza o uso de gpus em relação a cpus porque as gpus são absurdamente mais eficientes para eh se fazer isso então mesmo que tenha cpus disponíveis ele não vai usá-las ele vai D prioridade para as para paraa GPU ou para as gpus e o e o mecanismo de detecção dessas várias gpus ele é de certa forma automático certo quando a gente tá trabalhando numa única máquina né porque ele já ele já foi concebido para isso então uma das primeiras coisas que ele faz é verificar Quais são as gpus que tem e tudo mais é claro que a gente tem algumas funções por exemplo essas aqui eh que é essa que aparece aqui que que Inclusive a gente já viu né que permite a gente eh listar Então quais são os devices físicos né então Eh como a gente já mencionou né existe toda toda a operação do tensor Flow é transformada num grafo de tensões que no caso seria um grafo de tarefas também né grafo de tensores a gente usa esse nome né porque é o nome da ferramenta né tensor Flow fluxo de tensores e esse grafo de tensores eles são escalonados né entre gpus e entre cpus que porventura estejam disponíveis para fazer o cálculo né então existe a figura de um escalonador né que vai fazer essas decisões claro

- *Corpus ID:* 6863
- *Score:* 0.8642154335975647
- *URL:* oculto
- *Início:* 00:33:40
- *Fim:* 00:35:42
- *Transcrição:* seria bem difícil de fazer isso né mas daria para fazer né vamos supor que tu tenha uma matriz gigante tu quer fazer um sei lá uma operação nessa Matriz nesse data né trabalhando com números né supor que é uma matriz de números tu quer fazer tu pode quebrar essa Matriz tu pode fatiar essa Matriz em pedaços né usando toda a parte de fatiamento que a gente viu e jogar o processamento em cada uma dessas gpus se fossem elas físicas realmente teriam ganho bastante grande né como elas são lógicas tu ainda vai ter um ganho né porque a GPU embora ela ela a GPU da NVidia especificamente né embora ela tenha esse conceito de tu mandar um Kernel para executar e ele retornar né dá para trabalhar de maneira assíncrona com essas gpus né inclusive invocação de querne é assíncrono né então se tu fizer a locação de memória síncrona e todo esse tipo de coisa a gente não entrou muito nesse nível de detalhe lá na outra disciplina né mas se tu fizer tu consegue fazer eh a execução de vários KS ao mesmo tempo entendeu então é isso que o tensor Flow tá usando lá por trás entende quando tu faz esse tipo de operação Mesmo trabalhando com gpus lógicas entende então ã isso pode trazer para ti um benefício que é o melhor uso da tua GPU física entende mas vai ter uma maior ocupação isso Dev pos entendi mas se eu por exemplo for treinar al uma rede profunda e eu tiver várias gpus eu não preciso em tese me preocupar com essa divisão ele vai usar todas ele vai usar todas né ele vai usar todos porque isso vai depender na realidade de como que tu instancia né tipo assim vai ser transparente somente se tu instanciar corretamente esse objeto aqui entende isso a gente vai ver mais para frente daí aí tu vai criar tua tu vai tu vai criar essa estratégia de distribuição vai ter lá uns parâmetros a gente vai ver tá E aí tu vai instanciar o teu modelo vai compilar o teu modelo na hora que tu for fazer Fit o Fit do


**Resposta gerada pelo LLM**: O TensorFlow prioriza a execução de operações em GPUs em relação a CPUs, mesmo que CPUs estejam disponíveis.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: A resposta deve ser relevante para a pergunta sobre em qual dispositivo o TensorFlow prioriza a execução de operações. Supporting Evidence: A resposta aborda diretamente a questão do prompt, afirmando que o TensorFlow prioriza a execução em GPUs em vez de CPUs. Isso é uma informação relevante e correta sobre o funcionamento do TensorFlow, respondendo completamente à pergunta feita.  Score: 3


---
