**Pergunta 156**: Como a política de um agente se relaciona com o comportamento do agente em um ambiente? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 4557
- *Score:* 0.8608911037445068
- *URL:* oculto
- *Início:* 01:03:23
- *Fim:* 01:05:44
- *Transcrição:* política do a gente ela é uma soft Max ou seja tem aleatoriedade já soft Max né então tem aleatoriedade tem aleatoriedade na política Ou seja a partir de um estado que o agente recebe né ele pode retornar ações diferentes no mesmo estado por causa do soft Mark que vai fazer um sorteio depois então e Outro fator né outro problema é aleatoriedade do ambiente aleatoriedade do ambiente né ou seja dado um estado né se o step ali né a partir de a da da mesma mesmo que o agente retornasse à mesma ação lá no estado o step pode retornar observações diferentes por causa de aleatoriedade né observação lembra lá da transição né do do deixa eu até modelar isso aqui como um grafo né Então a partir de um estado o ag gente tem que decidir qual ação ele faz então aqui usando a convenção de cores de antigamente né então o Estado aqui em preto e a ação que o agente pode escolher em preto também né então supondo que ele tem duas ações aqui então o primeiro problema tá aqui ó na política eu desenh a política de preto né que é que tá ao alcance do agente então a política já é soft Max então o agente pode est na mesma situação do ambiente então na primeira vez ele executa a ação A1 na outra vez ele executa a ação A2 né então já tem aleatoriedade aí ou seja isso aqui pode se ele executou ação boa né é a mesma política tá aí ele vai ter um desempenho bom vai ter um jot lá em cima né com a mesma política e numa outra execução e supondo né que fosse essa mesma política ele executou ação ruim o desempenho pode ir ler embaixo né a mesma política tem uma variância grande no no valor aqui e a segunda questão é que mesmo que ele executar 100% das vezes né uma das ações o ambiente tem aleatoriedade também né Pode ser que aqui tem uma certa probabilidade do ambiente aqui ele cai num estado que seja que tenha valores né diferentes

- *Corpus ID:* 4558
- *Score:* 0.8607090711593628
- *URL:* oculto
- *Início:* 01:05:13
- *Fim:* 01:07:23
- *Transcrição:* supondo né que fosse essa mesma política ele executou ação ruim o desempenho pode ir ler embaixo né a mesma política tem uma variância grande no no valor aqui e a segunda questão é que mesmo que ele executar 100% das vezes né uma das ações o ambiente tem aleatoriedade também né Pode ser que aqui tem uma certa probabilidade do ambiente aqui ele cai num estado que seja que tenha valores né diferentes então tem uma aleatoriedade dupla aqui né uma na política do Agente né O que que tá desenhado em preto e outra na elidade do ambiente né que é o que tá desenhado em vermelho aqui tá eh só para ilustrar né os dois pontos de aleatoriedade ou seja mesmo que a política né a política esteja estável aqui né o o desempenho pode variar Bastante né E aí eu posso achar que a mesma política estava boa num momento e no outro momento a a política muito parecida vai tá muito ruim né E aí Isso me deixa ou seja se a política tava muito boa no primeiro momento eu vou achar que os paretos estão bons E aí se a política tava muito ruim no é a mesma política mas eu vou achar que ela tá ruim no outro momento eu vou querer mudar né vai ter um Gradiente aqui indicando uma direção de mudança E aí isso gera né mudanças comportamentos erráticos aqui no no agente Então são esses dois problemas né advindos dessa questão né do de eu ter que esperar até o final do episódio né Porque pensa lá no estado Inicial quantos estados eu tenho que percorrer até chegar no fim né Quais são as possíveis trajetórias né são muitas então por isso que a variancia é muito grande né a partir do Estado Inicial Eu posso percorrer uma infinidade de caminhos para chegar até o final então a variância dessa do desempenho aqui pode ser muito grande deu para ter uma ideia mais ou menos dessa dessa instabilidade aqui hã deixa eu ver se tem uma um

- *Corpus ID:* 4273
- *Score:* 0.8588435649871826
- *URL:* oculto
- *Início:* 01:30:08
- *Fim:* 01:32:20
- *Transcrição:* tô rotulando aqui o rótulo estando correto né E aqui esquerda é ação de para baixo eu fiz com a setinha para cima mas não tem problema mas aí né para completar esse desenho teria que completar aqui o que que acontece quando o a gente vai para baixo né o que que pode acontecer e aqui à esquerda também completar né o que que pode acontecer então tá incompleto aqui esse desenho vou deixar os três pontinhos aqui para simbolizar o incompleto tá qualquer dúvida a gente pode completar aí depois tá aí quando eu me falei quando eu mencionei aquele negócio do o jeito do ag gente se comportar isso é a polí né tem um nome técnico para isso política é o comportamento do agente Agora sim a política é uma coisa do Agente né enquanto Todas aquelas regras ali né ou Todas aquelas outras coisas lá os estados ações a função de de transição e a função de recompensa são do ambiente ambiente e o agente ele tem a política a política mapeia eu posso definir a política dessa forma ó eu jogo um estado e uma ação lá dentro e ela vai me retornar qual é a probabilidade do ag gente escolher aquela ação é isso aqui ó qual é a probabilidade do ag gente escolher a ação a no estado s ou eu posso pensar também né como uma distribuição de probabilidades eu posso só digamos informar né entre aspas omitir a ação aqui ó e eu obtenho uma lista né de cada ação ação A1 a chance dela probabilidade do agente fazer ela ação A2 a chance do a gente fazer ela ação A3 e assim vai né eu posso pensar dessa forma aqui também isso é a política né É o jeito que o agente vai se comportar naquele ambiente hã e é um caso geral isso né a política estocástico Ou seja no mesmo estado o ag

- *Corpus ID:* 4279
- *Score:* 0.8581236004829407
- *URL:* oculto
- *Início:* 00:01:49
- *Fim:* 00:04:14
- *Transcrição:* realmente não tem sentido desenhar nenhuma setinha para onde a gente iria querer ir porque aqui termina a interação dele com o ambiente e aí agora Existem várias maneiras né do agente se comportar no ambiente essa é uma delas outra por exemplo né desenhar de vermelho aqui seria o a gente agir né mapear a cada estado para essa setas em vermelho aqui né E talvez essa outra maneira seja faça menos sentido ou seja pior Enfim então tem várias maneiras e algumas maneiras são ótimas né no sentido de que elas conseguem fazer o agente a gente consegue fazer a coisa certa com elas e o fazer a coisa certa é conseguir a maior né o maior somatório maior sequência de Recompensas possível e só que não é só uma sequência de Recompensas tá a gente vai tratar isso né uma pensar aqui ó em aplicar um raciocínio em cima dessas Recompensas contabilizar algumas coisas interessantes aqui por exemplo né uma das coisas que a gente queira contar né contemplar aqui eu cheguei a mencionar isso brevemente Mas vamos supor um estado aqui né que o a gente tenha duas ações para tomar é uma ação a um aqui uma ação A2 logo que ele toma essa ação A1 ele chega no estado que ele perde né Tem uma recompensa de menos um e aqui tem uma recompensa de mais E aí depois né tem só uma sequência de ações aqui para fazer que no final das contas né vai ter uma soma de Recompensas aqui que no total vai dar mais 100 Total aqui e aqui vai dar menos 200 né as outras Recompensas que ele for tomar aqui então uma das coisas que a gente tem que contemplar né para pensar na política ótima por exemplo nesse estado aqui que eu vou chamar desse zero ação A2 ela dá a maior recompensa imediata então se eu fosse olhar só a recompensa que eu ganho

- *Corpus ID:* 4244
- *Score:* 0.85222989320755
- *URL:* oculto
- *Início:* 00:36:41
- *Fim:* 00:38:57
- *Transcrição:* disponíveis retorna uma delas retorna uma delas e recebe o retorno dela o retorno é quantos pontos aquilo valeu e o novo estado que chegou né Então essa que é a a interação né o ciclo do agente com ambiente então a gente recebe o est est o estado vou chamar de S aqui né de que é A nomenclatura comum que a gente usa em inglês né state então a gente recebe o estado retorna uma ação e aqui o ambiente ó ambiente e aí o ambiente processa aquela ação né e retorna pro agente o novo estado e a recompensa é o rzinho aqui de recompensa que é esse sinal aqui ó de imediato né avaliação da ação que o agente fez tá então esse que é o a ideia aqui do que que a gente quando tiver né pensando no ambiente esse sinal aqui não é ação correta né o ambiente não sabe qual é a ação correta né Como que o agente deveria se portar o agente tem que se virar e descobrir mas a gente sabe se aquilo foi bom ou ruim né então a avaliação de o quão bom ou quão ruim foi tá aqui na recompensa certo e aí uma outra questão importante né né ao pensar em modelar o ambiente é que esse sinalzinho de recompensa né Igual tá aqui no slide anterior deixa eu copiar el aqui copiar aqui esse sinalzinho de recompensa não precisa ser a cada ação micro né que o agente faz ali né Então pensa no num sei lá num jogo de plataforma lá no clássico né aqui né pessoal que é mais novo já talvez não esteja Ah se bem que tá né fizeram várias versões novas aí aí tem que saltar l o bigodinho dele aqui tem que saltar numa plataforma né e chegar do outro lado aqui e pegar a moeda né tem a moedinha dele aqui e eu não preciso ficar dando eh por exemplo se eles consegu fazer o salto aqui né eu não preciso recompensar imediatamente esse salto nesse ambiente do Mário ele pode simplesmente a

- *Corpus ID:* 4242
- *Score:* 0.8476595878601074
- *URL:* oculto
- *Início:* 00:33:22
- *Fim:* 00:35:35
- *Transcrição:* se você tiver já pronto um algoritmo e realmente já existe o seu trabalho vai ser modelar né o ambiente descrever esse ambiente para que o agente Quais são as percepções né que o a gente pode fazer Quais são as ações dele e cada ação que ele faz como que o que o ambiente muda é exatamente isso tá o o matematicamente falando é o processo de ção de marcov é descrever Exatamente isso quais são as possíveis situações do ambiente quais jeitos né que que o o ambiente pode estar né tem a ver com as percepções do agente Quais são as ações que o a gente pode fazer e quando a gente faz uma ação Como que o ambiente muda e gera a próxima percepção do agente é isso e matematicamente então pensando nesse seguinte na seguinte ideia tá eh e em aprendizado por reforço né antes de entrar ali na matemática do processo de ção de marcov isso aqui é muito interessante ó o feedback né o o que que o agente recebe é uma avaliação da da agente dele um sinal né sobre a ação que ele fez então pensa assim tá aprendendo a jogar basquete E aí você vai tenta arremessar a bola numa cesta aí a bola cai um pouco abaixo da cesta lá então você é uma avaliação né você tem essa avaliação a bola Foi muito baixa mas você não tem né não é que vai ter um oráculo que vai ser materializar no meio das nuvens e dizer ó seu ângulo do seu braço tinha que ter sido 45º e a força de de de 15 newtons para empurrar a bola isso essa instrução né é o feedback instrutivo né seria como se fosse é um aprendizado supervisionado né alguém te disse exatamente qual é a ação correta e esse é o caso do aprendizado supervisionado aqui em aprendizado por reforço a gente não tem isso a gente tem só uma avaliação um sinal né de como que a ação foi feita a gente não sabe qual é a ação correta não sabe E aí um um E aí do ponto de vista do agente de aprendizado por reforço é um pouco ainda mais complicado do ponto

- *Corpus ID:* 4275
- *Score:* 0.8473120331764221
- *URL:* oculto
- *Início:* 01:33:29
- *Fim:* 01:35:53
- *Transcrição:* men1 esse aqui Men me e aqui 0.3 positivo é a política gulosa do ag gente vai ser escolher 100% das vezes isso aqui né ação que ele acha que tem um maior valor né claro né Não Faz Sentido se a gente acha que vai ganhar esses números aqui eu querer escolher outros números outra ação que vai dar outros números n não é racional isso então um agente aprendizado por reforço ele tende a ter um comportamento racional Ok Hã tá aí agora né teria um novo um exemplo aqui de de política eu posso representar né aqui numa política determinística né eu posso representar ela desenhando né Em cada estado eu vou lá e desenho né Qual é a ação que a gente vai fazer é o ou seja por exemplo aqui no estado 31 né a política do agente pro estado 31 né a política determinística 31 esse aqui né é ação de ir para cima enfim né é simples assim OK Hã tá aí depois né a gente tá Eh vamos tirar a dúvida aí depois a gente vai pro intervalo aí eu falo de política ótima vamos lá Salvador Professor eh não sei se você vai falar depois aí talvez negócio política ótima eh pensando aí nesse exemplo aí do robô que tem que aspirar né ele tem ao mesmo tempo que sim que ele tem que chegar na base para se recarregar ele também tem que passar por todos os quadrantes aí do do mundo 4x TR aí para limpar tudo ah como representado is aí tá o jeito de eu apresentar esse mundo pode ter induzido a isso né mas deixa eu reformular então a apresentação do mundo vamos supor que já tá limpo tá E ele só precisa ir voltar pra base porque realmente não faria sentido eu dar uma recompensa negativa para ele por passar em cada lugar se ele tivesse limpando né limpar é cumprir a missão então eu teria que recompensá-lo positivamente por isso mas vamos supor que é um um ele tá no estado já de só querer recarregar né e não mais precisar limpar É talvez robô aspirador não tenha sido o melhor melhor metáfora

- *Corpus ID:* 4567
- *Score:* 0.8454449772834778
- *URL:* oculto
- *Início:* 01:22:11
- *Fim:* 01:24:40
- *Transcrição:* tem no próximo estado e a recompensa é Idêntica né porque é o que veio do ambiente então nos dois a diferença tá é que no dqn a política tá baseada nesses valores q aqui né lembram ele né ou nesses aqui enfim nos valores q primeiro DKN calcula os valores q e depois escolhe né a ação lá de acordo n com guloso então primeiro o dqn calcula os valores q para depois escolher a ação na hora de atualizar É bem parecido só que a diferença é na hora da de agir né o ator lá no no ator crítico ele não usa os valores q aqui para escolher a ação tá ele usa a política dele né Por exemplo o soft Max né ele não faz éson guloso né então tem toda a vantagem de não precisar calcular o valor e tem inclusive né aquela vantagem de eh eu só preciso ordenar as ações corretamente né a melhor ação tem que ser a preferida né tem que ter a maior chance de ocorrer e enquanto no dqn ele tem um compromisso né esses valores q aqui são justamente né O que que o a expectativa de recompensa né e o ator não tem isso o crítico tem né o crítico tem isso mas o ator não tem então a parte de agir fica né desacoplada da Estimativa de valor né o ator só precisa gerar uma política na qual a melhor ação esteja né com preferência sobre as Mais enquanto aí tudo bem O crítico ele cuida da Estimativa de valor então junta o melhor o benefício das duas coisas o dqn tinha o benefício de melhorar a política cada interação com o ambiente o rainforce tinha o benefício de não precisar estimar os valores pro agente agir né então com ator crítico ele tem as duas coisas né o ator não precisa estimar valores só precisa ordenar as ações e eu posso atualizar né o a cada momento porque agora eu tenho né o crítico para cuidar né da Estimativa de valor Ok deu para pegar mais ou menos a a ideia básica

- *Corpus ID:* 4568
- *Score:* 0.8441553115844727
- *URL:* oculto
- *Início:* 01:24:03
- *Fim:* 01:26:55
- *Transcrição:* o benefício de melhorar a política cada interação com o ambiente o rainforce tinha o benefício de não precisar estimar os valores pro agente agir né então com ator crítico ele tem as duas coisas né o ator não precisa estimar valores só precisa ordenar as ações e eu posso atualizar né o a cada momento porque agora eu tenho né o crítico para cuidar né da Estimativa de valor Ok deu para pegar mais ou menos a a ideia básica aqui aí então tem vários métodos né baseados nessa ideia Cada um com as suas sofisticações tá E e vários deles têm né Hã suas características com relação à paralelização a controle de atualização e hã aqui por exemplo ó o a2c né É asynchronous acoc critic e aqui asynchronous Advantage actor critic né tem três as e um C aqui no nome né mas tudo um dos as é ator e o c é crítico o saque também é soft actor critic né ator crítico suave aqui para ações contínuas o ppo não tem actor critic no Name mas no nome dele né mas ele é um método baseado em ator crítico e o ppo em especial né merece a nossa atenção aqui por várias características tá ele é fácil de paralelizar e a atualização dele ele tem a ideia de clipping né o pro o p aqui dele um dos PS né o nome dele é proximal policy optimization tá esse P aqui Inicial né do proximal é que ele não faz atualizações bruscas na política tá não faz atualizações bruscas na política ou seja né quando tem aquele negócio lá do ator de receber né uma atualização essa atualização né em vez de ter tem lá eh a questão da taxa de aprendizado tudo bem mas o próprio ele limita digamos o o erro né ele tem algum componente lá que de forma aqui uma nova

- *Corpus ID:* 4261
- *Score:* 0.8437824249267578
- *URL:* oculto
- *Início:* 01:06:14
- *Fim:* 01:08:32
- *Transcrição:* diferenciar né se eu tô dizendo eh realmente quando eu tiver né interagindo com o ambiente eu vou querer saber né o estado que o agente chegou se a qualidade dele mas de uma maneira geral eu posso definir a função né sem me preocupar se o estado que tá sendo consultado é o estado que o agente tá ou que ele chegou Ok e até porque também né o quando o agente interage com o ambiente ele tá no estado faz uma ação recebe lá uma recompensa e chega no novo estado esse linha ao chegar nesse novo estado acontece em algum momento que o estado do agente vai ser esse novo estado né né então ele tem que anotar eu cheguei nesse mas agora vai ser meu estado atual né para num próximo momento eu decidir qual é vai ser a minha nova ação nele e receber a nova recompensa e chegar no novo estado atual E aí vai isso vai se repetir né o estado eh futuro estado atingido vai ser o estado atual dele né que é assim na vida né a gente faz uma coisa e agora no momento presente aquilo que eu cheguei é onde eu tô né mais ou menos essa ideia também ok professor na recursão vai ser eh na função vai ser uma recursão ele vai atries o s não Hã Não é não não chega pode pensar de maneira recursiva Mas você pode pensar de maneira interativa também né um laço ali né um for um a alguma coisa assim no qual né você faz uma ação eh faz uma ação em algum estado recebe recompensa e para no estado futuro e aí dentro desse laço né você vai atribuir né o estado que você tá é o que você chegou e aí no próxima no próximo execução desse laço na próxima execução desse laço Você tá no s aí você vai escolher o a e recebe a recompensa e recebe o s linha e faz isso aqui de novo e aí repete isso é uma repetição talvez dê para pensar assim também ok E aí então aqui tem um monte de conceito novo né algumas notações matemáticas


**Resposta gerada pelo LLM**: A política de um agente define o comportamento do agente, mapeando um estado e uma ação para retornar a probabilidade do agente escolher aquela ação naquele estado.  Pode ser vista como uma distribuição de probabilidades para cada ação em um dado estado. Em um caso geral, a política é estocástica, ou seja, no mesmo estado, o agente pode retornar ações diferentes.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the relationship between an agent's policy and its behavior in an environment. It explains how a policy maps states to actions and describes the probabilistic nature of this mapping, which is essential to understanding the agent's behavior. The explanation is clear and relevant to the prompt's inquiry about the connection between policy and behavior.  Score: 3


---
