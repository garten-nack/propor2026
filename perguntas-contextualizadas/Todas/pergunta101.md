**Pergunta 101**: O que são stop words e por que elas são removidas durante o processamento de texto?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8532
- *Score:* 0.894615650177002
- *URL:* oculto
- *Início:* 00:16:46
- *Fim:* 00:19:09
- *Transcrição:* quando a gente fala em impeding contextual ali na fase do pré-processamento tava lendo alguma coisa a respeito o que a gente sempre fazia quea que é aquela etapa de remoção de stopwords nesse caso eh normalmente não se aplica né não se aplica olha aqui ó já vou entrar Desculpa pessoal que eu não tô vendo o nome das pessoas porque me sumiu essa tela Eu não vou mexer eh tu tens razão tá ó aqui essa primeira etapa a gente não usa ah a gente sabe assim ó as Stop Word são um problema pro tópico a gente não quer um tópico com o a ele ela isso não significa nada para nós né com palavras de ligação ali ão a gente não quer elas essas palavras não representam um tópico Então a gente vai em algum momento eh retirar ess essas stopwords tá então isso é importante entender que sim impacta na nossa representação lembra da figurinha cada tópico tem um conjunto de palavras eh que se relaciona que que que denota que nos traz a mente um conceito então não pode ser Stop Words nós vamos usar todas as técnicas que a gente usou anteriormente nesse processo vamos usar também mais poderíamos usar poderíamos usar mais filtros de classes gramaticais por exemplo a gente olha e diz não adjetivo não representa bem um tópico eu quero só verbos e substantivos que é uma coisa comum que se faz tá porque ah bonito não acho que não no meu caso aqui de Notícias isso não não vai agregar então eu vou remover V só que tudo isso aqui tem um momento para ser feito então esse é importante entender que esse pré-processamento ocorre em dois momentos nesse início O que nós vamos fazer E aí no no exemplo a gente vai explorar isso a gente vai remover links e emojis por exemplo eu acho que nem emojis eu removo ali mas são coisas que elas não elas não estariam naturalmente num texto tá coisas como links assim eu diria de maneira geral e e e eu vou explicar já por quê Porque nós vamos usar o modelo para gerar

- *Corpus ID:* 3941
- *Score:* 0.8939350843429565
- *URL:* oculto
- *Início:* 00:26:23
- *Fim:* 00:28:56
- *Transcrição:* eles de Stop Words são removidos do do vocabulário eh tem uma grande discussão aí depende muito do nosso poder de processamento né Eh removê-los é interessante porque são dimensões que eu vou tirar do conjunto de dados Eles não têm um poder discrimite muito alto né já que eles aparecem em grande parte dos dos dos documentos mas mas para fazer alguma análise mais mais sólida né um PLN mais pesado para criar embeds por exemplo ou para entender não é a sintaxe do texto se nós fôssemos aplicar alguma técnica para selecionar sua por exemplo sujeitos verbos predicados elas são importantes de serem mantidas durante esse processamento mas depois elas podem ser removidas Então é eu só só queria mostrar aqui o Se vocês fizerem Não não é Amazon eu quero que procurar por por a né então para vocês verem como o que que significa isso né a a letra A em várias línguas eh significa artigo né em algumas significa preposição eh ela ela eh aqu ele botou substantivo masculino ainda mas o que eu me importa que eu quero mostrar isso né a quantidade de de vezes n aparece aqui aqui seria 1 milhão bilhão 24 bilhões né de documentos ou trilhões no caso então Praticamente tudo tudo que é que é que é lugar ela aparece então ela não é discrimite se eu botar Se eu colocar essa dimensão ela vai ser uma dimensão que que todos os os as entidades vão ter Então como ela não é discrimite é um pouco daquilo que a gente viu na primeira aula né coisas que não são discrimites não precisam ser usadas no Esso então eu posso tirar fora então as preposições Elas têm essa preposições de outras Stop Words Elas têm essa propriedade não é e seguindo adiante nessa questão uma uma outra técnica que que também é importante é é a distem iação né é um fcis isso mas o stem é uma

- *Corpus ID:* 8107
- *Score:* 0.8820817470550537
- *URL:* oculto
- *Início:* 00:12:55
- *Fim:* 00:15:25
- *Transcrição:* comuns então a gente pode mover essas essas palavras também por por questões de eficiência porque a gente vai ter um texto bem menor para para trabalhar por outro lado a gente perde significado algumas vezes Então aqui tem um exemplo Ser ou não ser eis a questão se eu remover toda tudo que é Stop Word vai sobrar só questão E aí perde totalmente o o significado da da expressão lá do do do Hamlet Então por por essa razão alguns sistemas optam por indexar as estores alguns sistemas de recuperação de informação ou de eh mais adiante a gente vai ver que os modelos que que os modelos generativos e os modelos de transformer eles precisam das stopwords mas aqui a gente tá na onda anterior ainda onde Era bem comum remover stopwords então a gente vai remover ou não vai remover bom aí depende do Objetivo se o que eu quero aprender é como é que a linguagem funciona eu quero formar a sentensas Eh que que sejam gramaticamente corretas Então eu preciso das stopwords agora Se o se o que eu quero fazer é uma classificação ou é recuperação da informação provavelmente eu não preciso das stopwords porque elas vão elas vão ser muito comuns e não distinguem uma coisa da outra por exemplo se eu quero distinguir um se um artigo é de não sei de de sei lá fala sobre redes de computadores ou sobre ou sobre computação gráfica eles vão ter stopwords igualmente distribuídas nos dois grupos de artigos então a presença delas não me ajuda a a distinguir um assunto do outro então depende do Objetivo eh um passo adiante das stopwords a gente também ã pode fazer Ah stemming então não é steaming É stemming mesmo é que é na prática é tratado como remover sufixos Então para que que serve remover sufixos das palavras serve para unificar as formas variantes de uma mesma palavra e o stem então da onde vem o o o nome da

- *Corpus ID:* 3975
- *Score:* 0.8814271688461304
- *URL:* oculto
- *Início:* 00:04:04
- *Fim:* 00:06:22
- *Transcrição:* análise resultante não teve efeito em termos de performance então isso dá dá a entender que as Stop Worlds poderiam ser removidas né O problema é que o objetivo justamente dessas redes e tem essa discussão aqui em outros lugares é que elas esse tipo de rede aprenda não é a sequência e a formação né sintática das coisas então todo pressuposto é que removê-las pode afetar né esse tipo de compreensão que essas redes têm desse tipo de sintaxe e esse aqui é um estudo né que mostra que eventualmente isso não aparentemente não seria um problema mas a gente não tem estudos ainda mais sólidas né que comprovem isso então a sugestão ainda manter de qualquer forma esses modelos são criados em arquiteturas computacionais uma grande quantidade de exemplos o problema desse caso não é não é ter poder computacional então querem continuar usando isso para que os modelos eventualmente não sejam prejudicados enquanto não tiver um estudo que comprove isso e partindo do pressuposto que o objetivo é aprender essa sequência e muitas tarefas que nós temos elas não são só de analisar similaridade ou agrupamento são tarefas até de gerar texto não é que é o caso que faz o chat GPT e outros ou de avaliar né eu dou um início de uma sequência ele completa a frase Enfim então é importante manter essas palavras nesses outros tipos de pelo menos para esses outros tipos de tarefas né e de qualquer forma como eles colocam aqui de alguma maneira o modelo aprendeu que essas palavras não são úteis né e faz um ajuste no peso como a gente no tfdf que algo mais simples para que isso seja considerado então sugestão mantém elas porque modelo em teoria é um modelo que vai ser mais robusto vai aprender que elas são palavras que sintaticamente são importantes e em algumas situações em

- *Corpus ID:* 3987
- *Score:* 0.8806443810462952
- *URL:* oculto
- *Início:* 00:25:07
- *Fim:* 00:27:37
- *Transcrição:* dentro dessa lista se estiver ele coloca trouxe e não estiver ele coloca falso E aí eu tenho uma coluna essa só para dizer se essa palavra seria Stop World ou não com base na minha lista e não vejam que eu tenho algumas aqui três a quantidade eu pedi para ele mostrar aqui embaixo eu peço para ele agrupar eu teria sete palavras que são Stop Word dessa lista de 35 aqui é a frequência frequência de aparição não é a contagem se fosse contar as concorrência seriam 10 de 37 é basicamente 25% aqui das palavras são são Stop Words né é uma quantidade considerável Então já sabemos identificá-las Vamos removê-las né então é muito fácil nós removermos as Stop Words basta eu na hora que for instanciar o modelo dizer para ele tem um atributo que é chamado Stop Words aí eu passo a lista de Stop Words ele já vai criar uma matriz tirando fora essas Stop World então quando eu chamo aqui o Fit ele já remove essas Stop Words é um resto aqui é a mesma coisa eu só agreguei uma coluna aqui para ele verificar se esse Stop Words foram removidas ele vai dar falso aqui em tudo porque só para a gente comprovar que elas foram removidas né então não tem mais Stop Words considerando aquela lista né que eu tinha passado para ele então se vocês quiserem incluir outras é só acrescentar aquela lista que aqui acho que não tem mistério deixa eu ver aqui do lado aqui não tem nenhuma acho que dúvida né não tem tem uma dúvida aqui é Salvador por favor não tá a palavra D é por alguma razão ou esquecimento só é eu coloquei eu coloquei de eu tirei ela de propósito né Para que vocês percebam né que primeiro Stop hoje é para o domínio né ela pode não não corresponder aquilo que

- *Corpus ID:* 3974
- *Score:* 0.8800138235092163
- *URL:* oculto
- *Início:* 00:02:09
- *Fim:* 00:04:43
- *Transcrição:* tem um pelo menos cada Microsoft uma dessa Universidade aqui que eu não pesquisei onde é que é CN Eu acho que isso aqui é que é faxina Ok chineses tempestade muita coisa nessa área mas enfim o repositório que eles usaram é público tá disponível em estudo de 2019 antigo é claro que a avaliação deles foi feita especificamente aqui para ver os comportamentos né desse tipo de modelo em ranqueamento de respostas né perguntas que são feitas em motores de busca E aí ele se eles fizeram um pouco dessa análise especial do fine tuning do modelo e eu achei interessante que foi alguma discussão que já apareceu antes acho que é mais embaixo um pouco aqui tem os resultados né e o mais interessante é que foi isso aqui e eles fizeram uma análise da remoção desses marcadores né eles fizeram marcações que eles usaram para poder ser treinado tem específica para palavras top Words e eles disseram que removendo esses marcadores primeiro ele analisar e viram que os marcadores de Stop Words em comparação com as palavras tradicionais receberam dentro da rede construir da mesma atenção quer dizer a importância foi foi a mesma e mas ao mesmo tempo removê-las não é da análise resultante não teve efeito em termos de performance então isso dá dá a entender que as Stop Worlds poderiam ser removidas né O problema é que o objetivo justamente dessas redes e tem essa discussão aqui em outros lugares é que elas esse tipo de rede aprenda não é a sequência e a formação né sintática das coisas então todo pressuposto é que removê-las pode afetar né esse tipo de compreensão que essas redes têm desse

- *Corpus ID:* 8119
- *Score:* 0.8799950480461121
- *URL:* oculto
- *Início:* 00:34:50
- *Fim:* 00:37:05
- *Transcrição:* pessoas que né mas eu poderia construir a minha lista eu poderia Inserir a palavra produto aqui tá ou outras palav que eu quero retirar Então isso é uma coisa simplesmente é uma lista que nós vamos usar e o nltk tem uma lá e as outras bibliotecas podem apresentar essa com mais algumas coisas ou menos então a gente também tem que cuidar o que tá nessa lista porque eu tô usando essa biblioteca do nltk mas eu preciso pensar né porque lá em cima apareceu que a palavra não ela tava muito importante ou seja ela aparece muito na classe negativa então aqui eu criei uma listinha on eu vou checar essas palavras porque eu pensei tá não deixa eu ver se tem sim também e nunca porque como é que será que tá isso n toop Words então eu vejo que a palavra não se encontra na minha lista de Stop Words e sim e nunca não não não via em cima como top 10 tokens mas eh resolvi verificar mas o não é importante aqui porque ele está então se eu remover isso eu posso estar tirando uma característica determite da classe negativa então então eu removo aqui da lista tá removo e agora eu sei que eu vou usar essa lista de Stop Word sem a palavra não aqui eu poderia incluir outras e remover outras palavras também aqui tem uma função de limpeza que é um exemplo ela tem que isso aqui tem que ser observado isso aqui não dá para ser usado assim ó para todos os casos são casos e casos e é tem que pensar em cada coisa que a gente remove ou que a gente altera Então essa listinha tem uma aqui uma lista de caracteres especiais a ela uma função de pré-processamento que ela remove esses caracteres especiais aqui de uma forma bem bobinha depois o que a gente faz a gente remove Stop Words aqui ó onde a gente simplesmente Verifica que a palavra que ele que ele gera com a tokenização está na lista de stopwords ela não entra ela não não não é não é ah considerada

- *Corpus ID:* 8120
- *Score:* 0.8796862363815308
- *URL:* oculto
- *Início:* 00:36:28
- *Fim:* 00:38:50
- *Transcrição:* remove ou que a gente altera Então essa listinha tem uma aqui uma lista de caracteres especiais a ela uma função de pré-processamento que ela remove esses caracteres especiais aqui de uma forma bem bobinha depois o que a gente faz a gente remove Stop Words aqui ó onde a gente simplesmente Verifica que a palavra que ele que ele gera com a tokenização está na lista de stopwords ela não entra ela não não não é não é ah considerada né Depois a gente remove pontuação do texto aí tem aqui nós estamos usando um reex aí tem que conhecer bem esse comando para entender o que que ele vai tirar tudo né às vezes vale mais a pena a gente tirar só o caracter que a gente tá enxergando se a gente não entendeu não entende bem o a abrangência desse comando né aqui um outro comando Rex que remove links então ele links ele vai remover o link inteiro se eu tiver um link lá vai sair do do dataset aqui eu vou remover contas eu poderia substituir Tá mas eu vou remover ã vou também remover hashtags remover números sozinhos e também vou remover palavras com números vou remover os acentos por que que eu vou remover os acentos porque algumas palavras em textos informais especialmente e reviews é uma um texto informal algumas pessoas escreveram água com acento e outras escreveram água sem acento então para eu unificar e ter uma feature representando correta uma coisa só eu retiro os acentos aqui remove espaços internos e esse aqui remove os espaços da ponta pronto é uma função simples e a gente esse retirar os assentos ele troca ou porlo craseado pelo a ou simplesmente tira ele é troca troca o a caseado pelo a Uhum aí eu mantenho a água sem acento TR entendeu porque aí eu não tenho senão é que nós não a gente vai entrar depois na matriz eu ia falar a matriz que f de f Mas pensa se eu tenho duas formas que a palavra água está escrito elas vão ser

- *Corpus ID:* 3940
- *Score:* 0.8782733678817749
- *URL:* oculto
- *Início:* 00:24:46
- *Fim:* 00:27:03
- *Transcrição:* mas isso na maneira mais clássica eu preciso eh fazer a identificação de de de de entidades né de palavras né daquilo que vai ser a granularidade né que vai ser processada e usada nas nas colunas não é quais são as dimensões e normalmente como eu disse as dimensões são são as palavras ou no caso de edens vão ser as os edens que foram criados mas no caso de palavras então eu tenho que identificar essas palavras nos textos né Então existe um processo aí de tokenização né que é a identificação dos tokens que são as as palavras ou os termos e alguns desses termos eles eles aparecem de maneira muito frequente não é no vocabulário normalmente artigos preposições eles são são chamados de palavras vazias porque eles não dão sentido sintático né Eh na verdade semântico sintático até até eles não E então tem dois motivos Eles não têm às vezes um sentido específico semântico né então não é não é um substantivo não é não é um um objeto algo que possa ou um verba né que que indique o ação tá sendo realizada ele serve para fazer a ligação entre essas coisas então por isso a semântica sintática existe significado sintático eles têm mas não a semântica então quando a gente mostra o resultado eles eles não não de maneira isolada não vão dar um não vou entender o que que que que significa aquele texto com base nele somente né então eles muitas vezes são removidos removidos a gente chama eles de Stop Words são removidos do do vocabulário eh tem uma grande discussão aí depende muito do nosso poder de processamento né Eh removê-los é interessante porque são dimensões que eu vou tirar do conjunto de dados Eles não têm um poder discrimite muito alto né já que eles aparecem em grande parte dos dos dos documentos mas mas para fazer alguma análise mais mais sólida né um PLN mais pesado para criar embeds por exemplo ou

- *Corpus ID:* 8533
- *Score:* 0.8773092031478882
- *URL:* oculto
- *Início:* 00:18:32
- *Fim:* 00:20:48
- *Transcrição:* feito então esse é importante entender que esse pré-processamento ocorre em dois momentos nesse início O que nós vamos fazer E aí no no exemplo a gente vai explorar isso a gente vai remover links e emojis por exemplo eu acho que nem emojis eu removo ali mas são coisas que elas não elas não estariam naturalmente num texto tá coisas como links assim eu diria de maneira geral e e e eu vou explicar já por quê Porque nós vamos usar o modelo para gerar vetores então eu não quero entregar para esse modelo para ele codificar o Mind beding para mim Eh usando retirando o stopwords que é uma coisa que ele conhece ele foi treinado com aquilo então a primeira fase é só remover links mesmo e talvez se o texto não tem né eu diria só isso mesmo E emojis também porque emojis a gente normalmente isso não é representativo no treinamento e não ele não ele não representa uma sequência que vai ser quebrada né se eu remover o Emoji por exemplo eu tenho uma frase Stop Word são coisas que quebram aquela sequência né Elas ligam as palavras de uma frase então eu não posso remover mas na etapa final que a gente vai chegar aqui na representação dos Tópicos Aí sim aí a a gente vai remover várias coisas vai usar enig gramas vai voltar pro bag of Worlds vocês vão dizer mas como se eu tô usando em bags a gente vai ver que sim isso tem muito valor nessa fase tá para conseguir uma representação adequada respondi a tua pergunta respondeu e e já gerou uma preocupação né porque nós nós nos adiantamos no trabalho na questão de uma codificação de embed de de eding contextual Ah e a gente fez né com a a tal remoção das stopwords e Bom D uma preguiça tremenda porque Rapaz aquele treinamento ali ele é demorado né É depende é vamos ver vamos ver o que que vocês fizeram quando eu eu for executando o código Talvez tu pode comentar a gente vai tirando as dúvidas


**Resposta gerada pelo LLM**: Stop words são palavras vazias, como artigos e preposições, que aparecem com muita frequência no vocabulário e não contribuem para o significado semântico ou sintático de um texto.  São removidas para melhorar a eficiência do processamento, pois diminuem o tamanho do texto a ser trabalhado, e porque não possuem poder discriminante, aparecendo em grande parte dos documentos e não ajudando a distinguir um assunto do outro.  Em alguns casos, no entanto, mantê-las é importante para a compreensão da sintaxe e para tarefas como geração de texto.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately define stop words and explain their removal during text processing. Supporting Evidence: The RESPONSE accurately defines stop words as "palavras vazias" and explains their frequent occurrence and lack of semantic contribution, which directly addresses the first part of the PROMPT. It also provides a clear rationale for their removal during text processing, discussing efficiency and the lack of discriminative power, which corresponds to the second part of the PROMPT. Additionally, it acknowledges exceptions where stop words may be important, adding depth to the explanation. Therefore, the RESPONSE is relevant to all aspects of the PROMPT.  Score: 3


---
