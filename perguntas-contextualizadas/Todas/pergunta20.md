**Pergunta 20**: Qual é a definição de mineração de dados proposta por Agrawal, e quais são os elementos-chave dessa definição? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 796
- *Score:* 0.8639253973960876
- *URL:* oculto
- *Início:* 00:53:59
- *Fim:* 00:56:22
- *Transcrição:* bom então alguma dúvida até aqui vamos falar de descoberta de conhecimento em banco de dados o termo foi cunhado em 2.993 por um cara chamado agraval foi o cara que inventou as associações a gravar Ganhou muitos prêmios ele trabalhou na IBM não sei se quer se tá vivo tá é mas é um pesquisador Fantástico ele inventou as associações que são aquelas regras que eu mostrei ali da cerveja e da das Fraldas Então esse termo foi cunhado em 1993 pelo cara que escreveu esse artigo E aí depois em 1996 veio outros que o samba e o Smith e eles disseram é o seguinte gente só a técnica em si para estranhos não é nada eu preciso de um processo eu preciso saber que tipo de pergunta que eu quero responder eu preciso achar os dados esses dados não foram propostos para esse fim eu preciso transformar esses dados aí eu aplico a mineração tem que validar esses padrões e aí tudo dá errado tem que voltar 300 Passos atrás e vai para frente vai para trás para frente para trás até que finalmente eu consigo extrair conhecimento aconteceu em 1996 E aí tudo isso tem uma ligação uma origem muito grande com dados e bancos de dados né porque boa parte do trabalho dentro desse processo e ainda continua assim e hoje a gente dá um nome bem bacana se chama engenheiro de dados mas é um cara que vai lá acha os dados transforma os dados faz a sanidade dos dados é transforma porque senão não tem mineração de dados bom e aí a gente começou a usar outras técnicas de mineração que foram pensadas para ir e dentro delas a gente tem o aprendizado de máquina e aí eu vou fazer um discler esse desenho é meu tá os desenhos é só para mostrar as relações das áreas que estão colocadas aqui não tem nenhuma ideia de proporção o aprendizado de máquina Ele nasceu para um outro propósito Ele nasceu para que a máquina aprendesse

- *Corpus ID:* 840
- *Score:* 0.8624480962753296
- *URL:* oculto
- *Início:* 00:39:15
- *Fim:* 00:41:42
- *Transcrição:* E aí ele ainda diz que clientes que compraram essas coisas também compraram esse outro monte de coisa aqui então além de saber na mesma transação quem comprou o quê né Ele disse que essas pessoas que costumam comprar isso também compram aquilo pois não a visualizar obrigada e vamos pular então para Associação já citei para vocês vou gravar online 1993 se criou esse paper seminal sobre essa área foi a primeira coisa com o nome de mineração de dados então ele é um monte de teoria Tá mas é tudo muito simples Então vamos imaginar que eu tenho um conjunto de coisas chamadas itens que esperar eles são literais ou símbolos e eu tenho um item 7 tá Um item 7 é um conjunto de coisas que pertencem esse símbolos e aí eu tenho uma transação que de novo né é um conjunto de coisas que pertencem esse conjunto de símbolos E aí eu posso dizer nessa transação aqui nessa transação aqui eu tenho esse X né se o x meu conjunto de símbolos está contido é igual ao conjunto de símbolos total da minha transação aí ele pega uma base de dados e diz assim a base de dados é um monte de transação que eu tenho colocado lá e o que eu quero derivar dessa minha base de dados são regras do tipo implicação dado x que é 1 + itens e y que não precisa ser mais tipicamente é um item só eu procuro coisas do tipo se x então Y x e y diferente para não dizer se eu comprei baa então eu comprei baa tá aqui se eu comprei baa então eu comprei baa então ele quer que é intersecção desses caras aqui seja vazio e ele quer que isso aqui seja pertencente esse conjunto de itens que eu tô trabalhando e essas regras Elas têm que ter dois parâmetros duas métricas uma se chama confiança e outra se chama suporte

- *Corpus ID:* 923
- *Score:* 0.85956871509552
- *URL:* oculto
- *Início:* 00:02:08
- *Fim:* 00:04:23
- *Transcrição:* Às vezes a gente esquece da história mas tem que se lembrar a primeira técnica com o nome de mineração foi a de associação proposta pela gravar em 1993 tem que se lembrar que as chamadas técnicas de aprendizado de máquina naquele momento elas não eram voltadas para pegar grandes volumes de idade dele vai padrões elas eram voltadas a gente aprender conceitos então elas eram usadas muito em Sistemas ditos inteligentes sistemas especialistas sistemas tutores sistemas voltados à educação por exemplo então elas não eram voltadas a tratar grandes volumes de dados Então as pessoas naquele momento né No início da década de 90 elas passaram a se interessar né foi a área que mais bombou naquele momento é por conta dessa dessa técnica de associação Só que aí o pessoal se deu conta que que a etapa de mineração as regras de associação era um algoritmozinho que a gente rodava sair um monte de coisa e a gente não sabia o que que fazia sentido vocês viram por um exemplo muito bobo né de quatro atributos apenas né Quanto tempo vocês passaram duas três horas ali em cima daquele estudo os dados já estavam preparados e na aula passada a gente falou inclusive né que no mundo real os dados não estão preparados E aí a gente não estudo de caso da semana passada da aula passada a gente comentou a primeira coisa que vocês tinham que garantir Inclusive era saber se essa população efetivamente ela era representativa de quem tava naquele naquele navio então você começou uma discussão de que esse processo de Descoberto ele de novo presente pois é vocês têm um problema com chapeiro né mas tem muito Shakira deve ser chapiro do teste estatística é uma família muito grande [Risadas] e pelo jeito é casado com a piete que aqui né esse Gregory aqui bom mas vamos

- *Corpus ID:* 845
- *Score:* 0.8570327758789062
- *URL:* oculto
- *Início:* 00:48:52
- *Fim:* 00:51:25
- *Transcrição:* sobre o total da base de dados que são 10 eu encontro dois registros que tem os três itens então um suporte de camisa gravata então calça camisa calça e tão gravada gravata então camisa sempre será o mesmo porque são esses três valores sobre o total da base de dados mas a confiança vai ser diferente sempre vai ser o lado esquerdo da regra e sobre o total que apresenta os três itens o lado esquerdo da regra aqui é a camisa gravata as transações pintadas aqui de Bege são as que tem lado esquerdo da regra e a parte de cima são as que tem o lado esquerdo e direito da regra 50% tá então dado o conjunto de transações teu objetivo das regras de mineração de associação e encontrar todas as regras associativas com suporte maior do que um mínimo que é dado como parâmetro e uma confiança que é mínima Dada como parâmetro é dado o suporte representa o quantitativamente com frequente isso aí na base de dados nos dados como um todo e a confiança Vai representar uma confiabilidade uma frequência nessa implicação Então essa regra social ativa né é uma implicação da forma x que pode ser uma mais itens determina a y pode também ser uma mais itens mas tipicamente é um só para facilitar a leitura da regra onde x e y são conjuntos de itens e x intersecção com item Aqui tem uma deturbação no meu no meu formato é conjunto vazio tá se vocês quiserem ter a definição desse algoritmo que foi definido então pelo agraval porque isso na verdade é uma combinação eu começo a fazer essa combinação aqui será que camisa e calça É frequente Será que camisa e gravata é frequentes Será que calça camisa e gravata frequente né e se eu tiver lá 10 itens eu começo a fazer todas essas combinações possíveis o que que o agravar fez Ele propôs um algoritmo que

- *Corpus ID:* 928
- *Score:* 0.8551732301712036
- *URL:* oculto
- *Início:* 00:10:30
- *Fim:* 00:13:00
- *Transcrição:* descobrir conhecimento alguma dúvida até aqui beleza bom em vez de discutir esse processo do fayad foram eles os primeiros que disseram em 96 Olha a gente tem que separar essa etapa Na verdade é um processo aí cada um começou a fazer esse processo de acordo com o background que a gente tinha do mesmo jeito que lá a gente fazia quando a gente não tinha metros a gente fazia sofre tudo do jeito que a gente achava tinha que fazer um pouco num processo meio artesanal e isso depende muito da do Belgrano que você tinha então por exemplo meu bem grande né é eu trabalho muito até a década de 90 meu chão era banco de dados então eu trabalho muito com a parte de dados propriamente dito e o jeito que eu trabalhava é foco nos dados foco nos dados de usar as técnicas clássicas de mineração de dados você vê um foco de estatística a própria o jeito que você representa os dados acaba influenciando o processo você em analisar se você vende a então era muito influenciado pela begram que você tinha e as suas experiências passadas então um grupo formado de uma série de empresas e também naquele cenário da década de 90 havia uma série de empresas criando as mais variadas ferramentas de apoio a análise de dados que eram desistensões a ferramentas de banco de dados por exemplo o agraval ele vem do universo da IBM então a IBM com seu devedores começou a incorporar uma série de algoritmos tradicionais para fazer associação classificação com a incorporação dos algoritmos 90 eu acredito porque né obviamente os bancos de dados vem lá da década de 80 não os bancos de dados os primeiros produtos comerciais relacionais que já existe um antes os produtos comerciais relacionais eles são da década de 80 inclusive os primeiros sequer tinha um Chaves estrangeiro só tinha um Chaves primário aí eles foram

- *Corpus ID:* 766
- *Score:* 0.8547974824905396
- *URL:* oculto
- *Início:* 00:00:03
- *Fim:* 00:02:49
- *Transcrição:* Deixa eu só começar a gravação aqui tá gravando deixa encerrar o transcript Então tá estão todos me vendo todos me ouvindo e estão estão todos vendo uma transparência c02 mineração de dados então tá então estamos gravando estamos à disposição Bom dia a todos eu vou ser então professora de vocês nas próximas cinco semanas nós temos alguns feriados aí na nas próximas semanas então a gente Tomou essa decisão de estender essa disciplina um pouco para conversar com mais calma né não prejudicar ninguém então seria a professora de vocês acompanhando vocês na segunda disciplina do curso que se chama então mineração de dados é bom se vocês quiserem dar outros nomes eu sinceramente o curso que eu prefiro né é descoberta de conhecimento em dados ou em grandes volumes de dados é dessa forma que essa área foi definida é verdade né no século passado 1990 e 3 descoberta de conhecimento na época todo o conhecimento era estruturado em bases de dados então a área se chamava descoberta de conhecimento em bases de dados ou do inglês não é mais o caso Porque grande parte do conhecimento não está mais embaixo das estruturas e aí então lá pela pelo início desse século pessoal achou mais chique chamar de deitar de Business analíticos e também nesse século Né desde que disseram que era a profissão mais sexy do mundo a gente está se chamando de cientistas de dados então todos esses nomes aí que vocês quiserem vocês podem usar aqui para nossa disciplina todos eles são válidos né eles traduzem algum tipo de propósito na nossa disciplina só não pode essa disciplina de machinlane tá nem imagido nem aprendizado de supervisionado aprendizado não supervisionado nem de ir

- *Corpus ID:* 3581
- *Score:* 0.8546222448348999
- *URL:* oculto
- *Início:* 01:07:49
- *Fim:* 01:10:13
- *Transcrição:* genérico né você já devem ter visto várias propostas de modelos ou processos de mineração de dados ou de aprendizado de máquina para área mais acadêmica a gente segue lá o fayad de 80 e poucos que foi um dos que primeiro estudou mineração de dados e definiu lá um modelo meio que em Cascata e aqui mais ou menos tá nessa nessa linha Mas vocês têm outras opções tem tem modelos mais mais estamos assim que a indústria adotou né Tem um modelo Cris é mais um modelo de mineração de dados mas que serve para aprendizado de máquina também mas enfim vocês A ideia é que vocês têm etapas e serem essas etapas essas etapas normalmente possuem alguma alguma forma de eu voltar atrás e fazer uma adaptação mais importante é que eu consiga reproduzir e registrar exatamente aquilo que foi feito para que eu possa reproduzir e compreender assim o que eu fiz foi correto para que eu possa fazer de novo com outros dados ou até corrigir alguma coisa mas em termos vai gerar então eu coloquei ali uma etapa de preparação onde eu vou selecionar dados Eu Vou extrair os dados de alguma de alguma base eu posso precisar fazer algum ajuste padronização colocar todos no mesmo mesmo formato no mesmo espaço Dimensional com os mesmos faixas de valores enfim posso excluir propriedade posso fazer ajustes na troca de propriedades enfim aí tem todas essas áreas de fitilho selection featu indianismo a gente vai ver alguma coisa disso depois Como eu disse para vocês para que eu possa separar os objetos em grupos eu preciso estipular não é avaliar alguma alguma de alguma maneira a distância entre eles o quão próximos ou distantes eles estão E então Existem várias equações que eu posso usar e nós vamos usar algumas mais comuns mas a gente vai

- *Corpus ID:* 4042
- *Score:* 0.8546161651611328
- *URL:* oculto
- *Início:* 00:27:21
- *Fim:* 00:29:41
- *Transcrição:* E aí essa esse algoritmo ele vai então fazer esse ajuste dessas componentes que são linhas que separam o espaço colocando essas linhas nas melhores posições possível para manter a representação dos dados originais e aí eu uso esses componentes no restante do processamento seja a mineração de dados ou visualização e aí como eles são uma quantidade menor eu vou processar em menos tempo bom e eu tenho eu tenho um exemplo aqui de como é que nós podemos poderíamos fazer isso no corrente dados que nós já usamos né então se você lembrar lá das Flores das ilhas nós vamos carregar aquele conjunto de dados E aí vamos aplicar Então essa técnica para Que ela possa nos dizer ao invés de usar as quatro dimensões originais quantos componentes principais eu teria que ter né para poder representar o mesmo conjunto de dados né bom primeira coisa carregar Os dados aqui e algo importante que que nós devemos fazer é escalar os dados né porque nós componentes principais já que eles são separadores lineares funcione da maneira mais adequada eu preciso colocar todas as dimensões no mesmo padrão né sinal de que ele pode ter problemas na hora de identificar essa essa e não representar adequadamente a variabilidade dos dados já que eles têm dimensões de tamanho diferente e depois não tem muito muito mistério basta eu executar não é o modelo distanciar o modelo E aí tem alguns parâmetros o principal parâmetro aqui é a quantidade de componentes vocês vão ver depois que tem outros outros parâmetros se vocês pegarem aqui pegar o decomposition sempre vão dar uma olhada no

- *Corpus ID:* 3751
- *Score:* 0.8532527685165405
- *URL:* oculto
- *Início:* 00:01:52
- *Fim:* 00:04:03
- *Transcrição:* decisão quando ele consegue então esses elementos eventualmente eles precisam ser analisados com cuidado né todo o objetivo da análise não se previa nada é criar modelos que representem a distribuição natural desses objetos no espaço de múltiplas dimensões as dimensões são os atributos né que vocês escolheram para fazer a mineração análise e e a ideia explorar para verificar como são esses grupos nesses conjuntos e se há alguma alguma categorização possível né categorias que possam ser criadas A partir dessa análise ou simplesmente Quais são os Qual é o perfil de cada de cada cluster né tipo de atributo que é mais comum dentro dele para eventualmente então definir uma classe uma um perfil representativo né então esse objetivo segmentar criar perfis é o que mais importa eventualmente Mas e quando vocês estão fazendo análise tomem Cuidado então nesses elementos que estão mais para as bordas porque eles não necessariamente podem estar no melhor grupo né pode ser importante pode ser conveniente vocês eventualmente fazerem algum ajuste Até manual porque não tem toda essa liberdade em conjunto com aí com algum especialista de domínio que vai analisar esse esses elementos E aí ele se ajuste eventual nós vimos também que obviamente existem outros outras configurações de dados não é que eu vou trazer outras hoje na última encontro eu já tinha deixado um uma outra função que gera dados que que tem um outro tipo de perfil né que são aquelas meias meias duas não é é a meia lua então para o algoritmo com a média se ela ela não é bem identificada Ele vai tentar criar grupos que tem aquele perfil de um centro elementos ao redor então ele vai ocasionamente ele vai misturar elementos de uma meia lua com outra então essa situação para este tipo

- *Corpus ID:* 4196
- *Score:* 0.853095531463623
- *URL:* oculto
- *Início:* 00:45:29
- *Fim:* 00:47:45
- *Transcrição:* pessoal não não não conhece programação eles são interessantes por isso para entender o processo de mineração poder fazer alguns experimentos e e já ter alguns resultados né enfim a ideia era dar uma um Panorama para vocês e e agora usar esse resto do tempo que nós temos para fazer os os exercícios né na verdade tem um exercício E aí a gente termina a disciplina por aqui então Eh espero que vocês tenham apreciado né a ideia for dar foi dar esse Panorama geral a gente focou bastante em clusterização que é a tarefa mais comum de aprendizado não supervisionado vimos uma gama grande de algoritmos eh Não vimos todos mas as principais famílias nós vimos em especial vocês têm que lembrar disso que que que a clusterização é exploratória então não não existe um um resultado que possa ser eh usado diretamente né como vocês fariam numa numa numa classificação ou coisa assim vocês têm que explorar avaliar diferentes opções tem muitos parâmetros mas seguindo esse pipeline essas dicas que eu dei vocês vão conseguir fazer uma análise exploratória correta e E aí terem indícios né de clusters e com as técnicas que nós vimos hoje entender Quais são os perfis desses desses clusters para então se for o caso vocês definirem categorias né junto com algum especialista de domínio para depois criarem um modelo classificador que consiga eh D dos novos registros classificar os elementos nesses nesses essas categorias que vocês definiram é uma análise mais de auxílio na criação né de de modelos do que o uso definitivo de um modelo que foi usado né foi criado pelo clusterizados do que para depois né mas enfim é uma etapa importante de vocês terem para fazerem


**Resposta gerada pelo LLM**: Em 1993, Agrawal cunhou o termo "descoberta de conhecimento em bancos de dados", que posteriormente evoluiu para "mineração de dados".  A definição de Agrawal se baseia em regras de associação, como as regras "se cerveja, então fraldas", que identificam relações frequentes em conjuntos de dados transacionais.  Elementos-chave incluem a identificação de padrões em grandes volumes de dados e a extração de conhecimento a partir desses padrões.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT, providing a complete definition of data mining as proposed by Agrawal and identifying the key elements of that definition. Supporting Evidence: The RESPONSE accurately references Agrawal's contribution to the term "data mining" and provides a definition that includes the concept of "knowledge discovery in databases." It also mentions key elements such as the identification of patterns in large datasets and the extraction of knowledge from these patterns. This directly addresses both parts of the PROMPT, making it highly relevant.  Score: 3


---
