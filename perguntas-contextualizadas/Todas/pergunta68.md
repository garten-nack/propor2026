**Pergunta 68**: Em que situações é mais vantajoso fornecer recompensas intermediárias durante o treinamento de um agente, em vez de apenas uma recompensa final? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 4247
- *Score:* 0.8837878108024597
- *URL:* oculto
- *Início:* 00:41:45
- *Fim:* 00:44:00
- *Transcrição:* tenham isso em mente né uma ação no presente pode afetar resultados eem um futuro distante e do ponto de vista do ambiente pode ser que eu avalie só esse futuro distante aqui né para o agente aprender a a se comportar tem temos uma dúvida pode falar oi oi Anderson Bom dia é assim eh eh Nessa situação você tem eh uma recompensa somente no final e ele tem um tempo por exemplo para para fazer milhões ou como você falou bilhões de testes Mas dependendo da situação eh será que pode ser que você vá dando Recompensas intermediárias porque porque ele pode avançar mais rápido na verdade tô falando isso mas é uma pergunta eh dependendo da situação pode ser que eu tenha que eh eh eh projetar algum tipo de recompensa para que ele Aprenda Mais rápido sem ter que ele aprender todos os passos possíveis e só no final e receber observação perfeita né o o a palavra-chave ali de poder computacional eh tem tudo a ver com com com com essa pergunta com esse comentário até né porque tá certo né não é é mais que uma pergunta é um comentário correto então a decisão né de de como que eu vou projetar né A recompensa aqui no ambiente tem que pensar se eu tenho poder computacional para treinar né o o algoritmo ali o algoritmo tá pronto né mas ele precisa interagir com o ambiente para para saber né para se comportar bem então eu eu tenho recursos computacionais suficientes para treinar esse esse a gente se eu tiver então eu posso né dar a recompensa digamos definitiva né da desse que é a tarefa mesmo no caso aqui do jogo eu só preciso dizer se o agente ganhou ou não mas se eu não tiver aí eu vou precisar de um pouquinho mais de cuidado para projetar recompensa para realmente né guiar o agente de forma que ele tenha né pontos aqui mais eh constantes de avaliação Para que ele não precise jogar toda uma partida né E só só no final lá que ele vai saber se aquela eh sequência de

- *Corpus ID:* 4248
- *Score:* 0.881860077381134
- *URL:* oculto
- *Início:* 00:43:28
- *Fim:* 00:45:37
- *Transcrição:* posso né dar a recompensa digamos definitiva né da desse que é a tarefa mesmo no caso aqui do jogo eu só preciso dizer se o agente ganhou ou não mas se eu não tiver aí eu vou precisar de um pouquinho mais de cuidado para projetar recompensa para realmente né guiar o agente de forma que ele tenha né pontos aqui mais eh constantes de avaliação Para que ele não precise jogar toda uma partida né E só só no final lá que ele vai saber se aquela eh sequência de ações foi boa ou ruim se eu tiver pontes mais frequentes o agente vai pode fica mais fácil para ele saber qual foi o ponto que ele começou a derrapar né ou ou se se ele tava indo bem até aqui até esse primeiro ponto e aí depois no segundo ponto Foi mal então alguma coisa aqui no meio né foi ruim ou seja é mais fácil isso do que se eu dou uma recompensa só ao final que aí o o ag gente tem que descobrir nesse meio todo aqui né onde é que ele errou né então Eh se eu se eu não tiver poder computacional então tenho que gastar mais tempo né projetando uma função de recompensa que dê uma avaliação mais frequente pro agente então comentário tá perfeito pertinente aqui OK pode falar bom dia bom dia bom dia não porque eu tava pensando também que depende da complexidade da tarefa talvez também né sim porque o conceito de que ele acertou também igual você falou né É melhor avaliar Talvez no final né dependendo da complexidade porque tem ele pode fazer de várias formas né acertar de várias formas mas tem aquela forma mais a melhor né que é o caso da pontuação né que você colocou lá né Eu acho que vale a pena avaliar a o fator da poder computacional e acho que é a complexidade da tarefa também né Eu acho que é uma coisa é porque o cuidado que eu falei aqui né quando a gente começa a dar uma avaliação mais frequente pro a gente a gente nós ao projetar a recompensa nós vamos ter que saber né O

- *Corpus ID:* 4246
- *Score:* 0.8669331669807434
- *URL:* oculto
- *Início:* 00:40:05
- *Fim:* 00:42:24
- *Transcrição:* né o o o agente a ação final digamos assim né que que vale a pena outro exemplo tá eh aqueles sistemas lá que jogam jogos de tabuleiro né ou xadrez ou go enfim tem um tabuleiro lá com as pecinhas com as pecinhas lá e aí o a jogada é colocar uma peça né então eu não preciso dizer se a jogada foi boa ou ruim na hora que o que o que o computador fez eu nem eu sei se a jogada é boa ou ruim mas eu sei que depois de uma sequência de jogadas se ele ganhou ou se ele perdeu então eu posso dar recompensa só aqui no final e é isso que foi feito mesmo aqueles sistemas igual o alfa go lá eles só recebia recompensa ao final da partida Então pensa um jogo como go tem sei lá 50 movimentos até acabar ou mais né pode ser centenas de movimentos E aí o computador sem saber nada né sem receber nenhum avaliação do que que ele tá fazendo aqui no meio recebe só o final da partida ó ganhou ou então Ó perdeu aí ele ao jogar milhões de partidas bilhões Talvez ele consegue descobrir sequências de ações que fazem ele ganhar né então não o projeto né Na hora que eu tiver pensando na recompensa eu posso e simplificar do ponto de vista do ambiente né eu não preciso avaliar exatamente cada ação Zinha que o agente faz se eu sei ao final né O que que é bom o que é ruim isso já pode ser suficiente né para a gente aprender se eu tiver o poder computacional suficiente para treinar ele tá mas então tenham isso em mente né uma ação no presente pode afetar resultados eem um futuro distante e do ponto de vista do ambiente pode ser que eu avalie só esse futuro distante aqui né para o agente aprender a a se comportar tem temos uma dúvida pode falar oi oi Anderson Bom dia é assim eh eh Nessa situação você tem eh uma recompensa somente no final e ele tem um tempo por exemplo para para fazer milhões ou como você falou bilhões de

- *Corpus ID:* 4245
- *Score:* 0.8656316995620728
- *URL:* oculto
- *Início:* 00:38:28
- *Fim:* 00:40:40
- *Transcrição:* pessoal que é mais novo já talvez não esteja Ah se bem que tá né fizeram várias versões novas aí aí tem que saltar l o bigodinho dele aqui tem que saltar numa plataforma né e chegar do outro lado aqui e pegar a moeda né tem a moedinha dele aqui e eu não preciso ficar dando eh por exemplo se eles consegu fazer o salto aqui né eu não preciso recompensar imediatamente esse salto nesse ambiente do Mário ele pode simplesmente a recompensa ser a pontuação na tela lá né Tem um score lá de quantas moedas ele pegou né então ele vai ficar andando aqui na aleatório na na no cenário E aí eu não preciso ficar dando recompensa para ele eu posso simplesmente fazer né O Agente eh maximizar justamente essa recompensa aqui que ele recebe de ele precisa fazer muitas ações essa ideia de decisão sequencial né precisa fazer muitas ações para conseguir maximizar essa recompensa conseguir ganhar ponto né então ele pode é percorrer um longo caminho até chegar aqui sem nenhum sinal de que aquilo foi bom ou ruim e quando ele saltar e pegar moedinha Aí sim essa pontuação muda não lembro quantos pontos vale sei lá mais 10 aí que isso né só mesmo assim é suficiente para a gente aprender nesse comportamento de seguir reto e depois pular e pegar a recompensa né Eh Ou seja eu não preciso est constantemente ali né digamos incentivando a ag gente a fazer coisa certa eu posso só eh avaliar quando realmente é importante né o o o agente a ação final digamos assim né que que vale a pena outro exemplo tá eh aqueles sistemas lá que jogam jogos de tabuleiro né ou xadrez ou go enfim tem um tabuleiro lá com as pecinhas com as pecinhas lá e aí o a jogada é colocar uma peça né então eu não preciso dizer se a jogada foi boa ou ruim na hora que o que o que o computador fez eu nem eu sei se a jogada é boa ou ruim mas eu sei que depois de uma sequência de jogadas se

- *Corpus ID:* 4393
- *Score:* 0.8643124103546143
- *URL:* oculto
- *Início:* 00:06:52
- *Fim:* 00:09:10
- *Transcrição:* por fazer aquela ação ação de pra direita nesse estado E verificou né que na verdade o valor dessa ação é um pouquinho maior ele porque ele ganha uma Recompensas minha negativa lá o os 0.04 mas né ele chega num estado que tem né uma perspectiva melhor aqui né de de conseguir um valor melhor E aí por isso o valor aqui sobe né na direção da de atualização correta e é o que a gente viu né quando executa ali o o agente Solto No Grid ali ó por por 10 iterações né ele vai atualizando as a recompensa o que ele acha que vai ganhar né estimativas de valor E aí quando ele cai num estado terminal né tanto O negativo quanto Positivo já fica mais claro para o próprio Agente né quais ações que levam aquela consequência ruim ou aqui em cima né que levam para consequência boa e aí que ele ainda no E olha que como que é isso é legal de ver como que é lento né o aprendizado no que learning porque olha só o aprendizado a partir de onde tem recompensa que aí o ag gente começa a propagar né aqueles valores maiores de recompensa para os outros lugares então o estado terminal né o valor dos Estados terminais demora para ser reconhecido ali no início e isso é importante a gente observar né que é um uma limitação dessa abordagem do que o learning e a gente vai nós vamos né tratar disso também no restante aqui da da aula hã Professor uma perguntinha Não na verdade é um comentário no tem um padrão para esse para para essa formatação de linha e coluna porque no Exercício lá acho que é o o a atividade três eu fui fazer eu errei porque tava 3S do e a gente tá vendo o primeiro como linha e só que lá é o contrário lá o três é a coluna E aí é certo que lá no exercí tá falando né coluna três eu que também não prestei atenção mas atrapalha um pouco assim assim quando o senhor fala que três do aí três é a linha né só que lá no exercício três é a coluna aí atrapalho eu errei um pouquinho as exer

- *Corpus ID:* 4330
- *Score:* 0.8623026013374329
- *URL:* oculto
- *Início:* 00:00:02
- *Fim:* 00:02:32
- *Transcrição:* Então acho que a gravação está iniciada bem-vindas bem-vindos Essa é a segunda aula do curso de aprendizado por reforço eu vou começar aqui né tentando revisar um pouquinho da aula anterior então eu vou compartilhar minha tela aqui vocês me avisem se não aparecer esses slides assim a gente viu um monte de conceitos né dessa área de aprendizado por reforço e um deles Talvez o o o mais importante né o é a ideia de valor né quando a gente tá em algum alguma situação né de determinar o que que é bom de fazer né se a gente tá em uma situação que a gente chama de estado né isso revisando o processo de decisão de marcov processo de decisão de marcov que são as regras né do do mundo no qual um agente está situado a ideia toda de aprendizado com reforço é isso né Tem um agente em um ambiente e a gente quer né descobrir como esse agente pode fazer para de forma conseguir o máximo de Recompensas possíveis E para isso ele tem que ser capaz de diferenciar né uma situação que é tem uma recompensa imediata talvez melhor do que outra igual eu tô desenhando aqui né mais duas situações duas ações que ele possa fazer n mas que a longo prazo né possa ser prejudicial então o nosso a ideia de de diferenciar né O que que é bom a curto prazo e o que que é bom a longo prazo e aí que entra né o conceito de recompensa uma coisa que você ganha imediatamente por fazer alguma ação e o valor né que é o somatório o total de Recompensas n que você ganha ao longo da da sua trajetória ao longo da sua vida e nós queremos né agir de forma a maximizar o valor né o total de Recompensas a ganhar lembrando né que tem um desconto Zinho né então Recompensas muito longe no futuro tem um desconto Zinho aqui de eh cada cada passo que a gente dá né Tem um um desconto aqui do um fator de desconto Gama hã E aí o um jeito né de representar

- *Corpus ID:* 4279
- *Score:* 0.8608322739601135
- *URL:* oculto
- *Início:* 00:01:49
- *Fim:* 00:04:14
- *Transcrição:* realmente não tem sentido desenhar nenhuma setinha para onde a gente iria querer ir porque aqui termina a interação dele com o ambiente e aí agora Existem várias maneiras né do agente se comportar no ambiente essa é uma delas outra por exemplo né desenhar de vermelho aqui seria o a gente agir né mapear a cada estado para essa setas em vermelho aqui né E talvez essa outra maneira seja faça menos sentido ou seja pior Enfim então tem várias maneiras e algumas maneiras são ótimas né no sentido de que elas conseguem fazer o agente a gente consegue fazer a coisa certa com elas e o fazer a coisa certa é conseguir a maior né o maior somatório maior sequência de Recompensas possível e só que não é só uma sequência de Recompensas tá a gente vai tratar isso né uma pensar aqui ó em aplicar um raciocínio em cima dessas Recompensas contabilizar algumas coisas interessantes aqui por exemplo né uma das coisas que a gente queira contar né contemplar aqui eu cheguei a mencionar isso brevemente Mas vamos supor um estado aqui né que o a gente tenha duas ações para tomar é uma ação a um aqui uma ação A2 logo que ele toma essa ação A1 ele chega no estado que ele perde né Tem uma recompensa de menos um e aqui tem uma recompensa de mais E aí depois né tem só uma sequência de ações aqui para fazer que no final das contas né vai ter uma soma de Recompensas aqui que no total vai dar mais 100 Total aqui e aqui vai dar menos 200 né as outras Recompensas que ele for tomar aqui então uma das coisas que a gente tem que contemplar né para pensar na política ótima por exemplo nesse estado aqui que eu vou chamar desse zero ação A2 ela dá a maior recompensa imediata então se eu fosse olhar só a recompensa que eu ganho

- *Corpus ID:* 4442
- *Score:* 0.8606429100036621
- *URL:* oculto
- *Início:* 01:30:09
- *Fim:* 01:32:25
- *Transcrição:* e disse né que o agente tinha que fazer isso né Ele simplesmente repetidamente lá a ao interagir com ambiente né múltiplas vezes ele aprendeu né que isso é é o melhor jeito né de de jogar de acumular Recompensas rapidamente né então ainda não é tão impressiote quanto aqueles os bonequinhos lá construindo né os o o pique esconde lá mas né mostrou o poderio de aprendizado por reforço né com aproximação de funções essa rede aqui né dqn conseguiu aquele feito em outros jogos também tá outros jogos teve desempenho sobrehumano mas esse é igualmente impressiote professor que ele consegue ter uma resposta rapidíssima e a bola vai numa velocidade e ele que ele consegue responder né isso muito acima do ser humano tá muito acima do ser humano isso aí né e é talvez o especialista no jogo ali não seja batido né mas na média tá muito superior Com certeza e bom aqui um alguns dos truques entre aspas né na de implementação ali que hã É algumas coisas dessas são comuns né a outros algoritmos de aprendizado por reforço também por exemplo né Toda vez que o agente faz uma ação em algum estado tem uma recompensa e atinge um outro estado ele armazena essas informações aqui num replay de experiências né para cada ação que ele tentou então ele tem um replay lá sei lá de 10.000 posições Então as 10.000 experiências mais recentes ficam lá nesse replay e aí na hora de atualizar a rede na hora de atualizar a rede ele coleta aquelas experiências lá então ele não precisa né interagir com o ambiente toda vez para atualizar a rede ele pode usar experiências que ele já viveu anteriormente para atualizar a rede de novo ele ter um ganho de tempo nisso aí né ele não precisa passar revisitar o mesmo estado duas vezes né ele pode simplesmente resgatar lá do replay de experiências Então isso é uma das ideias tá Outra ideia esse o target aqui versus comportamento

- *Corpus ID:* 4249
- *Score:* 0.858902096748352
- *URL:* oculto
- *Início:* 00:45:06
- *Fim:* 00:47:19
- *Transcrição:* de várias formas né acertar de várias formas mas tem aquela forma mais a melhor né que é o caso da pontuação né que você colocou lá né Eu acho que vale a pena avaliar a o fator da poder computacional e acho que é a complexidade da tarefa também né Eu acho que é uma coisa é porque o cuidado que eu falei aqui né quando a gente começa a dar uma avaliação mais frequente pro a gente a gente nós ao projetar a recompensa nós vamos ter que saber né O que que é bom o que que é ruim ao invés de deixar o a gente descobrir né Nós vamos ter que ter o trabalho ou sermos especialistas no domínio né para informar o agente né se esses se o que ele tá fazendo ali no meio do caminho tá bom ou ruim e isso pode enzar né E talvez impedir ele de aprender umas formas inusitadas e talvez mais eficientes né de resolver a tarefa mas a gente ganha né ao a gente ter um Sinal mais frequente ele vai ter um treino mais rápido e é bem bem nessa linha Jean e quando não tem um resultado bom e nem ruim tipo o empate jogo da velha tá ou o xadrez o cara se declarar declarar empate no jogo como é que vou colocar aqui mais ou menos h a questão do de vitória né empate ou derrota eh só a questão de empatar não é ruim tá o o o sinal aqui de de de avaliação né ao final ele na verdade eu eu nem chamo né de Vitória empate e derrota do ponto de vista do computador tá porque o que eu tenho é números aqui então a única coisa que eu preciso é dar sinais de forma que eu diferencie uma situação de outra né então então o computador não na verdade ele não sabe a semântica dele ter empatado ele sabe aqui ó que ele recebe ao final dessa partida que empatou ele recebe Zero isso O computador entende número O computador entende bem aqui ele recebe menos um menos um perdeu e aqui ele recebe mais um ganhou E aí então o que que vai acontecer né ele vai jogar várias

- *Corpus ID:* 4484
- *Score:* 0.8582124710083008
- *URL:* oculto
- *Início:* 01:00:16
- *Fim:* 01:02:35
- *Transcrição:* né aqui mesmo que no caso anterior né O Agente tá errado aqui né Ele vai morrer de ir para baixo mas ele tá decidido né então ele tem Grande Chance mesmo de ir paraa ação que ele acha que é melhor já que o ag gente tá muito indeciso e se a gente fosse fazer Epson guloso em cima disso né o ag gente ia tomar uma decisão muito drástica de 90% das vezes para baixo né então a o fato de haver o soft Marx ajuda a modelar talvez de uma maneira bem direta né A questão de decisão versus indecisão é uma vantagem né do soft Marx mas né quando o ag gente tá Decididamente errado pode ser uma desvantagem Mas enfim foi para ilustrar aqui o caso né de de uma os potenciais nas preferências estarem mais equilibradas e isso gente de só só para completar eu já abro para PR pra dúvida no início do treinamento do agente é legal que isso ocorra aqui ó a gente digamos está indeciso Para justamente né ele ter boa chance de escolher todas as ações né ao invés de de um é ali de 90% das Às vez ele escolher só uma delas né então a política soft Marks é interessante Nesse sentido porque ela favorece bastante a exploração no começo do do treino do agente pode pode falar dúvidas eh acho que eu me perdi aqui em algum momento mas é o seguinte nas aulas anteriores a gente havia dito havia dito que se a esperança de uma determinada ação fosse um pouquinho maior que que as outras 100% das vezes ele ia o agente ia Tom aquela ação porque eh a recompensa maior digamos assim né OK e agora a gente tá falando já em probabilidade ou seja dependendo do do do do valor Aí ele vai 6% das vezes tomar aquela ação e não ação que tem um valor maior ele vai todas as vezes né Eh eu não sei onde foi que eu me perdi mas que mudou um pouco a filosofia entendeu isso tá então aqui ó eh antes né do a gente tava aqui no nesses nessa nesse paradigma né Eh de da política C ela tem um éon tá não é que é 100% das vezes % das vezes ele não pega


**Resposta gerada pelo LLM**: É mais vantajoso fornecer recompensas intermediárias quando não há poder computacional suficiente para treinar o algoritmo com apenas uma recompensa final, ou quando a complexidade da tarefa exige uma avaliação mais frequente para guiar o agente e acelerar o aprendizado.  Em situações com poder computacional limitado, recompensas intermediárias permitem que o agente aprenda mais rápido, sem precisar jogar partidas inteiras para receber uma avaliação final.  Isso facilita a identificação de pontos onde o agente começou a errar.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address the advantages of providing intermediate rewards during training rather than just a final reward. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining situations where intermediate rewards are advantageous, such as limited computational power and the complexity of tasks. It discusses how intermediate rewards can facilitate faster learning and help identify errors, which aligns with the inquiry about the benefits of using intermediate rewards in training an agent. The RESPONSE is relevant to the entire PROMPT and provides a complete answer.  Score: 3


---
