**Pergunta 22**: Quais são os dispositivos visíveis para o TensorFlow?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 6844
- *Score:* 0.8849621415138245
- *URL:* oculto
- *Início:* 00:01:50
- *Fim:* 00:03:59
- *Transcrição:* Quais são os devices os Physical devices que estão disponíveis Então nesse caso aqui a gente consegue observar que tem dois é uma lista né onde o primeiro é a CPU e o segundo é a GPU Então se no caso aqui a gente tivesse mais de uma GPU ou mais de uma CPU né a gente teria listado então várias vezes esses eh com com um identificador diferente aqui provavelmente esse número aqui seria diferente se tivesse mais de uma GPU por exemplo Então como que eh ele gerencia isso depois iso é bastante transparente né a gente vai realmente só instanciar os tensores dizer onde é que eles têm que ficar tá depois as operações acontecem de maneira transparente tá a gente tem como também aqui no caso get visible devices pega todos que estão disponíveis ã no caso que a gente tá no ambiente do collab então tem uma CPU e uma GPU mas a gente poderia por exemplo também listar os devices baseado numa string certo Seria tipo uma string de consulta como se ele estivesse fazendo uma filtragem com essa string Então se a gente quiser por exemplo somente os devices que são eh gpus a gente pode passar GPU tudo maiúsculo daí ele procura por GPU tudo maiúsculo no nome e lista somente aquelas que que estão nesse que onde essa essa verificação é é é é verdadeira então Eh nas situações onde a gente quer fazer uma operação onde tem uma implementação tanto em CPU e uma implementação em GPU se ambos os recursos estiverem disponíveis o tensor Flow sempre vai priorizar GPU Ok talvez porque operações de álgebra em geral são muito mais rápidas em GPU tem algumas operações que não estão portadas né a gente viu no exemplo da atividadea anterior o exemplo do cast né então Casting por exemplo só é só ocorre em CPU não tem como fazer mudança de tipo uma vez que os o dado já está na memória da GPU Então nesse caso a gente tem que sempre fazer os casts sabendo que sempre

- *Corpus ID:* 7024
- *Score:* 0.8706517219543457
- *URL:* oculto
- *Início:* 00:18:08
- *Fim:* 00:20:12
- *Transcrição:* basicamente um arquivo csv que descreve cada modelo de alguma forma né E aí vai ter um mecanismo que vai pegar uma dessas dessas configurações e vai pegar Ah tá qual GPU tá livre vou treinar nessa GPU depois ele vai pegar a próxima configuração vai botar na outra GPU e vai fazendo isso enquanto tem GPU disponível e como é que tu força para que eh eh cada script veja só uma GPU daí tem uma variável de ambiente chamada kuda visible devices que ela é respeitada pelo tensor Flow E aí a gente diz no cuda visible devices Qual que é o device ID eh da GPU que a gente quer usar naquele naquela execução de tensor Flow né então com isso daí eles conseguem automatizar esse mecanismo de procura bem manual né a gente vê isso bem manual embora existam mecanismos mas eh eficientes para fazer isso talvez já mais prontos assim né mas essa seria a forma manual de fazer Então essa é uma alternativa né tu tem várias eh gpus tu testa um modelo GPU Ok ah uma outra uma outra possibilidade de paralelização né que essa Inclusive a gente já viu é usar o prefet né então a gente vai ao mesmo tempo que tá treido com uma GPU né a gente pode usar CPU pra gente fazer pré-processamento né então por exemplo aqui nessa nossa nesse nosso grafo de tarefas essas tarefas que aparecem aqui em CPU podem ser de pré-processamento né para isso a gente vai usar o dataset prefet que a gente viu na aula passada e aí a gente põe o paralelismo que a gente precisa ter nesse dataset esse paralelismo vai ser um paralelismo de CPU e a gente usa todas as cpus para preparar os dados para que a GPU possa fazer o treinamento de maneira eficiente então com isso a gente vai ter um paral ismo em CPU e um paralelismo em GPU Ok tem uma outra alternativa também que é a gente usar para aqueles modelos que são de

- *Corpus ID:* 6709
- *Score:* 0.856248140335083
- *URL:* oculto
- *Início:* 00:53:36
- *Fim:* 00:55:41
- *Transcrição:* Google e o tensor Flow é o é a única até onde eu conheço né o único que tem suporte para TPU entendeu então mas assim depois a gente na semana que vem a gente vai fazer uns experimentos comparativos tá e se vocês têm uma GPU tipo assim não sei se vale a pena tipo assim vocês vão ver que o desempenho de Treinamento paraas nossas para para alguns exemplos simples tá eh é bem equivalente o desempenho de treinamento em GPU e TPU né então não sei se vale a pena tu tu se preocupar com esse tipo de coisa tá ter o TPU lá específico para ti bom então indo adiante aqui a gente logo em seguida já vai passar pra parte de de eh do queras mais específico mas só para ter uma ideia geral né então o tensor Flow a gente vai importar como TF e do tensor Flow a gente vai importar o caras então com isso a gente vai estar usando a p keras implementada pelo tensor Flow e lá dentro daí a gente vai ter então a instanciação de um modelo então a gente pode instanciar um modelo sequencial dessa forma onde a gente vai especificar Então as camadas né através dessa uma lista é uma lista é um vetor de camadas então todas do mesmo tipo aqui né Então a primeira é uma é uma quer dizer assim do tipo Python né todas elas são queras layers mas a primeira é uma é uma de flaten para para pegar a entrada e tornar ela plana né porque a gente só tá trabalhando com tensores Então são vetores né embora a gente possa trabalhar com vetores multidimensionais a camada de entrada de uma rede neural é um é um não é um cubo ela é sempre um vetor Ok então essa fleten ela permite a gente pegar o dado seja ele qual for uma matriz uma imagem whatever transformar num num vetor e isso vai ser o estímulo inicial para minha rede neural E aí depois tem camadas aqui tem uma camada densa tem uma camada de drop Out para prevenir overfitting com 20% que ela vai meio que esquecer um pouco perder um

- *Corpus ID:* 6843
- *Score:* 0.8550460934638977
- *URL:* oculto
- *Início:* 00:00:08
- *Fim:* 00:02:25
- *Transcrição:* deixa eu ver se tá funciodo a transcrição tá funciodo a transcrição beleza bom pessoal então H Bom dia novamente sejam bem-vindos então a segunda parte da aula de hoje Onde nós estamos falando então da parte de emprego de gpus com tensor Flow E aí agora a gente vai de imediato então começar Então a nossa AD quatro essa atividade dirigida onde o objetivo a gente vê como é que funciona pra gente empregar GPU para fazer aquelas operações básicas que a gente viu antes da pausa ok operações básicas sobre tensores especialmente eh nós vamos ver então algumas operações elementais de álgebra linear no caso especificamente a multiplicação de matriz tá então para isso a gente vai instanciar essa esse eh esse cab aqui né Eh e esse colab ele já foi configurado então para ser instanciado com gpus Ok com uma GPU no caso a GPU T4 que é muito semelhante àquela exatamente a mesma situação que a gente já abordou na disciplina anterior Ok então H falando sobre uma visão geral sobre gpus no tensor Flow né o flow ele tem capacidade de usar na realidade ã eh e usar de maneira concomitante tanto cpus quanto gpus dependendo né da onde é que estão os dados tipo de coisa a gente tem Como Influenciar onde é que os dados serão posicionados tá então pra gente poder ver quais são os devices visíveis pro tensor Flow então tensor Flow tem um pacote chamado eh um módulo chamado tf.cfg que dentro dele então tem um um método ali que nos permite observar Quais são os devices os Physical devices que estão disponíveis Então nesse caso aqui a gente consegue observar que tem dois é uma lista né onde o primeiro é a CPU e o segundo é a GPU Então se no caso aqui a gente tivesse mais de uma GPU ou mais de uma CPU né a gente teria listado então várias vezes esses eh com com um identificador diferente aqui provavelmente esse número aqui seria diferente se tivesse mais de uma GPU por exemplo Então como que eh ele gerencia

- *Corpus ID:* 6698
- *Score:* 0.8533744812011719
- *URL:* oculto
- *Início:* 00:35:50
- *Fim:* 00:38:03
- *Transcrição:* importante mesmo assim a gente ter essa visão geral porque sobretudo classificado dessa forma porque daí a gente consegue saber o que existe né é importante saber o que existe de vez em quando a gente não sabe o que coisa existe a gente implementa tudo e diz pô já existiu o negócio né então ess tem essa classificação tá a gente vai se focar então mais no TF data que é essa parte no TF distributes que é como que a gente distribui os dados vamos falar de TPU Então como é que Quais são as especificidades de TPU e o resto aqui eh talvez a gente não consiga entrar muito detalhe A gente vai falar do eras claro como como trabalha com tensor board mas o resto assim não não vamos entrar em muitos detalhes tá então eh olhando para as apis de programação do tensorflow né as as apis mais comuns são essas duas o TF keras e o TF data TF keras é uma API de alto nível Então a gente vai configurar realmente as redes anais com camadas Então a gente vai tá programando bem lá no alto nível e o TF data é pra gente lidar com os dados Ok para fazer pré-processamento de dados e não só pré-processados mas assim H pré processá-los de um jeito que vai tornar o treinamento mais eficaz que assim do ponto de vista da descida do Gradiente esse tipo de coisa então Eh as operações se a gente entrar lá mais nos no no no meio do tensor Flow a gente vai ver que as operações fundamentais têm várias implementações chamadas também de kerna foco foco em cores de CPU em gpus E tpus então a gente tem esse esses diferentes backends de execução então provavelmente Quando vocês tiverem usando danor Flow eh a maior parte do código de vocês vai ser ou para api TF keras ou para api do TF data Talvez né para alguns cenários bem específicos talvez se queira programar com uma API mais de baixo nível do do tensor Flow Mas isso é bem rápido tá então assim a gente vai ficar só nesses dois universos aqui mais alto

- *Corpus ID:* 7097
- *Score:* 0.8531183004379272
- *URL:* oculto
- *Início:* 00:58:30
- *Fim:* 01:00:49
- *Transcrição:* podem inclusive testar com TPU também mas antes de gente chegar lá vamos falar rapidamente is aqui de novo é uma é uma um assunto que eu não vou conseguir mostrar para vocês na prática tá porque envolve como executar o tensor Flow no cluster Mas se vocês têm um cluster de computadores o tensor Flow pode ser usado para vocês distribuírem a carga de Treinamento usando essa estratégia de espelhamento em várias máquinas Ok e supondo que as máquinas elas são homogêneas no sentido de que todas elas têm as mesmas placas aceleradoras normalmente é o caso né Tu compra um cluster de 10 nós Cada nó vem com sei lá duas ou oito quatro placas aceleradores e todas as placas aceleradores são iguais Ok então a gente viu essa questão do paraliso de dados né com processo único a gente fez multicore a gente fez GPU uma no caso mas a gente viu como é que funciona para mais de uma e a gente fez TPU Ok então ã para executar o tensor Flow de maneira distribuída né Cada processo eh vai ser um servidor tensorflow Então a gente vai ter um endereço IP uma porta Ok e os tipos possíveis de eh workers digamos assim de servidores seriam esses quatro a gente tem ã o worker que seria quem faz o o treinamento mesmo né Eh o Chief que seria quem eh gerencia a os workers digamos assim o parameter server que é o PS e o e o evaluator esse último aqui seria eh um um tipo possível para fazer eh inferência Ok então a gente pode ter vários workers né esses workers eles vão ser quem responsável por fazer o treinamento mesmo ã e eles vão ser equipos de gpus ou equipados de tpus esse Tif Ele trabalha como um worker A diferença é que esse Tif ele também alimenta o tensor board e ele é responsável por fazer o checkpoint do modelo que a gente tem que fazer sempre o checkpoint né Vocês lemam bem dos callbacks que a gente viu e nós temos o parameter server que o parameter server

- *Corpus ID:* 6724
- *Score:* 0.8530502319335938
- *URL:* oculto
- *Início:* 00:03:19
- *Fim:* 00:05:23
- *Transcrição:* porque é tudo pixelizado ali né Cheio de pixel e tal e não consigo dizer o que que é aquilo ali né então esse modelo ele tem a uma acurácia menor né do que o emin tradicional ele atinge apenas 83% de acurácia enquanto que o emist de dígitos ele atinge uma curá de 92% então El ele realmente é mais desafiador por esse quesito né A não consegue ter uma acurácia tão boa quanto do emin de dígitos tá então esse é o que a gente vai usar de dados olhando agora PR parte de código então a gente vai usar o tensor Flow tensor Flow ele já está instalado aqui no colab e a gente vai usar o o o keras do tensor Flow então a gente tá usando aqui a versão do tensor Flow que é a 2.14 que acredit seja uma das últimas tá então isso é o que a gente vai fazer dessa parte de de uso de software depois quando a gente for usar aqui o cluster o cluster tem uma versão diferente aqui inclusive já andei executando ele tem uma versão mais antiga aqui que é a 2110 Ok mas é assim mais ou menos a mesma coisa digamos assim tem algumas funcionalidades diferentes mas é fundamentalmente assim do que a gente vai usar é a mesma coisa então para obter os dados tem diferência se você escolher aí a TPU ao invés de CPU nesse caso é nesse caso tu pode escolher a TPU mas por default eh teria que fazer um esforço para usar TPU Então por enquanto a gente vai usar só o runtime normal CPU ah a gente não vai ter um paralelismo muito grande Tá não vai vai ser porque tipo só tem um Core aqui né mas quando a gente for usar agora logo em seguida o nosso o nosso cluster a gente vai ter 44 cores daí aí todos vocês vão ter acesso ao mesmo tempo vai ser meio caótico assim né mas para vocês terem uma percepção eu quero fazer antes antes de dar acesso a vocês eu quero fazer uma uma reflexão sobre a questão do treinamento AL como é que a gente acelera o treinamento eh usando essa api

- *Corpus ID:* 7036
- *Score:* 0.8519866466522217
- *URL:* oculto
- *Início:* 00:36:35
- *Fim:* 00:39:00
- *Transcrição:* espelhamento ela realmente pega ela realmente faz isso que eu acabei de descrever ela ela replica ela espelha o modelo e faz com que cada placa atue sobre betes diferentes e depois tem uma uma uma fase de suavização né uma combinação Então essa é uma forma de implementar existe uma outra forma que são através de parâmetros centralizados a gente ter um servidor dedicado para gerenciar os parâmetros o que que são os parâmetros é o modelo em si com seus pesos né e Bis Ok E aí no caso dessa segunda estratégia a gente tem uma atualização síncrona ou aí então vamos pras perguntas que parece que apareceu bastante né Jan Carlo Pode falar a sua Av visação dos dados é pegar os modelos que estão nos devices e e ajustar os baias e pesos para todos ficem iguais exatamente tu vai pegar os os baias e os pesos das placas diferentes né que foram que divergiram né E tu vai fazer uma operação de suavização para que tu tenha só uma modelo como se tu estivesse fazendo uma combinação e essa combinação daí tem várias formas de fazer pode fazer uma média por exemplo entendeu seria Exatamente isso Antônio Fagner pode perguntar Bom dia Lucas Bom dia eh a minha pergunta é assim é uma questão técnica do seguinte a gente tá estudando essas técnicas aí eh um p tort o eh tensor Flow cu e eh dnn né todos esses frameworks esse eles oferecem maneiras de a gente fazer isso utilizar essas técnicas que a gente tá estudando na prática tá porque você vai vamos vamos trabalhar em cima disso mas qual desses te oferece uma melhor te oferece a os algoritmos essa divisão ah de ess ess quebrar esses dados né paralelizar por processo paralelizar por dados qual desses é melhor pra gente utilizar entendi ah não existe uma resposta definitiva para essa pergunta porque

- *Corpus ID:* 7017
- *Score:* 0.8514547348022461
- *URL:* oculto
- *Início:* 00:06:58
- *Fim:* 00:09:08
- *Transcrição:* também Ok bom feito então Eh dito isso Eh vamos adiante então começar então com a parte efetivamente da aula então para isso a gente vai olhar Então esse primeiro conjunto de slides aqui que é usando gpus para serar acelerar cálculo é a é esse primeiro conjunto de slides aqui então tá eh então o objetivo Então desse conjunto de slides a gente entender como que o treinamento acontece né Eh com múltiplas gpus Então a gente vai começar falando assim de uma parte mais básica né de como que a gente pode usar uma GPU e depois a gente avança para mais de uma GPU então h o suporte a GPU no tensor Flow como a gente já viu né Ele é nativo e o próprio tensor Flow ele prioriza o uso de gpus em relação a cpus porque as gpus são absurdamente mais eficientes para eh se fazer isso então mesmo que tenha cpus disponíveis ele não vai usá-las ele vai D prioridade para as para paraa GPU ou para as gpus e o e o mecanismo de detecção dessas várias gpus ele é de certa forma automático certo quando a gente tá trabalhando numa única máquina né porque ele já ele já foi concebido para isso então uma das primeiras coisas que ele faz é verificar Quais são as gpus que tem e tudo mais é claro que a gente tem algumas funções por exemplo essas aqui eh que é essa que aparece aqui que que Inclusive a gente já viu né que permite a gente eh listar Então quais são os devices físicos né então Eh como a gente já mencionou né existe toda toda a operação do tensor Flow é transformada num grafo de tensões que no caso seria um grafo de tarefas também né grafo de tensores a gente usa esse nome né porque é o nome da ferramenta né tensor Flow fluxo de tensores e esse grafo de tensores eles são escalonados né entre gpus e entre cpus que porventura estejam disponíveis para fazer o cálculo né então existe a figura de um escalonador né que vai fazer essas decisões claro

- *Corpus ID:* 6823
- *Score:* 0.8513369560241699
- *URL:* oculto
- *Início:* 01:04:57
- *Fim:* 01:07:14
- *Transcrição:* queria mostrar para vocês sobre o tensor board é uma ferramenta bem interessante para vocês incorporarem aí no no rol de de ferramentas se vocês tiverem usando o tensor Flow tá outras outras midor tipo pythor também tem esse tipo de coisa tá então agora tem uma pergunta Pode falar não vi quem que era ou não tem pergun não eu cancelei obrigado Beleza então assim professor Oi pode falar Carol eh eu executei aqui e na hora que eu verifico lá no tensorboard o gráfico pra acurácia na validação ele começa da época tipo assim ele tem menos épocas do que o treinamento aí eu não entendi o que pode ser quando tá executando esse último bloco aqui que a gente pediu 20 épocas né então ã o que pode ser pode ser que o teu gráfico não se atualizou daí tem esses botões aqui embaixo ó e tem esse botão especificamente aqui que ele meio que el ele ele recentralização é isso deixa eu ver aqui Ã Não sei nem achei esse botão aqui pera aí deixa eu ver tá no vai assim ó tem aqui a interface né daí tem time series daí tem scalars Aham Aí tu vai em scalars daí vai aparecer aqui a a a acurácia da época por exemplo e aqui tá vendo no meu gráfico aqui ó o verde não tá aparecendo Todo ó tá vendo Então tu clicas nesse botão aqui ó Fit domain to data que é basicamente resolver o x e o Y para que mostre todos os dados E aí ele mostra tudo entendeu Pode Será que não é isso que tá acontecendo É pode ser pode ser deixa eu ver aqui Ah era beleza daí é isso aí eu acho que é o tipo de coisa que poderia se atualizar sozinho né mas não não se atualiza né daí Enfim então tem que clicar ali para poder ver tudo mas aqui aqui aqui tu podes ver também ó que no no caso aqui dessa última execução especificamente que a gente fez com 20 ó tá eu tô mostrando aqui os dados ó a a parte de Treinamento ó foi até a 15ª e depois h a parte de Treinamento não tem


**Resposta gerada pelo LLM**: No ambiente do Collab, há uma CPU e uma GPU.  Em outros ambientes, podem ser listados dispositivos baseados em uma string de consulta, permitindo filtrar por tipo de dispositivo (ex: GPU).

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should provide relevant context to the question about visible devices for TensorFlow. Supporting Evidence: The RESPONSE addresses the PROMPT by mentioning that in the Collab environment, both a CPU and a GPU are visible devices. It also explains that in other environments, devices can be listed based on a query string, which is relevant to understanding how to identify devices in TensorFlow. This information is pertinent to the question asked, covering the aspects of visible devices in TensorFlow.  Score: 3


---
