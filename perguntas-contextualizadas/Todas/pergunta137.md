**Pergunta 137**: O que é o teorema CAP?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8877
- *Score:* 0.8466359972953796
- *URL:* oculto
- *Início:* 00:13:12
- *Fim:* 00:15:57
- *Transcrição:* principal é como pro particionamento como eu vou né Sincronizar as transações em várias partições Diferentes né para manter a consistência da base de dados na rplica ação como eu vou manter as réplicas sempre sincronizadas né quando eu tenho né dados em memória Cash então como eu vou sincronizar com os dados que estão armazenados de uma forma permanente né então nos bancos de dados distribuídos né O que a gente tem então é o que é chamado de Teorema Cap né que são as três letras iniciais então aí quando eu tenho a distribuição né para eh resolver esses três Desafios que a gente tem aqui sempre olhando a consistência e a disponibilidade Então a gente tem as as as três restrições principais né então é consistência então cada nós sempre vê né enxerga os mesmos dados Então os dados sempre estão en ados corretamente né a disponibilidade é se ocorre uma falha né em um dos nós então toda a base de dados deve continuar disponível né pro usuário mesmo na ocorrência de falhas né E se a gente tem falha na rede então aqui a a terceira restrição que é né tolerância de partição então o sistema continua operando com a base de dados consistente então quando a gente vai né pros bancos de dados eh distribuídos é muito difícil eh conseguir essas três restrições ao mesmo tempo né então o teorema Cap né O que que ele faz então qualquer sistema distribuído ele tem que ele pode no caso né optar por duas propriedades E aí a gente consegue usando duas propriedades consegue garantir que a base de dados eh se mantém consistente né então aqui a gente tem consistência né disponibilidade e tolerância de partição Então a gente tem aí no caso três combinações Diferentes né então por exemplo aqui o

- *Corpus ID:* 8882
- *Score:* 0.8412377834320068
- *URL:* oculto
- *Início:* 00:23:29
- *Fim:* 00:26:16
- *Transcrição:* considerando essa combinação aqui então a gente tem garantias de que a base de dados né vai tá sempre consistente professora por nesse gráfico dos círculos aí o meio tá na não seria o Cap Os Três É aqui assim o aqui no meio seriam os três aqui tá na ass ó porque eh nos distribuídos então é difícil conseguir manter os três manter o com Bom desempenho né então se eu mantenho os três certamente a gente vai perder em desempenho mas faz sentido sim se eu tenho os três aí ali seria o Cap o mas eh Desculpa só que mas desempenho não faz parte do Teorema de Cap né não faz parte não porque é que é que eu acho estranho ele está validando o desempenho sem que não faz parte do da teoria dele H é que assim ó eh no caso eh também da lado relacional o desempenho também não faz parte né mas aí a gente no relacional eu tenho as quatro restrições e por exemplo eh eu no processamento de transações eu posso ter mais ou menos eh desempenho né aqui nos distribuídos eh o quando a gente distribui ou faz replicação ou usa Cash a principal penalização é o desempenho né então por isso que no uso das restrições a gente leva em conta também o desempenho que é o tempo de resposta por usuário né porque a disponibilidade é eh o fato de eu deixar os dados né disponíveis ou não pro usuário ele tem uma relação bem direta com o desempenho né entendi Dea é rapidinho aqui ó agora interess é interessante pelo seguinte com advento e É principalmente a questão de da distribuição do dos dados né das partições da distribuição uma das coisas é que você você tem um grande volume de dado você precisa que ele que ele esteja disponível com a velocidade que você precisa e a distribuição também tem esse fator de melhorar de repente a Eu sei que você

- *Corpus ID:* 8885
- *Score:* 0.8404834866523743
- *URL:* oculto
- *Início:* 00:29:43
- *Fim:* 00:32:29
- *Transcrição:* importante aí o que que acontece né então a gente vai eh pras para pro teorema Cap de novo né então o início quando eh os bancos de dados no o SQL surgiram então nas primeiras aplicações então a ideia era usar continuar usando né as três restrições e usando o teorema Cap porque a gente continua né Com todas essas características de distribuição né então aqui a gente tem duas características nov novas Mas não tão novas né que é o aumento do volume de dados e a estrutura dos dados Então ela passa a ser mais flexível mas a gente então precisa Então desses três fatores né Eu preciso da consistência preciso da disponibilidade e como né os dados estão particionados Então se a gente tem falha no nó ou se a gente tem falha na rede então a gente pode eh colocar em risco a consistência ou a disponibilidade ou as duas coisas né então aqui pros bancos de dados no SQL né então o que que a gente tem a gente tem eh aplicações eh lá nos bancos de dados distribuídos né então antes de todo esse advento dos nocel a maioria das aplicações que são aplicações transacionais então Eh precisam muito da consistência né então quanto mais a gente aumenta eh a consistência é mais difícil de implementar e a gente acaba perdendo no desempenho né quando a gente eh diminui a consistência então né o sistema fica mais rápido e fica mais fácil de implementar então partindo pro pros noq né então o que que a gente tem a gente continua usando o teorema Cap né então mas no início eh eh dos bancos de dados no SQL Então essas características então né Essas restrições elas foram assim flexibilizadas mais do que o que a gente flexibilizou lá no no teorema Cap né então aqui nos bancos de dados

- *Corpus ID:* 8896
- *Score:* 0.8387573957443237
- *URL:* oculto
- *Início:* 00:52:02
- *Fim:* 00:55:16
- *Transcrição:* questão coloquei ali para colocar os nomes né Para quem para quem vai fazer em grupos aí para facilitar Depois daí só só um preenche então V olhar eu acho que eu não é eu vou vou só liberar aqui porque eu acho que eu não tinha liberado e aqui eu só vou mostrar ali o material tá então isso aqui eh eu coloquei dois links eh aqui então é a segunda é essa segunda publicação ela é um artigo naé que ela define bem eh eh o teorema capida então é um é um artigo de 2012 então é é quando essa teoria foi eh consolidada aí pro uso né nos no SQN então aqui a gente tem um artigo mais teórico e aqui a gente tem o o Cap que é como a Amazon e o eBay eles usam né então também para ver uma uma situação mais mais prática Então se se alguém tiver interesse eu só ver que ele aqui não atualizou aí vou Per eh até aqui das restrições alguém tem alguma dúvida mais deixa a tá aí então vou a gente mantém né como eu expliquei lá dos prazos para vocês então Eh sugerido paraa semana que vem mas a data de entrega oficial é dia primeo de Julho deixa eu já abrir aqui também bom aí deixa eu só perguntar para vocês deixa eu ver que horas são né agora é 9:30 é então assim é do a o que a gente vai ver na sequência Então a gente vai pros bancos de dados de grafos né Então aí a gente vai eh começar esse novo conteúdo eh Então a gente vai ver como é a modelagem e como é o processamento de consulta e depois a gente aí vai fazer uma atividade de modelagem de grafos eh usando ali uma ferramenta então eu pergunto para vocês assim se a gente a até às 10 horas a gente deixa o tempo daí para fazer essas questões aqui das restrições de integridade que é né o planejamento oficial eh então uma alternativa é essa daí né a

- *Corpus ID:* 1518
- *Score:* 0.8346807956695557
- *URL:* oculto
- *Início:* 00:19:03
- *Fim:* 00:21:07
- *Transcrição:* que nos permitem justamente estimar essa questão de com a probabilidade B ocorrer dado a então a gente consegue obter essa probabilidade posterior tá então em outras palavras se a gente mapear esse teorema de bens para as variáveis que a gente tá usando a gente assume aqui e o nosso a vai ser a nossa classe y e o nosso B vai ser o nosso vetor de atributos X e isso nos leva a justamente essa expressão aqui dado que eu tenho a probabilidade da classe ocorrer que é y a probabilidade daquela Instância ocorrer Ou seja a probabilidade de observar aquela combinação de atributos e a probabilidade dessa Instância ocorrer nessa Instância x dado que a minha classe é Y ou seja né dado que a classe é y é verdadeiro ou seja ela ocorre qual é a probabilidade de observar aquela Instância naquela classe eu consigo juntar tudo isso no teorema de bens e pensar em estimar essa probabilidade posteriori que a probabilidade da classe ocorrer dado que eu tenho aquela Instância certo justamente isso se eu tenho uma Nova Instância eu sei que aquele vetor de atributos aquela combinação correndo bom então qual é a probabilidade de ser a classe A classe B A classe C enfim só que a gente vai ver que bom isso aqui é mapeamento direto do Teorema de vez para o nosso domínio tá de estimar a probabilidade das instâncias da classe a probabilidade da instância dado uma classe para então estimar a probabilidade de uma classe da instância é Nosso principal interesse Mas aí a gente vai ver que tem algumas modificações que a gente vai precisar fazer para chegar no algoritmo na evidência como a gente conhece tá pensando um pouquinho sobre esse componente aqui tá cada um dos componentes teorema de bem só para a gente interpretar eu vou colocar aqui

- *Corpus ID:* 8355
- *Score:* 0.8332209587097168
- *URL:* oculto
- *Início:* 00:56:58
- *Fim:* 00:59:30
- *Transcrição:* ocap modelo vetorial modelo buano Berg fords todos eles e ele ele teve esse nome porque ocap era o nome do sistema e eles estavam experimentando com várias fórmulas para ver qual que que gerava o melhor ranking E aí eles tinham a fórmula BM que é vem de da nome best match tinha o bm1 tinha o bm11 tinha o bm15 E aí e eles ficavam eles começaram a dar nomes para essas variações da da fórmula E aí no final eles viram que que dava o melhor resultado era essa variação bm25 Então esse modelo antigo né anos 80 e 90 ainda hoje é usado como como baseline então o que que ele ele tem várias coisas aqui do que que estão presentes no modelo vetorial hã só que Originalmente ele foi proposto para eh quando a gente já já conhecia algum documento relevante alguns documentos relevantes então ele ele faz aqui ele tem ele tem os os componentes aqui é o número de documentos para consulta número sabidamente relevantes né o número de documentos relevantes que contém aquele termo específico o número de documentos que contém o termo i relevantes ou não o número total de documentos na coleção a frequência do termo no documento a frequência no termo na consulta E aí ele tem algumas coisas que ele normaliza que é pelo tamanho do documento e pelo tamanho médio dos documentos tem parâmetros aqui tem k1 K2 E e esse k que são parâmetros esse B também é é um parâmetro eh e ele pode para que que funciona esses parâmetros aqui eu tenho um que controla a a importância do term Frequency nos documentos eu posso setar isso para de zero que aí ele ele tem um efeito de um modelo binário ou eu posso colocar um valor alto que é como se eu tivesse usando o o valor do TF mesmo então uma palavra que acontece 10 vezes ganha 10 vez o peso de uma palavra que

- *Corpus ID:* 2669
- *Score:* 0.8288079500198364
- *URL:* oculto
- *Início:* 01:25:06
- *Fim:* 01:27:17
- *Transcrição:* gráfico ele muito Muitas vezes ele é usado como uma única forma de explicar um modelo porque ele explica de uma forma Global então o que que o modelo tá aprendendo de relações de uma forma geral para todas as instâncias e ele de certa forma carrega um pouco a informação do que a gente viu nas duas primeiras análises de importância de atributos e de associação a relação de valor do tributo com valor de saída do modelo tá então esse Chape é uma das técnicas mais utilizadas digamos assim para interpretação de modelos hoje em dia tá E ele pode ser usado em qualquer modelo posso usar num samba tipo Floresta aleatórias posso usar em redes neurais aprendizado profundo svm certo vamos passar para as perguntas a Caroline primeiro depois o Antônio professora nesse gráfico a gente vê essa análise em cima de todas as instâncias né Eu não sei se eu entendi direito mas olhando de baixo para cima me parece que não tão representadas todas as instâncias ali nas últimas variáveis Eu Acho É eu acho que é porque tem muita sobreposição de pontos isso pode enganar um pouco tá porque por exemplo aqui pode ser tá vendo que tem zero Pode ser que aqui muitas não tem porque aqui é o tempo né de diagnóstico da primeira primeiro e último diagnóstico para sexualmente transmissíveis Então a gente tem muito zero Então acho que essa sobreposição não não assim dá essa falsa impressão de que não tem mesmo número de instâncias mas E aí Claro as variáveis tanto que a gente vê que as variáveis principalmente Bom se tivesse variáveis binárias aqui seria um problema né E essas variáveis que tem pouca variação que eu acho que são casas aqui por exemplo a gente acaba não vendo uma dispersão muito grande então sobreposição muito grande parece ter

- *Corpus ID:* 3787
- *Score:* 0.8281561732292175
- *URL:* oculto
- *Início:* 01:16:47
- *Fim:* 01:19:06
- *Transcrição:* vizinhos mais próximos então a base de certa forma é a mesma né eu tento avaliar uma uma um gráfico de Vizinhança né e os elementos então que fazem parte desse tráfico de Vizinhança são considerados do mesmo câncer e aí se esse grave vizinha se ele se torna muito distante provavelmente próximos de outros vizinhos tão segredo seria montar esses gráficos de Vizinhança identificar no gráfico não é pontos ou trechos ou arestas digamos assim aonde essa esse grau de vizinhança é maior do que o grau de Vizinhança que vinha sendo identificado nos outros vizinhos então se eu consigo desenhar aqui as caneta Imagine que eu tenha elementos não é e aí eu consigo estabelecer a vizinhança entre eles né vou estabelecendo a vizinhança entre eles e aí entre eles todos né Então existe aí uma concentração e eventualmente tem aqui Um uma distância muito grande entre outro conjunto de elementos que também tem uma vizinhança local que entre eles é muito forte é um segredo tá identificar esses trechos aqui de onde a vizinhança é quebrada se torna muito grande né então isso basicamente que ele faz ele tenta identificar esse espectro que representa esse esse grau de Vizinhança ali esse acoplamento entre os elementos e identifica também essas quebras desse acoplamento E aí onde há essa quebra que ele ele faz o corte entre os classes e para fazer isso né imagine com muitas dimensões montar gráficos é meio complicado então isso todo uma matemática baseada em Matriz né então cria matrizes de singularidade entre os elementos para poder identificar essa essa vizinhança e essa Matriz então é decomposta não é

- *Corpus ID:* 1562
- *Score:* 0.8275865912437439
- *URL:* oculto
- *Início:* 01:30:33
- *Fim:* 01:32:35
- *Transcrição:* pequena ela não é uma constante arbitrária por exemplo né eu vou usar 0,001 a gente faz um pouquinho diferente isso aí existe um algoritmo chamado de correção de Laplace vocês vão ver em alguns lugares também chamados de Laplace smoot bem suavização de Laplace acho que seria a tradução tá isso algoritmo que ele faz é o seguinte ele assume ter uma Instância mais para cada valor possível desse atributo certo então ele assume uma Instância fictícia ou seja essa distância ela nunca é inserida no dado é como se ele colocasse no cálculo da probabilidade condicional sempre é uma constante dizendo Olha eu tenho no mínimo eu vou assumir que eu tenho no mínimo uma Instância para aquele valor de atributo para aquela classe então por exemplo para vento igual assim nesse problema aqui eu tenho cinco instâncias só que eu tô assumindo uma pessoa fictícia ou virtual né ela não é inserida não é a base de dados ela inserida como uma constante no meu cálculo para vento igual a não eu faço a mesma coisa assumo opa pera aí desculpe assumo uma Instância extra aqui voltando aqui assuma uma Instância Extra e o meu valor original é zero então o resultado disso é que se eu tô assumindo que eu sempre tenho pelo menos uma com sim uma com não na minha classe parmegiana no caso que eu não tiver nenhuma eu não vou ter zero eu vou ter um né no numerador só que o denominador ele ajusta porque eu não posso ter probabilidades acima de um né se eu colocasse aqui cinco mais um seis e dividisse por 5 que é o denominador original eu teria uma probabilidade acima de um então para ajustar o fato de eu estou assumindo que eu tenho uma Instância mais para cada valor da tributo o meu denominador é ajustado com o número de valores possíveis para esse atributo Porque se o atributo tem

- *Corpus ID:* 8894
- *Score:* 0.8275320529937744
- *URL:* oculto
- *Início:* 00:47:59
- *Fim:* 00:50:39
- *Transcrição:* também beleza aí eu depois se te interessar agora eu me lembrei eu posso te passar também temho eu orientei um aluno aqui da graduação tem não tem um ano ainda acho foi no outro semestre que ele trabalha eh trabalha numa empresa ficeira e daí todos os eh eh com dados de streaming né então ele ele analisou eh três modelagens diferentes né uma de eh colocar tudo num documento outra em particionar o documento com referências e uma terceira modelagem eh de usar chave estrangeira igual no relacional né então e E aí ele tinha um volume bem grande de dados porque ele né fez o trabalho com dados reais que foi um resultado que foi paraa empresa e analisou todos esses aspectos né então Então dependendo da modelagem daí que é o que a gente vai ver aqui né como a gente coloca os dados então eu posso eh vou melhorar num aspecto e vou piorar em outro né Ah legal Professor obrigado e aí daí o foco era no desempenho né Então tinha que ter a consistência em todas as transações mas ele queria o que tivesse o melhor tempo de resposta e acaba sempre o que a gente o relacional funciona bem né então com as chaves ali ele teve um tempo de resposta melhor eh mas anotei aqui daí para acho que nas na terça-feira que a gente vai trabalhar com com uma parte de documentos aí eu eu vou incluir essa parte aqui de transações tem lá esse exemplo que eu falei agora do do Mongo tá alguém até esse pedaço aqui Alguém tem mais eh alguma [Música] dúvida tá então assim ó aqui eu passei geral né Então as as restrições então pro relacional né a gente tem as quatro restrições atomicidade consistência isolamento disponibilidade aqui o errei nos distribuídos né então a gente tem o teorema Cap que são três restrições e a


**Resposta gerada pelo LLM**: O teorema CAP, em bancos de dados distribuídos, afirma que é muito difícil conseguir consistência, disponibilidade e tolerância a partições ao mesmo tempo.  Qualquer sistema distribuído deve optar por duas dessas propriedades para garantir a consistência da base de dados. As três restrições são: consistência (todos os nós veem os mesmos dados), disponibilidade (a base de dados continua disponível mesmo com falhas) e tolerância a partições (o sistema continua operando com a base de dados consistente mesmo com falhas na rede).

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately explain the CAP theorem and its components. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining what the CAP theorem is and detailing its three components: consistência, disponibilidade, and tolerância a partições. It also correctly states the challenge of achieving all three properties simultaneously in distributed databases, which is the essence of the CAP theorem. Therefore, it is relevant and provides a complete answer to the PROMPT.  Score: 3


---
