**Pergunta 36**: O que acontece quando o gradiente some em uma rede recorrente?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 3369
- *Score:* 0.8675644397735596
- *URL:* oculto
- *Início:* 01:32:59
- *Fim:* 01:34:35
- *Transcrição:* some né porque treido corretamente esses elementos tá a gente eu vou mostrar como é que é as ativações aqui parte a parte da LSD Ok mas então a gente chegou né no ponto esse né do Gradiente que some é o caso do problema de sequências longas eu calculo final aqui ó E aí logo na mesma no momento né Sem sem fazer o desdobramento através do tempo né o erro já fica pequeno então como esse erro aqui na ponta é a entrada do momento anterior aqui vai ficar menor ainda e assim vai ficar ainda menor até zerar né Em algum momento isso pode vir acontecer tá E aí para resolver isso vem essa rede lsdm que eu vou comentar um pouquinho né Na volta do intervalo e aí depois a gente passa para percorrer os grupos aí pra gente fazer um checkpoint do trabalho final Ok então agora são 10 e 9 vamos voltar 10 e 25 é um pouquinho menos de intervalo aí 26 minutos 16 minutos de intervalo E aí a gente continua a aula a partir desse momento OK então estamos 10:25 vou interromper a gravação aqui E aí a gente recomeça né com dúvidas que ficaram dessa parte da rede recorrente E aí a gente começa a entrar ali stm para terminar esse conjunto de slides aqui ok

- *Corpus ID:* 3370
- *Score:* 0.8615209460258484
- *URL:* oculto
- *Início:* 00:00:04
- *Fim:* 00:02:32
- *Transcrição:* e a gente começa com dúvidas até no seguinte sentido tá conceitualmente viu não é o momento da gente ter pretensão de nessa Beto da Matemática ali mas tá fazendo sentido o funcionamento de uma rede recorrente e tá mais ou menos OK assim o problema aqui do Gradiente que some Professor diga eu tô com uma dúvida sobre o gradiente que some porque assim o nosso objetivo não é justamente esse diminuindo o erro boa colocação a gente quer diminuir o erro Tá mas o erro que a gente quer diminuir a gente quer diminuir o erro obtendo uma rede treinada que dê boas predições né que a saída dela aqui ó é ao comparada com alvo né versus o alvo aqui ó versus a saída é isso aqui versos a saída seja deu uma diferença pequena né Então esse é o erro que a gente quer diminuir né o erro de predição o que que acontece quando acontece um erro de predição né Vou colocar aqui ó Esse deltazinho é igual a esse ozinho aqui a última saída da rede colocar óleo em geral né onde saída da rede ou f difícil que é o que a gente está acostumado né a produção da rede menos o y Então realmente se a gente conseguir reduzir esse erro beleza só que quando esse erro acontece a gente precisa atualizar os elementos da rede para que na próxima vez né esse erro de predição seja pequeno E aí para atualizar os elementos da rede eu calculo né digamos a parcela de culpa de cada um que é o deltazinho lá do backpropation E aí eu preciso digamos atribuir um pouco de culpa para cada elemento da rede para poder fazer a atualização né vamos lembrar lá que normalmente a regra né de atualização tem a ver com o a regra generalizada vamos até para dar ideia do Gradiente aqui né É para cada peso recebe o que tinha antes menos alfa e o gradiente da função de custo com relação àquele peso com

- *Corpus ID:* 3373
- *Score:* 0.8583155274391174
- *URL:* oculto
- *Início:* 00:05:22
- *Fim:* 00:07:31
- *Transcrição:* PIB eu não lembro qual os outros que eu falei os outros indicadores E aí vai ter uma nova predição e assim vai né então a séries temporais podem ter né múltiplos fatores a serem analisados né esse exemplo mais simples Aqui só tem um né só uma grandeza aqui ok Professor esse problema do vanishing só acontece quando a gente tem séries muito longas ou ele sempre acontece quando a gente usa redes recorrentes ele ele é mais grave né É difícil quantificar assim tá não tem um Limiar de a partir de tal tamanho de série ele acontece não é porque lembra das redes convolucionais Quando Eu mencionei isso aqui em redes convolucionais também quando a gente introduziu a né É o mesmo cenário né daquele Hello com um líquido não necessariamente tá é o cenário do Reluz versus sigmoide né que o gradiente da sigmoide é muito menor e o da Hello é maior mas o que que acontece a rede convolucional ela pode ser muito longa né pode ter muitas camadas né pode ter muitas camadas pode ser centenas de camadas com evolução um com evolução dois três 50 convoluções Depois tem o Multilaser não pode ter mais algumas também dezenas enfim e aí o gradiente some porque né tem que calcular de todas as camadas até lá no começo aqui não é que tem muitas camadas né mas é que a mesma quando eu desdobro a rede aí fica a rede fica muito comprida né mesmo ela tem muitas camadas ao longo do tempo digamos assim né como se fosse mesmo uma rede compridona lá só que é a cópias da mesma rede aqui né cada momento só que quando eu vou atualizar para primeiro para edição ali eu não consigo é a mesma ideia tá só que uma é comprida digamos conceitualmente mesmo né comprida no fisicamente né Tem muitas camadas e essa aqui é por eu processar né nas camadas dela múltiplas vezes o processamento fica Comprido né e o gradiente soma igual

- *Corpus ID:* 3104
- *Score:* 0.8527033925056458
- *URL:* oculto
- *Início:* 01:36:27
- *Fim:* 01:38:26
- *Transcrição:* camadas iniciais né porque o delta Digamos que ele some né o gradiente some é um problema tem um nome mesmo é o gradiente que some o vanishing graydents Tá mas então só para terminar aqui os intervalo eu quero fazer eu quero ver se eu vou diminuir aqui ó eu vou fazer um control c control V aqui né dos valores então novo esses aqui não mudaram né então vou colocar isso aqui w22 novo dá um ctrl shift ver ó já melhorou né então eu fiz uma atualização parcial da rede ó erro melhorou e delta 6 é o erro na saída aqui né E esse último aqui vamos ver ó melhorou né então atualizei só dois componentes né da rede e já deu uma melhoradazinha aqui no no erro tá agora o desfiz né Restaurei se eu fizesse todos então né todos os elementos aqui né calcular todos os novos elementos e melhorar ainda mais é o reduzir ainda mais o erro então deu certo gente deu certo a matemática aqui funciona né Por mais difícil Aquele que ela seja ela funciona É assim todas as peças viu tudinho lá e é assim que funciona um uma rede geral organizada assim né como Multilaser percebe que é um multi camadas certo dúvidas Então tá vamos fazer a pausa agora né 10:13 a gente volta 10:30 17 minutos aí de intervalo e aí a gente continua o restante aí dos slides até mais vou interromper a gravação aqui para ficar as duas partes da aula

- *Corpus ID:* 3374
- *Score:* 0.8485596776008606
- *URL:* oculto
- *Início:* 00:07:00
- *Fim:* 00:09:21
- *Transcrição:* assim né como se fosse mesmo uma rede compridona lá só que é a cópias da mesma rede aqui né cada momento só que quando eu vou atualizar para primeiro para edição ali eu não consigo é a mesma ideia tá só que uma é comprida digamos conceitualmente mesmo né comprida no fisicamente né Tem muitas camadas e essa aqui é por eu processar né nas camadas dela múltiplas vezes o processamento fica Comprido né e o gradiente soma igual deu para pegar mais ou menos tá então agora eu vou falar um pouquinho da lspm mas aqui também a gente pode pensar na stm como uma caixa preta tá uma rede recorrente né O que que a gente precisa levar embora né da aula de hoje lsm é uma rede recorrente melhor melhor que que o modelo tradicional é melhor que o modelo tradicional o modelo tradicional por isso que esses neurônios da IBM que tem a memória resistiva que tá tá tão tendência para [Música] Como eu posso falar é uma das tendências né O que é que até assim ó para processamento de sequências elast m é muito bom só que em especial texto é uma sequência né só que para texto aí tem os Transformers né que são um tipo de sequência específica chamada texto né Transformers são a melhor tecnologia mas aqui nós vamos ver tá o princípio então a gente não precisa se ficar travado aqui ah não entendi essas matemática aqui dessas ativação Não precisa preocupar que a gente tem que entender né que LS tem a mesma coisa né uma célula de memória que funciona melhor do que uma rede recorrente tradicional e por que que ela funciona melhor ó ela tem um monte de elementos aqui mas o que que é por enquanto a gente não vai entrar nessa aqui dentro não mas ela tem algumas portas ou que são ajustáveis é uma porta de esquecimento uma porta de entrada

- *Corpus ID:* 6904
- *Score:* 0.8465645909309387
- *URL:* oculto
- *Início:* 00:03:35
- *Fim:* 00:05:53
- *Transcrição:* dois slides aqui que são esses que eu tô mostrando para vocês agora sobre o gradiente descendente né que é a ideia que tá por trás do aprendizado profundo né então assim vocês já já viram isso em disciplinas anteriores eu acredito que foi na disciplina do Anderson Tavares onde vocês abordaram então eh eh aprendizado eh supervisionado né e aprendizado com Red Morais então eh só que aqui eu vou trazer uma ótica assim talvez um pouco mais prática não eu não eu não cheguei a acompanhar a aula dele né não sei vocês por favor me me enriqueçam com essa informação né mas aqui a ideia é só mostrar para vocês um pouco da questão da terminologia tá pra gente ficar realmente todo mundo na mesma fase OK então comentando sobre isso são dois slides da da próxima aula mas eu só vou trazer esses dois slides pra gente não e aí inclusive tá num Flow completamente diferente aqui tá eh primeira questão é que o gradiente descendente que a gente chama de gradient descent né no inglês né Ele é o que tá por trás do aprendizado profundo ele é capaz de encontrar boas soluções se a gente tiver um um um número uma uma quantidade de dados suficientemente grande ess esses dados devem estar já anotados né a gente precisa saber o que que são esses dados pra gente poder treinar então a rede né É assim que a gente diz né então existem eh na assim nesse universo basicamente três algoritmos tá o primeiro algoritmo Ele nasceu com o nome Bat gradient descent né E esse Bat gradient descent ele usa a integralidade do dos dados a cada passo de Treinamento a cada passo de Treinamento Vejam Só não é a cada época é a cada passo de Treinamento ou seja cada vez que eu vou fazer uma propagação do meu uma atualização da minha rede a cada vez que eu vou atualizar os pesos e os baias da minha rede eu vou usar todos os dados certo então isso aqui nasceu por com

- *Corpus ID:* 3368
- *Score:* 0.8452630043029785
- *URL:* oculto
- *Início:* 01:31:14
- *Fim:* 01:33:37
- *Transcrição:* predizendo por hora né quantas horas que tem aqui de entre Março e setembro então se eu tenho sei lá 100.000 horas é 100 mil pontos de dados aqui né 100 mil x aqui para 100 mil predições Então o meu Gradiente aqui né eu tenho que calcular 100 mil deltas né de desde o final lá né o delta maiorzão até o começo tá muito provável que no começo ali o delta né o errinho seja bem próximo a zero ou suma mesmo né vir de zero e eu não consigo atualizar né então é um problema de redes neurais recorrentes e é o motivo delas não serem né o conceito a gente mantém né mas numa aplicação de ponta não se usa esse tipo de rede porque é muito difícil de treinar é o gradiente e some na medida que tem mais pontos para predizer E aí por isso né vem essa rede lsdm aqui tá eu vou explicar ela mais depois do intervalo que ela tem bastante componentes aqui ó ela tem muitos componentes tá E aí a ideia tem uma recorrência ou seja os cálculos que eu faço em um momento são usados tá são usados no momento o seguinte então a recorrência tá aqui o dado de entrada ou então digamos o valor atual do Bitcoin tá ali aí eu uso esse valor para fazer um monte de cálculo aqui dentro E aí os resultados né Eu uso como entrada também para o cálculo no momento seguinte tá e o fato dela ter vários elementos aqui vários elementos a gente vai conseguir tá controlar essa questão do Gradiente que some né porque treido corretamente esses elementos tá a gente eu vou mostrar como é que é as ativações aqui parte a parte da LSD Ok mas então a gente chegou né no ponto esse né do Gradiente que some é o caso do problema de sequências longas eu calculo final aqui ó E aí logo na mesma no momento né Sem sem fazer o desdobramento através do tempo né o erro já fica pequeno então como esse erro aqui na ponta é a entrada do

- *Corpus ID:* 3447
- *Score:* 0.8439550995826721
- *URL:* oculto
- *Início:* 01:28:09
- *Fim:* 01:30:20
- *Transcrição:* mesmo erro que vai para camada anterior ele vai lá para o começo também vai ajudar o eu nesse tamanho aqui né vai ajudar atualizar essa camada Inicial então todo esse errinho é o gradiente que tava começando a sumir aqui ele foi somado né que ele foi somado com o erro que veio de lá de longe né porque a rede neural passa como tem informação né ela transmite essa informação lá longe Então ela recebe o erro lá de longe também e isso ajuda bastante evitar o gradiente que some Ah tá E aqui o atalho que não é de informação Passando só a entrada direto e aqui processando né uma camada uma Olha que interessante né agora não tem só um caminho né de processar a imagem de convolução tem dois né Então essa isso aqui recebe não só a saída dessa convolução mas recebe a saída dessa também né e ou seja Lembra daquela ideia de que as camadas né vão pegando características de baixo nível aqui retas aqui Quinas sei lá e vai compondo aqui né aqui quadrado na hora que eu junto aqui ó eu junto o que vem dessa camada que pegou a imagem né então eu meio que volta a olhar de novo característica de baixo nível e agrega com outras características de alto nível então isso pode ser benéfico também para fazer predições né E na hora de treinar também ajuda muito né que essa aqui que tá com coisa de baixo nível vai receber né um erro aqui ó bem maior né do que essa pelo caminho normal né que essa aqui receberia é o erro que iria sumindo com ao passar pelas camadas né então o que que acontece toda vez que o erro passa por uma camada ele tem tendência de diminuir né E nesse caso como eu tenho um salto aqui ó como eu tenho um salto né Então essa aqui recebe um erro que passou por menos camadas do que esse aqui que recebeu um erro que passou por três camadas

- *Corpus ID:* 3371
- *Score:* 0.843290388584137
- *URL:* oculto
- *Início:* 00:01:55
- *Fim:* 00:04:06
- *Transcrição:* de cada um que é o deltazinho lá do backpropation E aí eu preciso digamos atribuir um pouco de culpa para cada elemento da rede para poder fazer a atualização né vamos lembrar lá que normalmente a regra né de atualização tem a ver com o a regra generalizada vamos até para dar ideia do Gradiente aqui né É para cada peso recebe o que tinha antes menos alfa e o gradiente da função de custo com relação àquele peso com relação aquele peso né E para calcular isso aqui é isso aqui já é o gradiente então quando o gradiente some a gente não consegue atualizar o peso a gente tem o custo tem o erro mas o gradiente né esse elemento aqui do erro a gente não conseguiu calcular Esse é o problema matemática né E substituindo-se que lá pela fórmula que a gente tinha visto no back propagation a gente calcula um Delta do neurônio vezes a coisa que o aquele peso multiplica né colocar aqui a ativação né que da camada anterior né que é a regra que a gente usou lá ela é menos um né que a gente tinha usado E aí esse errinho aqui é aquele cálculo né do dos erros que ele causou Delta causa depende né dos erros causados E aí eu uso esses erros causados para calcular esse erro acontece que aí beleza aí eu uso esse erro para calcular os erros os outros deltas da camada anterior e eles vão diminuindo né de uma camada para outra naquela planilha do Beck propagation a gente meio que vê os erros ficando calabores E aí se esses erros chegam em 0 Houve um erro lá na saída mas aí na hora que eu vou calculando os erros intermediários ali né e iniciais da rede eu não consigo atualizar esses elementos porque o erro fica próximo a zero então Houve um erro na saída que é o que a gente quer mesmo a gente quer que não exista esse erro de saída mas quando tem erro na saída eu não posso zerar ele né nos elementos

- *Corpus ID:* 7771
- *Score:* 0.8431621193885803
- *URL:* oculto
- *Início:* 01:16:14
- *Fim:* 01:18:57
- *Transcrição:* vetor Gradiente no back prop tá learning rate de novo depende do otimizador tá critério de parada de novo né em geral a gente tem lá um conjunto de treino um conjunto de validação e a gente olha qual é o comportamento da L no conjunto de validação quando ele começar a subir eu olha provavelmente tá ver fitando mas existem vários Protocolos de Treinamento que não usam validação eu pego todo o meu conjunto de treino aplico data augmentation e assumo que o data augmentation é suficiente para gerar variabilidade na imagem então a lógica dessa segunda estratégia é eu não vou gastar dado de treinamento para validação eu vou usar tudo para treinar o meu modelo e quando ele saturar no treinamento já era tá então não existe assim uma coisa muito consolidada certo bom dito isso tá nesses minutinhos que restam aqui deixa eu Ah vocês podem abrir também né o lá no Moodle tem alguns collabs tá ã tem alguns collabs eh um deles é mais simples tá e eu eu acho que eu só vou comentar esse e depois eu vou ir pro outro cab que ele tem mais relação com o segundo trabalho de implementação tá então Ahã pera aí deixa eu só mechar nas minhas janelas aqui não eu eu ratei deixa eu abrir o moodle de novo tá então esse esse cab aqui que eu vou deixar para para vocês darem uma olhada em casa ele basicamente faz o quê ele cria né um um um conjunto de imagens né de pares de imagens e dados de categorias de Treinamento tá baseado no emist tá E aí a gente vai simplesmente criar uma rede parecida com a lenet certo e depois a gente vai avaliar a gente não vocês vão avaliar né o que que acontece com o treinamento desse cara vê o comportamento da Cura e da lzz de


**Resposta gerada pelo LLM**: Quando o gradiente some em uma rede recorrente, não é possível atualizar os pesos da rede porque o erro fica próximo de zero, mesmo havendo um erro na saída.  Isso dificulta ou impossibilita o treinamento, especialmente em sequências longas.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to the prompt regarding what happens when the gradient vanishes in a recurrent neural network. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining the consequences of the gradient vanishing in a recurrent neural network. It mentions that the inability to update the weights occurs due to the error approaching zero, which is a key aspect of the vanishing gradient problem. Additionally, it highlights the impact on training, particularly for long sequences, which is relevant to the context of recurrent networks. Therefore, the RESPONSE is relevant to the entire PROMPT.  Score: 3


---
