**Pergunta 78**: Como podemos comparar dois modelos preditivos usando o teste de McNemar?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2300
- *Score:* 0.8652071356773376
- *URL:* oculto
- *Início:* 00:00:02
- *Fim:* 00:02:11
- *Transcrição:* Olá pessoal na segunda parte dessa aula né finalizando essa discussão sobre avaliação de modelos preditivos que a gente vai praticar a longo das próximas aulas eu vou dar uma Panorama geral sobre a ideia de comparação de modelos né então a gente vem fazendo comparação de modelos quando a gente fez os que a gente fez comparação de modelos quando vocês por exemplo trabalharem com otimização de preparamentos vocês farão também otimização é desculpa em comparação de modelos mas aqui a gente tá falando no ponto de vista mais estatístico certo para testes que possam avaliar se essa diferença que a gente está observando É de fato significativa e não ao acaso então toda parte de teste de hipótese estatística ela é muito bem fundamentada muito bem consolidada e o que a comunidade acadêmica tem feito é verificar quais tipos de testes que já existem da estatísticas são apropriados para lidar com as particularidades que a gente tem em desenvolvimento de modelos com aprendizado de máquina uma das formas é bastante simples digamos assim de fazer uma comparação de modelos né de forma mais robusta é analisando os intervalos de confiança então é muito comum a gente executar avaliação de modelos inúmeras vezes e calcular média desvio padrão mas nem sempre com esses valores de médias e o padrão a gente consegue ver se de fato é esse essas esses desempenhos são diferentes ou seja se eu tenho um modelo que de fato é melhor do que o outro então uma das formas de fazer isso é calcular intervalo de confiança o intervalo de confiança por padrão eles são calculados com amostras de busca aleatórias se vocês lembrarem nós falamos de busca Trap quando discutimos aprendizado né Floresta aleatórias por exemplo ele peguem e também na questão da própria divisão de dados para avaliação de modelos certo

- *Corpus ID:* 2832
- *Score:* 0.8569527268409729
- *URL:* oculto
- *Início:* 01:40:00
- *Fim:* 01:42:09
- *Transcrição:* informação veio ali e bom que você já já colocaram porque é uma forma diferente de explorar o desempenho e fiquei feliz que vocês pensaram em explorar essas estratégias que são menos comuns que elas deixam né mas que são interessantes quando a gente tá comparando modelos ficou bem bem interessante pessoal parabéns pelo parabéns pelo trabalho aí né todos tem uma pergunta do Antônio Deixa eu só tô confirmando aqui tem alguém como levantada é o Antônio isso é só nessa questão do na parte da estatística do dos modelos só querem entender uma coisa a gente também fez o teste ter pareado então para escolher qual dos dois sentidos realmente diferença dos dois melhores mas quando você fala de colocar o intervalo de confiança eu vou comparar também pode ser só um pode ser o outro eu tenho que depois de fazer o teste até pareado faz os dois faz um intervalo de confiança para ver se eles se sobrepõe dos dois modelos é só isso que eu não entendi como a gente pode melhorar isso na realidade vocês poderia até optar por uma ou por outro tá teste de o teste de hipótese né pelo P valor ele dá uma uma métrica muito objetiva apesar das suas falhas que é o p valor então ali pelo valor vocês vão ter esse dia vocês vão rejeitar ou não é hipótese nula então ah se o p valor é menor que 0,05 considerando que vocês definiram Esse trecho bom Existe diferença significativa entre os modelos né os modelos eles têm é então não precisa na verdade não precisa os dois tá então um só está bom pessoal fazendo um comentário os finais então eu até coloquei no Moodle um conjunto de slidesumarizando coisas que a gente conversou ao longo da disciplina tá mas dado o tempo avançado assim a gente realmente acabaria passando não cinco minutos mas talvez uns 15 do horário e ficou um pouquinho apertado mas eu acho

- *Corpus ID:* 2385
- *Score:* 0.8522432446479797
- *URL:* oculto
- *Início:* 00:27:05
- *Fim:* 00:29:05
- *Transcrição:* são coisas totalmente diferentes né o modelo aí eu teria que usar a mesma a mesma proporção de transformação do treino no teste né para poder realmente ter uma base comparativa é então tem esse Vamos pensar o seguinte a cada um vai ter sua média seu desvio padrão né concordo contigo que podem ser um poucos diferentes mas vamos assumir que as duas distribuições são exatamente iguais tá vamos assumir que isso a gente não consegue garantir na vida real Mas vamos assumir que o dado de teste ele segue exatamente a mesma distribuição do trailer então se eu fosse capaz de estimar a média do teste e desvio padrão seriam iguais ou do treino tá Ou seja a gente não teria esse problema de discrepância mas ainda assim eu teria um problema de fazer esse tipo de cálculo aqui para os dados que estão chegando novos dados ver se alguém tem algum outro tipo de isso que o Alexandre falou você pode ser um problema tá isso com certeza é só tentando fazer vocês pensarem em outro tipo de problema que pode surgir se você quiser você pega um registro sofre dado como é que você vai aplicar esse essa esse único registro fazer essa transformação para para poder comparar os dados né para eles ficarem dentro da mesma sei lá da mesma escala digamos assim exatamente a gente não pode assumir que quando a gente tiver o nosso dado de teste o nosso dado de teste durante o desenvolvimento modelo representa um conjunto de dados quando a gente for usar o modelo dado de teste pode ser uma Instância Só se a gente tá fazendo uma questão para definir por exemplo se existe nada de imprensa risco de inadimplência de um cliente vai chegar um cliente pedindo um empréstimo Enfim então como é que eu vou chamar a média cidade desse cliente como é que você chama a média de salário a média não tem como estimar uma média chega uma

- *Corpus ID:* 2309
- *Score:* 0.8513365387916565
- *URL:* oculto
- *Início:* 00:14:51
- *Fim:* 00:17:00
- *Transcrição:* é que não tem nenhuma sobreposição entre treino e teste certo então os resultados são visto como mais Independentes entre si ao longo de múltiplas execuções porque muitos desses testes assumem Independência entre os dados e essa amostragem permite a variar medir essa variação de desempenho em função dos dados de Treinamento eu mantenho os demais configurações de preparamos então como eu falei esse teste foi implementado com alguns ajustes para esse caso de aprendizado de máquina de avaliação de modelos preditivos mas é bastante Fácil de usar né No momento que a gente tem uma implementação pronta por fim se eu quero comparar múltiplos modelos ou seja se eu tô avaliando três quatro cinco modelos né existe um teste de esse teste de q de cálculo né que é uma generalização daquele primeiro teste que a gente é discutiu né da tabela de contingência então ele faz essa generalização para comparar esses acertos e erros entre três ou mais classificadores e é interessante isso porque quando a gente está comparando três ou mais classificadores a gente roda esse teste de teste de coco para verificar se existe uma mudança uma diferença de desempenho significativa Entre esses classificadores certo se o valor de p é significativo ou seja se ele é menor que 0.05 sempre assumindo né que a gente adotou 0.05 como trecho então diz assim existe uma diferença significativa entre ser modelos Tá três ou mais mas não diz exatamente quais modelos diferem entre si e aí o que a gente tem aqui é uma o que a gente chama de uma análise pós-rock a gente detecta que existe uma diferença significativa entre esse ser classificadores e depois a gente faz o teste demanda para par para avaliar quais classificadores

- *Corpus ID:* 2826
- *Score:* 0.8502888679504395
- *URL:* oculto
- *Início:* 01:30:04
- *Fim:* 01:32:05
- *Transcrição:* da nossa classe Alfa aí no data 7 já começar a ter uma noção um pouco mais Inicial né bom a gente pegou dois algoritmos que para poder fazer o Spot cheque avaliar Qual que é o melhor né então eu escolhidos foram calientes igual a um e a árvore de decisão aí com cropped como profundidade cinco né E para se balanceado aí agora é assim super estratégias que a gente utilizou para fazer a comparação desses dois algoritmos desses dois modelos a gente fez logo de cara a gente aplicou aí a classificação andando né apesar de ser uma coisa uma coisa que não diz lá muita coisa mas a gente achou interessante fazer no início Porque como ele é um troço meio ser inteligência ele ele é um classificador de cima de patrições aleatórias né ou então em cima da classe majoritária então bom vamos comparar nesses dois algoritmos que a gente depois vocês não estão perto do dano e se eles estiverem perto ou piores significa que a gente tem que voltar para crocheta lá pro pré-processamento ou até rever-se para dizer que tá sede a expectativa é que o os nossos dois modelos que a gente escolheu para comparar fossem bem melhor do que abando e foi de fato o que aconteceu tá uma outra coisa que a gente fez foi teste de hipótese e é mais uma comparação também que a gente ajudou seria interessante fazer no início para ver se a gente levou poucos algoritmos para o Spot chat né para o objetivo aqui apresentação a gente fez o teste de hipótese pelo momento para ver se eles estão equipados em termos de performance uma disputa meio injusta tá então a gente conseguiu pelo teste de hipótese comprovar que eles têm uma performance muito parecida lá né considerando aquela hipótese que os dois são iguais né a gente acabou tendo aquele aquela comparação lá do testezinho né do Tyler de ter teste a gente vê que a diferença de performance deles ele não pode ser

- *Corpus ID:* 2819
- *Score:* 0.8497216701507568
- *URL:* oculto
- *Início:* 01:17:49
- *Fim:* 01:20:11
- *Transcrição:* apresentação Só essa aqui que é a comparação do esporte cheque usando a métrica F1 tá onde nesse caso ficou o Randon flores na composição a gente acabou optando por ele a etapa seguinte foi já definindo que a gente vai trabalhar currando um Forex fores né a gente comparar várias conjuntos de Hiper parâmetros e ver qual que seria mais adequado para isso a gente usou a ideia do mestre de crosta né segundo o exemplo dos próprios coisas que foram vistas nas aulas né com esses conjuntos de parâmetros ali A partir dessa análise a gente observou que o que na maioria dos casos de como o melhor modelo foi uma profundidade 13 com 300 estimadores mas olhando e pensando com mais calma sobre isso a gente ficou com a impressão de que isso estava Produzindo um certo over fiting por causa dessa profundidade muito grande da das Árvores né a gente então partiu para comparar usando a métrica F1 né qual que era o desempenho no conjunto de treinamento de validação e deu realmente uma certa diferença então a gente imagina que pode ter acontecido um certo over fiting aí então também é uma oportunidade de explorar um pouco melhor o que fazer aí para reduzir isso né então o que que a gente fez então a gente Partiu para comparar no conjunto de teste porque isso aqui é esse aqui que eu voltando slide tá é a comparação do treinamento com a validação produzida no próprio nessa de cross validation tá agora a gente Partiu para ver o desempenho desse modelo no conjunto de teste então para isso a gente treinou com todo o conjunto de treinamento né E chegamos a esse resultado que está apresentado aqui então aqui tem a matriz de confusão né e a própria os próprios indicadores lá que o Python traz para gente né Então na verdade acabou dando um pouquinho melhor né Então é isso daí E aí então a gente fez

- *Corpus ID:* 1668
- *Score:* 0.849677324295044
- *URL:* oculto
- *Início:* 01:07:19
- *Fim:* 01:09:29
- *Transcrição:* poderia se eu quisesse testar diferentes eu poderia também testar variações né dessas combinações então eu uso k = 1 com distância de manhatta cai igual a um com a distância cleidiana k igual a três qual a distância de manha rata cai igual a 3 com distância clidiana então assim eu poderia ter o que a gente chama de uma Grid né de valores possíveis de combinações de prepaâmetros tá por isso que quando a gente trabalha com aprendizado de máquina treido esses modelos preditivos existe um custo computacional melhor config uração de prepaâmetros para o algoritmos para o algoritmo que a gente escolheu para o conjunto de algoritmos que a gente tá usando certo mais alguma pergunta bom o que acontece acho que eu vou continuar porque ainda não sou dessa eu vou continuar mais um pouquinho antes do nosso intervalo o que acontece é o seguinte esse svm linear de Margem suaves que é esse que a gente consegue usar com esse termo de regularização em muitos casos ele vai funcionar bem tá claro de margens rígidas ele é muito restrito né o de imagens suaves ele muitas vezes conseguem porque percebam que aqui para esses dados eu não vou ter necessariamente uma divisão linear não é uma divisão linear aqui mas no momento que eu digo para ele que eu aceito que ele ergue algumas algumas instâncias né eu consigo fazer com que ele determina uma fronteira de decisão linear para tentar fazer o melhor possível né para classificar aqueles dados com essa tolerância de erro tá só que a gente vem discutindo que nem todo o problema e da grande maioria dos problemas na vida real não são linearmente separáveis Ou seja a gente já sabe que uma fronteira de decisão linear dada por uma reta o hiperplano não vai ser capaz de modelar complexidade daquela daquele problema de classificação certo então o

- *Corpus ID:* 2827
- *Score:* 0.8495773673057556
- *URL:* oculto
- *Início:* 01:31:36
- *Fim:* 01:33:57
- *Transcrição:* gente fez o teste de hipótese pelo momento para ver se eles estão equipados em termos de performance uma disputa meio injusta tá então a gente conseguiu pelo teste de hipótese comprovar que eles têm uma performance muito parecida lá né considerando aquela hipótese que os dois são iguais né a gente acabou tendo aquele aquela comparação lá do testezinho né do Tyler de ter teste a gente vê que a diferença de performance deles ele não pode ser resultado que a hipótese ou seja eles têm realmente desempenho similares tá bom para a gente fazer saindo essa parte inicial a gente já tem uma ideia que nos dois algoritmos vão ser comparados dois tipos de modelo a gente agora vai fazer tirar tema de forma mais precisa e para isso a gente definiu a nossa métrica primária para terminar o melhor modelo ou o F1 score né E para isso a gente utilizou aí Eu repito de como ele deixa utilizando Aí sim todos repetições ao invés de software sem repetição vai na tela para execução é uma evidênciazinha da execução do repito de nélidation e logo abaixo vocês estão vendo aí o box né respectivos aí do kmn do árvore de decisão e já dá para ter uma boa noção que o ninja se apresentou bem melhor aí que praticamente todos os aspectos aí o métodos digamos assim não é o F1 que a nossa América primária aí então vocês podem ver que ele se destacar já se destaca né exatamente melhor do que o árvore de decisão mas visualizações gráficas né Para a gente determinar com mais precisão Porque a gente já tá observando sobre a ótica do FM Store que a nossa América é privada melhor no modelo e novamente dessa vez de forma mais Evidente ainda em todos os gráficos a gente percebe que também pelas escalas aí que eu cair Ele tá se demonstrando um modelo bem melhor bom definindo que o KNN foi melhor na disputa aí com a decisão a gente vai

- *Corpus ID:* 1906
- *Score:* 0.8491965532302856
- *URL:* oculto
- *Início:* 01:10:29
- *Fim:* 01:12:34
- *Transcrição:* e teste ou treino e dados Independentes para ver qual é o valor de preparamento que me dá a melhor generalização possível aquela em que o desempenho no dado de teste tá próximo do dados de treino e Ambos são satisfatórios para mim né porque por exemplo eu posso dizer eles estão muito próximos sei lá aqui por exemplo tá mas aqui não é muito maior a diferença e o meu desempenho geral é maior por exemplo Então existe esse subjetividade aqui tá pessoal por exemplo alguém pode talvez não nesse gráfico mas em outros alguém pode dizer olha esse para mim esse é o melhor parâmetro e alguém e outra pessoa dizer que é outro tá então mas vocês podem ver que mesmo né seleciodo Por exemplo esse ou esse o que a gente tá fazendo é evitando um overfito em que acontece nessa parte aqui sempre no casamento de treinamento de Treinamento desculpa de teste a maior eu entendi você disse a gente tem que procurar uma diferença menor entre a métrica de treinamento de teste mas porque simplesmente não pegar o teste e pegar a maionese com o valor da maionese a gente pode até pensar em supondo que a gente tem o nosso dado de treino de validação que a gente vai usar para otimizar isso aqui e aí depois é a gente pode até pode ver olha né eu vamos supor que que é a validação que eu prefiro essa nomenclatura é eu pego uma hora de validação Até posso mas é também importante ver como é que esse maior de validação sai em relação ao dado de treino entende porque às vezes essa diferença ela pode ser maior de validação Mas tem uma não necessariamente A diferença vai ser menor possível é isso eu quero dizer então é bom olhar as duas coisas mas eu acho que para essa questão de maior validação pode ser uma boa desde que a gente não ignore as outras características que é a diferença entre

- *Corpus ID:* 2308
- *Score:* 0.8489188551902771
- *URL:* oculto
- *Início:* 00:13:06
- *Fim:* 00:15:22
- *Transcrição:* tem uma documentação para esse teste que tá disponível já para uso né nessa biblioteca é minha Nextel se eu quero fazer a comparação de dois modelos e eu consigo fazer múltiplas execuções do algoritmo uma sugestão dada né que é pelo Detran que o autor né Desse artigo que a gente tá usando como base é usar um teste t de estudos pareado né com cinco repetições de dois validez ou seja essas repetições lembrando que quando eu uso dois Fontes basicamente tô pegando meus dados de forma juntas e um para teste e eu faço isso cinco vezes certo faça essa divisão cinco vezes com sementes aleatórias diferentes E aí ao final dessa análise a gente roda esse teste de Studart né Que que foi adaptado digamos assim é para fazer essa essa estimativa a partir desses dessas cinco repetições de dois e ele também vai me dar um p valor certo e aí a partir do P valor a gente faz a mesma análise for menor que 0.05 assumindo o nível de significância de 95% a gente diz que existe uma diferença significativa entre os modelos em termos de desempenho claro que isso a gente faz em relação a uma métrica de interesse né um ponto importante dimensionar eu posso avaliar por exemplo a sensibilidade se é a minha métrica de interesse ou F1 que eu quero ponderar entre precisão e assim por diante dessa forma o que acontece né usando os dois Fontes é que não tem nenhuma sobreposição entre treino e teste certo então os resultados são visto como mais Independentes entre si ao longo de múltiplas execuções porque muitos desses testes assumem Independência entre os dados e essa amostragem permite a variar medir essa variação de desempenho em função dos dados de Treinamento eu mantenho os demais configurações de preparamos então


**Resposta gerada pelo LLM**: nan


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Não avaliada


---
