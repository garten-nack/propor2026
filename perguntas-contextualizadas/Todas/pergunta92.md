**Pergunta 92**: Como o aprendizado de máquina pode ser usado para treinar um agente a jogar um jogo de tabuleiro como Go, onde a recompensa só é recebida ao final da partida? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 4246
- *Score:* 0.8944036960601807
- *URL:* oculto
- *Início:* 00:40:05
- *Fim:* 00:42:24
- *Transcrição:* né o o o agente a ação final digamos assim né que que vale a pena outro exemplo tá eh aqueles sistemas lá que jogam jogos de tabuleiro né ou xadrez ou go enfim tem um tabuleiro lá com as pecinhas com as pecinhas lá e aí o a jogada é colocar uma peça né então eu não preciso dizer se a jogada foi boa ou ruim na hora que o que o que o computador fez eu nem eu sei se a jogada é boa ou ruim mas eu sei que depois de uma sequência de jogadas se ele ganhou ou se ele perdeu então eu posso dar recompensa só aqui no final e é isso que foi feito mesmo aqueles sistemas igual o alfa go lá eles só recebia recompensa ao final da partida Então pensa um jogo como go tem sei lá 50 movimentos até acabar ou mais né pode ser centenas de movimentos E aí o computador sem saber nada né sem receber nenhum avaliação do que que ele tá fazendo aqui no meio recebe só o final da partida ó ganhou ou então Ó perdeu aí ele ao jogar milhões de partidas bilhões Talvez ele consegue descobrir sequências de ações que fazem ele ganhar né então não o projeto né Na hora que eu tiver pensando na recompensa eu posso e simplificar do ponto de vista do ambiente né eu não preciso avaliar exatamente cada ação Zinha que o agente faz se eu sei ao final né O que que é bom o que é ruim isso já pode ser suficiente né para a gente aprender se eu tiver o poder computacional suficiente para treinar ele tá mas então tenham isso em mente né uma ação no presente pode afetar resultados eem um futuro distante e do ponto de vista do ambiente pode ser que eu avalie só esse futuro distante aqui né para o agente aprender a a se comportar tem temos uma dúvida pode falar oi oi Anderson Bom dia é assim eh eh Nessa situação você tem eh uma recompensa somente no final e ele tem um tempo por exemplo para para fazer milhões ou como você falou bilhões de

- *Corpus ID:* 4252
- *Score:* 0.8841428756713867
- *URL:* oculto
- *Início:* 00:50:06
- *Fim:* 00:52:19
- *Transcrição:* com isso né ao jogar milhões lá de partida eles tinham um poder computacional né são era a Deep Mind né Deep Mind que a Google comprou então ninguém menos do que eles então eles tinham o poder computacional se puderam se dar o luxo de não precisar estudar o jogo né para projetar o computador e aí com isso fizeram eh só projetaram a recompensa final aqui que é muito mais fácil muito mais fácil isso aí a gente consegue ver né uma pessoa sabendo a regra do jogo vê um tabuleiro final lá e sabe ó ganhou perdeu e empatou então isso foi fácil de fazer e então deixaram a parte difícil né com ag gente no a gente descobrir aqui a sequência de coisas né que fazia vencer ou não então tiveram essa esse cuidado e aí agora né pensando um pouco do do ponto de vista do do do agente vamos lembrar né que cada ação Zinha que o de maneira conceitual tá o agente tá num estado aqui vou chamar de S1 aí escolhe uma ação vou chamar de A1 também E aí vai recebe uma recompensa e chega no próximo estado e aí escolhe uma próxima ação e aí vai recebe uma próxima recompensa e chega no próximo estado e isso se repete né então o a gente recebe uma sequência de Recompensas né Mesmo que seja tudo zero aqui né E só no final ele receba lá o número né mais um e e menos um de Vitória e derrota Tá mesmo que as recompensas intermediárias sejam zero Mas pode ser que elas não sejam zero né pode ser que não sejam Então a gente tem que aprender a maximizar uma sequência de Recompensas né E pode ser o caso por exemplo né de ã por por alguma questão do do projetista lá eh alguma ação né ser recompensada negativamente mas ao final ela por exemplo se ele tomasse outra ação aqui ação sei lá a linha um aqui outra ação alternativa né que fosse tivesse uma recompensa maior né chamar aqui de mais 20 né aí ao final de tudo aqui ele recebe 200 mais 200 e aqui ele recebe

- *Corpus ID:* 4232
- *Score:* 0.88335782289505
- *URL:* oculto
- *Início:* 00:14:57
- *Fim:* 00:17:32
- *Transcrição:* n de uma forma eles conseguiram programar isso eh com a gente só de de fazer um né tentativas e e erros sem sem não não existe um algoritmo descrevendo né que o que o agente Deva fazer as coisas é só por tentar né eles tentam arrastar caixa tenta arrastar bloco E aí ao receber um ao ver que aquilo deu certo né que ele venceu o jogo digamos assim aí eles H ganham pontos e o objetivo é maximizar os pontos e a gente vai vê exatamente né Qual é o o mecanismo matemático né disso do que que faz os agentes aprenderem a terem comportamentos tão sofisticados assim e acabou até descobre erro no no jogo ó ele tá surfando aqui em cima de uma caixa né não deveria acontecer mas como tá foi o jogo né Foi mal programado o ag gente pode aprender né a a se a expl né esses problemas enfim né um exemplo agora né da computacional de aprendizado por reforço tá gente então para ilustrar né o poderio de de ã dessa dessa área e outra outra vertente aqui também ó eh quando né um agente de aprendizado por reforço tá bem treinado né o treino foi bem sucedido deixa eu abrir o o vídeo aqui eh quando a gente tá bem treinado ó aqui é um um da da do Disney research né pessoal de pesquisa lá da da Disney e a ideia aqui é um robô sendo treinado a a jogar né a bola então ele é treinado a fazer o movimento ali de pegar uma bola e jogar outra né então ele isso aqui depois que ele já tá treinado né obviamente antes de de conseguir ser bem sucedido ele errou muitas vezes né então e olha que ele consegue né fazer os movimentos e pegar a bola jogada em várias eh posições né então tem um sistema de câmeras que rastreia a bola e diz né qual é e o robô aprende Qual é o o movimento certo a se fazer E aí depois ele depois de bem treinado ele ele consegue jogar né mesmo com pessoas que

- *Corpus ID:* 4248
- *Score:* 0.8822154402732849
- *URL:* oculto
- *Início:* 00:43:28
- *Fim:* 00:45:37
- *Transcrição:* posso né dar a recompensa digamos definitiva né da desse que é a tarefa mesmo no caso aqui do jogo eu só preciso dizer se o agente ganhou ou não mas se eu não tiver aí eu vou precisar de um pouquinho mais de cuidado para projetar recompensa para realmente né guiar o agente de forma que ele tenha né pontos aqui mais eh constantes de avaliação Para que ele não precise jogar toda uma partida né E só só no final lá que ele vai saber se aquela eh sequência de ações foi boa ou ruim se eu tiver pontes mais frequentes o agente vai pode fica mais fácil para ele saber qual foi o ponto que ele começou a derrapar né ou ou se se ele tava indo bem até aqui até esse primeiro ponto e aí depois no segundo ponto Foi mal então alguma coisa aqui no meio né foi ruim ou seja é mais fácil isso do que se eu dou uma recompensa só ao final que aí o o ag gente tem que descobrir nesse meio todo aqui né onde é que ele errou né então Eh se eu se eu não tiver poder computacional então tenho que gastar mais tempo né projetando uma função de recompensa que dê uma avaliação mais frequente pro agente então comentário tá perfeito pertinente aqui OK pode falar bom dia bom dia bom dia não porque eu tava pensando também que depende da complexidade da tarefa talvez também né sim porque o conceito de que ele acertou também igual você falou né É melhor avaliar Talvez no final né dependendo da complexidade porque tem ele pode fazer de várias formas né acertar de várias formas mas tem aquela forma mais a melhor né que é o caso da pontuação né que você colocou lá né Eu acho que vale a pena avaliar a o fator da poder computacional e acho que é a complexidade da tarefa também né Eu acho que é uma coisa é porque o cuidado que eu falei aqui né quando a gente começa a dar uma avaliação mais frequente pro a gente a gente nós ao projetar a recompensa nós vamos ter que saber né O

- *Corpus ID:* 4247
- *Score:* 0.8820921182632446
- *URL:* oculto
- *Início:* 00:41:45
- *Fim:* 00:44:00
- *Transcrição:* tenham isso em mente né uma ação no presente pode afetar resultados eem um futuro distante e do ponto de vista do ambiente pode ser que eu avalie só esse futuro distante aqui né para o agente aprender a a se comportar tem temos uma dúvida pode falar oi oi Anderson Bom dia é assim eh eh Nessa situação você tem eh uma recompensa somente no final e ele tem um tempo por exemplo para para fazer milhões ou como você falou bilhões de testes Mas dependendo da situação eh será que pode ser que você vá dando Recompensas intermediárias porque porque ele pode avançar mais rápido na verdade tô falando isso mas é uma pergunta eh dependendo da situação pode ser que eu tenha que eh eh eh projetar algum tipo de recompensa para que ele Aprenda Mais rápido sem ter que ele aprender todos os passos possíveis e só no final e receber observação perfeita né o o a palavra-chave ali de poder computacional eh tem tudo a ver com com com com essa pergunta com esse comentário até né porque tá certo né não é é mais que uma pergunta é um comentário correto então a decisão né de de como que eu vou projetar né A recompensa aqui no ambiente tem que pensar se eu tenho poder computacional para treinar né o o algoritmo ali o algoritmo tá pronto né mas ele precisa interagir com o ambiente para para saber né para se comportar bem então eu eu tenho recursos computacionais suficientes para treinar esse esse a gente se eu tiver então eu posso né dar a recompensa digamos definitiva né da desse que é a tarefa mesmo no caso aqui do jogo eu só preciso dizer se o agente ganhou ou não mas se eu não tiver aí eu vou precisar de um pouquinho mais de cuidado para projetar recompensa para realmente né guiar o agente de forma que ele tenha né pontos aqui mais eh constantes de avaliação Para que ele não precise jogar toda uma partida né E só só no final lá que ele vai saber se aquela eh sequência de

- *Corpus ID:* 4245
- *Score:* 0.8812609910964966
- *URL:* oculto
- *Início:* 00:38:28
- *Fim:* 00:40:40
- *Transcrição:* pessoal que é mais novo já talvez não esteja Ah se bem que tá né fizeram várias versões novas aí aí tem que saltar l o bigodinho dele aqui tem que saltar numa plataforma né e chegar do outro lado aqui e pegar a moeda né tem a moedinha dele aqui e eu não preciso ficar dando eh por exemplo se eles consegu fazer o salto aqui né eu não preciso recompensar imediatamente esse salto nesse ambiente do Mário ele pode simplesmente a recompensa ser a pontuação na tela lá né Tem um score lá de quantas moedas ele pegou né então ele vai ficar andando aqui na aleatório na na no cenário E aí eu não preciso ficar dando recompensa para ele eu posso simplesmente fazer né O Agente eh maximizar justamente essa recompensa aqui que ele recebe de ele precisa fazer muitas ações essa ideia de decisão sequencial né precisa fazer muitas ações para conseguir maximizar essa recompensa conseguir ganhar ponto né então ele pode é percorrer um longo caminho até chegar aqui sem nenhum sinal de que aquilo foi bom ou ruim e quando ele saltar e pegar moedinha Aí sim essa pontuação muda não lembro quantos pontos vale sei lá mais 10 aí que isso né só mesmo assim é suficiente para a gente aprender nesse comportamento de seguir reto e depois pular e pegar a recompensa né Eh Ou seja eu não preciso est constantemente ali né digamos incentivando a ag gente a fazer coisa certa eu posso só eh avaliar quando realmente é importante né o o o agente a ação final digamos assim né que que vale a pena outro exemplo tá eh aqueles sistemas lá que jogam jogos de tabuleiro né ou xadrez ou go enfim tem um tabuleiro lá com as pecinhas com as pecinhas lá e aí o a jogada é colocar uma peça né então eu não preciso dizer se a jogada foi boa ou ruim na hora que o que o que o computador fez eu nem eu sei se a jogada é boa ou ruim mas eu sei que depois de uma sequência de jogadas se

- *Corpus ID:* 4251
- *Score:* 0.8770933151245117
- *URL:* oculto
- *Início:* 00:48:28
- *Fim:* 00:50:34
- *Transcrição:* hã Isso é o agente que vai cuidar né se ele vai como que ele vai eh armazenar as experiências dele né o ambiente só se preocupa nisso ó receber uma ação processar aquela ação e gerar né a próxima situação pro pro agente nós vamos tratar disso também né Como que o agente faz né Para dar dessa sequência aqui né de coisas como fazer para aprender n nós vamos lidar com isso também aí mas do ponto de vista do ambiente não é a preocupação dele essa trajetória né O Agente que vai ter que cuidar de armazenar ou não tá Tá certo obrigado Beleza muito bem muito bem pessoal est Estamos indo bem eh e aí então o que ao né trabalhar com o ambiente a gente vai ter essas coisas em consideração né se a gente tem poder computacional suficiente como que a gente vai projetar a e quanto conhecimento do domínio a gente tem também né para projetar a recompensa aqui por exemplo tá no negócio do do Gol lá né o bote que jogava Gol nenhum dos programadores era Eh sei lá jogava a nível né de campeonato e talvez sabia as regras do jogo talvez fossem bons jogadores mas eles não tinham conhecimento né para introduzir aqui né uma avaliação eh bem Hã qualificada né de qual seria uma jogada boa ou ruim determinado da situação então só deram isso aqui né pro agente a recompensa final do da partida Vitória empate derrota E com isso né ao jogar milhões lá de partida eles tinham um poder computacional né são era a Deep Mind né Deep Mind que a Google comprou então ninguém menos do que eles então eles tinham o poder computacional se puderam se dar o luxo de não precisar estudar o jogo né para projetar o computador e aí com isso fizeram eh só projetaram a recompensa final aqui que é muito mais fácil muito mais fácil isso aí a gente consegue ver né uma

- *Corpus ID:* 4442
- *Score:* 0.877054750919342
- *URL:* oculto
- *Início:* 01:30:09
- *Fim:* 01:32:25
- *Transcrição:* e disse né que o agente tinha que fazer isso né Ele simplesmente repetidamente lá a ao interagir com ambiente né múltiplas vezes ele aprendeu né que isso é é o melhor jeito né de de jogar de acumular Recompensas rapidamente né então ainda não é tão impressiote quanto aqueles os bonequinhos lá construindo né os o o pique esconde lá mas né mostrou o poderio de aprendizado por reforço né com aproximação de funções essa rede aqui né dqn conseguiu aquele feito em outros jogos também tá outros jogos teve desempenho sobrehumano mas esse é igualmente impressiote professor que ele consegue ter uma resposta rapidíssima e a bola vai numa velocidade e ele que ele consegue responder né isso muito acima do ser humano tá muito acima do ser humano isso aí né e é talvez o especialista no jogo ali não seja batido né mas na média tá muito superior Com certeza e bom aqui um alguns dos truques entre aspas né na de implementação ali que hã É algumas coisas dessas são comuns né a outros algoritmos de aprendizado por reforço também por exemplo né Toda vez que o agente faz uma ação em algum estado tem uma recompensa e atinge um outro estado ele armazena essas informações aqui num replay de experiências né para cada ação que ele tentou então ele tem um replay lá sei lá de 10.000 posições Então as 10.000 experiências mais recentes ficam lá nesse replay e aí na hora de atualizar a rede na hora de atualizar a rede ele coleta aquelas experiências lá então ele não precisa né interagir com o ambiente toda vez para atualizar a rede ele pode usar experiências que ele já viveu anteriormente para atualizar a rede de novo ele ter um ganho de tempo nisso aí né ele não precisa passar revisitar o mesmo estado duas vezes né ele pode simplesmente resgatar lá do replay de experiências Então isso é uma das ideias tá Outra ideia esse o target aqui versus comportamento

- *Corpus ID:* 4359
- *Score:* 0.8743401169776917
- *URL:* oculto
- *Início:* 00:48:26
- *Fim:* 00:50:49
- *Transcrição:* coletei é isso Ó o estado que eu tava a ação que eu fiz a recompensa que eu ganhei e o estado que eu cheguei tá eu vou usar isso para atualizar o meu minha estimativa E aí eu vou mostrar né qual vai ser essa regra de atualização daqui a pouco mas por enquanto eu tô só abstraindo ela né Ou seja eu coletei uma amostra aqui do ambiente uma experiência aí fiz lá minha atualização que eu ainda não mostrei como E aí o estado que eu atingi passa a ser o estado atual né o algorismo faz isso aqui O Agente né faz isso é é o que aconteceu com ele mesmo né o lugar que que eu cheguei é o lugar que eu estou né então agora eu tô pronto para repetir nesse novo lugar que eu tô vou escolher uma ação nesse novo lugar que eu tô observar a próxima recompensa e o próximo novo estado e aí Eu repito Isso eu aprendo ali com essa nova amostra e o estado que eu atingi é o ponto de partida da próxima ação Ok esse algoritmo nessa ideia aqui de treinar o a gente tá Tá Ok professor uma dúvida eu eu tenho sempre que ter num num numa rede dessa não sei se é rede que chama um um estado terminal né Não pode ser por exemplo um círculo né não é obrigatório tá o que o lan funciona né o aprendizado por refuso funciona mesmo que não haja estados terminais que é o caso até desse do Grid aqui ó esse estado onde a recompensa aqui é positiva ele não é terminal né O Agente sai dele e volta de novo por exemplo a gente pode sair ele tá aqui ó tem uma recompensa de um até porque vamos explicar por que que o valor aqui é 1.2 né Por quando a gente chega aqui e o jogo não terminou digamos assim ele pode ir para pras tentar né as quatro direções aqui ó é esquerda e aliás acho que esquerda não tá habilitado né e direit e para cima pra direita e para baixo e tentar se chocar contra parede aqui acho que ele pode e quando ele tenta se chocar contra a parede ele

- *Corpus ID:* 4233
- *Score:* 0.8734320402145386
- *URL:* oculto
- *Início:* 00:17:00
- *Fim:* 00:19:20
- *Transcrição:* outra né então ele isso aqui depois que ele já tá treinado né obviamente antes de de conseguir ser bem sucedido ele errou muitas vezes né então e olha que ele consegue né fazer os movimentos e pegar a bola jogada em várias eh posições né então tem um sistema de câmeras que rastreia a bola e diz né qual é e o robô aprende Qual é o o movimento certo a se fazer E aí depois ele depois de bem treinado ele ele consegue jogar né mesmo com pessoas que não eh não praticaram com ele né então eh a pessoa não jogava da mesma forma exata que ele foi treinado né mas como ele tá bem sucedido né ele aprendeu várias situações corretas ali então ele consegue eh gerenciar né o essa eh enfim aí Eh agora são outras Hã o robô fazendo caretas né quando a tá um pouco fora né da do aprendizado por reforço Mas é pro robô ter um pouco mais de aqui de eh fe né de reações né um pouco mais eh humanas aqui né Eh mas enfim dentro do aprendizado por reforço é a ideia de realizar movimentos né uma sequência de movimentos pensa que em cada micr movimento que tem que ser feito né na nas juntas do robô para ele conseguir ser bem sucedido nessa tarefa e é sobre isso que esse trata né E aí a gente tem vários exemplos de sucesso né de técnicas de aprendizado por reforço em aqui em jogos no caso né o alfa go é foi um sistema treinado e grandemente com aprendizado por reforço ou seja jogava contra ele mesmo né o sistema E aí descobria maneiras de vencer e conseguiu bater né o o melhor jogador humano considerado o melhor jogador né humano de GO e aqui também né um jogo de videogame que é super complexo um monte de coisa acontece na tela ao mesmo tempo e aí um são times que jogam times de cinco agentes e um time de cinco ias né esse aqui ó Open i5 conseguiu vencer cinco humanos nesse jogo e aqui também no Starcraft 2 é outro jogo Super complexo né que são dois jogadores mas


**Resposta gerada pelo LLM**: O agente recebe recompensa somente ao final da partida (vitória ou derrota).  Através de milhões ou bilhões de partidas, o agente aprende sequências de ações que levam à vitória, sem precisar de avaliação de jogadas intermediárias.  O poder computacional é crucial nesse processo.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address how machine learning can be used to train an agent to play a board game like Go, particularly in the context of receiving rewards only at the end of the game. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining that the agent receives rewards only at the end of the game, which is a key aspect of the question. It also discusses how the agent learns through playing millions or billions of games, which is relevant to the training process in machine learning. Additionally, it mentions the importance of computational power, which is relevant to the implementation of machine learning techniques in this context. Overall, the RESPONSE effectively covers the main points of the PROMPT.  Score: 3


---
