**Pergunta 136**: O que é o garbage in, garbage out?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 624
- *Score:* 0.8362791538238525
- *URL:* oculto
- *Início:* 00:12:59
- *Fim:* 00:15:18
- *Transcrição:* variância desse Ou seja eu tô imagido aqui alguma coisa do eixo X aí ele vai verificando se essa variância ela vai aumentando até chegar no último lá que ficou muito maior do que daquele primeiro bloquinho que ele pegou Tá certo entendi com isso não sabia disso não tá mas tem teste também tá tem o teste que faz isso de forma numérica tá aqui a gente vai só visualizar isso no gráfico precisa saber que geralmente é tranquilo olhar isso no gráfico a não linearidade né então se não for linear vai aparecer alguma coisa tipo quadrado tipo sei lá alguma coisa trocando de sinal é um polinômio alguma coisa subindo assim tortinha né pode ser uma exponencial logaritmo tá então aí tem uma outra relação que não linear são escadas você desenhar você colocar um escape deles aí você consegue visualmente de repente O que é isso né eu tô enganado de repente isso é você plota o plota os resíduos né O que é o que cada a posição dele que eu acho que você disse né que você é amostra cada cada cada mostra é o x e cada elemento e cada resido é o y Então você vai fazer um escape disso E aí você vê uma você vê mais ou menos do desenho que ele come picada eu não tinha entendido dispersão é isso vai fazer um gráfico de dispersão do resíduo aqui nesse eixo e aqui em mente quase todos esses gráficos tem um X aqui tá mas na verdade x do eixo X mas geralmente aqui vem o nosso y o chapéu O Valor estimado para Y Aí ele vai ficar aqui cruzado com os resíduos tá tudo bem pensei que era a posição da amostra tá aqui sim a posição da bosta tá você joga observação um até observação n perfeito porque se eu enxergar que é um ascendente uma descendente eu tô enxergando uma dependência nos resíduos

- *Corpus ID:* 6666
- *Score:* 0.8361718058586121
- *URL:* oculto
- *Início:* 01:25:23
- *Fim:* 01:28:03
- *Transcrição:* mas é que agora tipo se eu f tar aqui aparentemente funcionou essa parte então agora nós temos tarefas de 100 me e agora lançando coline [Música] regr tentar pegar papai acha que vai dar certo é ainda não deu ainda deu problema aqui na enfim mas esse seria a reação assim n seria talvez ir mudando CONSEG beleza obrigado profess mais alguém tem alguma pergunta mais alguém tem alguma pergunta Sim professor pode falar beron Opa eh numa situação dessa teria como a gente programaticamente eh intervir com alguma espécie de um garb um garb de collector uhum eh quando algum evento que seria por exemplo assim 80% da memória RAM atingida a gente agir programaticamente para ir e liberando espaço algo do tipo assim não jogar tudo para sei lá a responsabilidade pro hardware para GPU e a gente de certa forma ajudar esse tipo de de código programaticamente é eu acho que não porque tu tá programando em Python né e essas coisas são automáticas tu até poderia antes de fazer essa operação tu chamar o garb collector entendeu mas imediatamente antes para dar uma limpeza antes né Depois que chamar não tem como não tem como agir como é porque tu essa essa implementação make a regression provavelmente ela não está escrit em Python lá dentro do do do talvez tipo o dask lá talvez Tem uma parte em C mas talvez temha uma parte em Python E se for em Python vai criar um monte de tarefa não tem como tu lá dentro assim dizer assim Ah agora eu quero executar eh o garbate collector entendeu a menos que dentro desse grafo de tarefas tu coloc colocasse uma tarefa que tu que fez com Desk delayed por exemplo e essa tarefa o objetivo dela fosse chamar o garbage collector mas não não resolveria

- *Corpus ID:* 6667
- *Score:* 0.8335469961166382
- *URL:* oculto
- *Início:* 01:27:33
- *Fim:* 01:29:43
- *Transcrição:* talvez tipo o dask lá talvez Tem uma parte em C mas talvez temha uma parte em Python E se for em Python vai criar um monte de tarefa não tem como tu lá dentro assim dizer assim Ah agora eu quero executar eh o garbate collector entendeu a menos que dentro desse grafo de tarefas tu coloc colocasse uma tarefa que tu que fez com Desk delayed por exemplo e essa tarefa o objetivo dela fosse chamar o garbage collector mas não não resolveria entendeu porque chegaria uma hora que se tu tivesse tarefa uma quantidade de tarefas grande o suficiente ainda sim tu chegaria No Limite entendeu ah outra coisa Professor É que eu achei bastante interessante aí essa animação do da GPU Memory né dá pra gente ver quão quão robusto é esse tipo de processamento né quão é é o negócio é como se diz aqui no no Nordeste é Rochedo mesmo é coisa é tipo assim o ele fica monitorando né GPU ali e tal e aí tu consegue acompanhar e ter uma ideia assim de como é que tá indo eu acho que na real o mais útil mesmo é o tesk stream que é no tesk stream tu consegue ver então o que que tá efetivamente acontecendo e tal e nesse caso aí ó a gente poderia até lançar a execução de novo ó e eu poder ficar olhando o tesk stream aqui eu não sei o que tá acontecendo mas ó vai tu vai vendo as tarefas acontecendo E aí por exemplo quando dá falha tu pode ver em qual tarefa ele falhou entendeu por exemplo aqui já já deve ter fal Ah não aqui eu tava só construindo por exemplo lançar o o a regressão vê a regressão acontecendo e vê Em que momento vai falhar Talvez tu consiga através dessa identificar exatamente o porquê da falha Entendeu Tá mas a falha vai vir com um bloco em outra cor porque a gente só vê ali amarelo e um pouquinho de verde é então né É aqui acho que já falhou mas para mim Professor particularmente o mais revelador foi o GPU Memory aquele ali à direita com aquela animação achei

- *Corpus ID:* 3814
- *Score:* 0.8322111964225769
- *URL:* oculto
- *Início:* 00:34:24
- *Fim:* 00:36:41
- *Transcrição:* distante vai colocar naquele que é mais próximo esse aqui não esse aqui como eu estabeleço um raio uma distância máxima e uma quantidade mínima de elementos alguns como vocês estão vendo nessa figura que eles vão ficar de fora então ele lida naturalmente com alto Lais ele exclui esses elementos do resultado então isso isso pode ser uma vantagem porque os Outlets eles não são agregados em classes de maneira incorreta por essa por essa definição do que é correto ou não mas por esse motivo também esses elementos eles não vão estar alocados a cluster nenhum então no resultado né quando vocês pedirem para ele dizer qual é o Label resultante né alguns elementos vão vir sem Label quer dizer ele não achou nenhum câncer para eles eles são outlast então vocês vão ter que filtrar o resultado e pegar esses que não tem Label e realmente avaliar a inscrição outlares ou se eles foram mal classificados E aí vocês podem manualmente alocar um cluster então é uma vantagem mas pode ser uma desvantagem e o DBS cantam é o algoritmo mais famoso e mais rápido dessa família e eu trouxe aqui algum conjunto de slides para tentar explicar para vocês como ele funciona então Imaginem que eu gerei eu tenho um conjunto de dados com essa configuração espacial não vejam que nós podemos perceber aqui que tem alguns elementos que talvez sejam out layers não é esses aqui mais mais afastados e existe aqui uma concentração de elementos que provavelmente possuem compartilham alguma característica e poderiam ser colocados em um aglomerado então e outro aqui em cima e eventualmente tem algum cluster aqui aí depende de nós nós quisermos que um mínimo seja três elementos e se esse esse aqui seria considerado um cluster ou nós podemos dizer que o mínimo são

- *Corpus ID:* 625
- *Score:* 0.8312342166900635
- *URL:* oculto
- *Início:* 00:14:44
- *Fim:* 00:16:56
- *Transcrição:* um X aqui tá mas na verdade x do eixo X mas geralmente aqui vem o nosso y o chapéu O Valor estimado para Y Aí ele vai ficar aqui cruzado com os resíduos tá tudo bem pensei que era a posição da amostra tá aqui sim a posição da bosta tá você joga observação um até observação n perfeito porque se eu enxergar que é um ascendente uma descendente eu tô enxergando uma dependência nos resíduos ou seja correlação que não pode ter tá você fica tem que chegar a gente é isso a coisa mais linda do mundo aqui é um quadradinho ou seja esse centro aqui é o zero eu quero essas coisas espalhando de forma aleatória em torno do zero ou seja ficou só aleatoriedade ruído branco já ouviu falar disso então há muito tempo atrás mas a ideia é essa sabe é algo que se comporta de forma aleatória sem padrão sem tendência porque é isso que eu espero que o que tinha de variabilidade a ser explicada o modelo explicou o que sobrou são as outras variáveis que não são importantes e por isso não estão no modelo e não vai ter nenhum tipo de padrão aparecendo aqui tá gente linhas Gerais é isso Faltou um pressuposto que eu não falei aqui que é o danormalidade tá então a normalidade eu posso Verificar se os meus resíduos são normais como é que eu avalio isso a partir dos gráficos tipo um histograma com o que que é plot principalmente melhor tem os pontinhos e tem aquela reta que a absorção normal padrão né teórica e eu quero que meus potinhos estejam perto né flutuando em volta dessa reta ou eu posso fazer um teste tipo que a gente já viu alguns exemplos anteriores chapiro Wilker pega os resíduos e aplica o teste tá Ok pode aplicar o seu peru que quando tem mais de sei lá 30 sei lá sem amostras

- *Corpus ID:* 664
- *Score:* 0.8311958312988281
- *URL:* oculto
- *Início:* 01:23:32
- *Fim:* 01:25:40
- *Transcrição:* ele não é ele é um outline ele tá considerado como outline aqui ó porque passou né de três desvios quase quatro desvios E aí tem que avaliar a presença dele que ele pode estar distorcendo nos nossos dados de resto o negócio tá bem bonitinho né aceitando na qualidade e aqui ó então só um exemplo aqui estão os meus resíduos padronizados e aqui os valores ajustados que eu falei para vocês Tá e isso aqui é que tem que ter uma flutuação lá como se fosse aquele retângulozinho veja falar o cinco problemático de novo ou cinco palavras um probleminha talvez tem que ir atrás de entender esse cinco mas de resto tá tudo muito bonitinho aqui ao longo do valores ajustados não tem nenhuma tendência subindo não tem nenhuma tendência descendo não tem nenhuma curvatura não tem o funil né começando com variabilidade pequena e aumentando a variabilidade a medida que passa os valores a auto correlação a autocolação eu teria que avaliar aqui né Se isso for realmente o primeiro e lá foram o último lá último né foram o último eu vou colocar em ordem do 1 até o 163 que eu acho que era isso que tinha né em ordem de tempo aqui para avaliar se os meus resíduos não vão subir ou descer e mostrar uma correlação entre os resíduos desculpa gente passei muito muito muito do tempo tá depois aqui tem mais um gráficozinho que vai mostrar um gráficozinho que vai mostrar para vocês aqui tipo aquele que a gente viu ali das condições olhando esse modelo né ajustado comparando aqui os condição nova acho que é né condição de novo e usado a ordem tá aqui ó aqui ó então resíduos ao longo dos valores observados estão vendo que não tem nenhum padrão de subida de descida nada disso tá lá o bonito do cinco lá incomodando só lá em cima tá

- *Corpus ID:* 619
- *Score:* 0.8311160802841187
- *URL:* oculto
- *Início:* 00:04:57
- *Fim:* 00:06:50
- *Transcrição:* que a gente não tinha falado Ainda não mas é outro pressuposto é a ausência de auto correlação nos resíduos isso a gente garante se os nossos indivíduos são os nossos n né os nossos dados sejam Independentes né então eu não posso ter medidas sei lá é um mês depois do outro né tipo da academia que as nossas medidas mês a mês tem que ser Independentes entre si tá não pode ter relacionamento entre as medidas ou seja não é porque no mês passado eu gastei mais né na academia com alguma coisa que o mês que vem eu tenho mais chance de gastar mais ou mais chance de gastar menos né então tem que ser resultado independentes e a gente enxerga isso ao plotados resíduos e também não vê nenhum tipo de padrão nos resíduos tá porque isso mostraria algum tipo de relacionamento tá eles aumentando ao longo do tempo né ou seja cada medida que passava aumentando os resíduos mostrando tanto que tem uma relação positiva entre as nossas unidades nossos indivíduos não pode fazer isso isso não tem mais nem correção tá ou seja isso tem que ser um cuidado na hora de retirar a amostra que os nossos as nossos valores realmente sejam Não correlacionados na verdade até tem tá só que daí são modelos bem mais complexos em que a gente avalia os relacionamentos entre as medidas tá tem mas bem mais complexo a gente também não vê aqui tá ou você da cidade dos resíduos eu não posso por exemplo valores menores eu tenho uma variância menor e valores maiores eu tenho uma variância maior E aí eu vou enxergar os meus resíduos como se fosse um funil Eles começam com uma variabilidade pequenininha e vai aumentando a medida que eu vou aumentando né passando andando no eixo dos X as pessoas chegar tudo isso funciona nos gráficos tá então ele tem que ser tem que ser homogêneos quanto a variabilidade desses resíduos e a ausência de multicolinidade nas

- *Corpus ID:* 6634
- *Score:* 0.8301418423652649
- *URL:* oculto
- *Início:* 00:30:19
- *Fim:* 00:32:26
- *Transcrição:* tá mas tem todos esses outros aqui tem até do read clipboard que é tu faz contrl c e depois tu faz read clipboard né eh Mas aí tem por exemplo capítulo sete data cleaning preparation que maior parte do tempo quando a gente vai analisar dados a gente gasta com limpeza desses dados porque normalmente os dados que a gente pega eles são meio tem coisas estranhas dentro a gente precisa entender bem as todas as coisas estranhas para não gerar conclusões erradas né então é na na disciplina de aprendizado de máquina a gente viu isso na prática que a gente gastava perto de 90% para limpar a nossa base isso para poder depois e processá-la isso E aí Aqui tem os métodos de como é que a gente limpa como é que a gente entende Quais são os nas que que a gente faz com esses nas que é são informações faltantes né como é que remove como é que verifica se existe enfim tem uma série de coisas que dá pra gente fazer aqui tem vários exemplos ó feeling in Miss data isso aqui extremamente perigoso fazer isso dependendo do cenário né tipo assim tu tem lá uma dado faltante tu quer colocar um dado lugar ali será que isso faz sentido entendeu ver como é que isso funciona né para olhar eh e e tomar decisões importantes ali na na hora da preparação né antes de fazer qualquer coisa então todas essas operações fio na aqui Peg pegi um exemplo aqui aqui né tipo sei lá qualquer outra operação aqui drop duplicates que a gente já viu que a gente inclusive já usou essas operações todas elas já estão paralelizador a gente acaba é muito mais importante como como acho que foi o Denis que resumiu né ficar bem prolífico nessa api aí porque depois existe uma correspondência com com rapids aqui no caso por que que eu eu lancei essa discussão porque eu queria entender se eu preciso continuar explicando cada um dos elementos aqui entendeu para vocês eu sei que a gente tá meio que talvez na

- *Corpus ID:* 899
- *Score:* 0.8289528489112854
- *URL:* oculto
- *Início:* 01:06:59
- *Fim:* 01:09:18
- *Transcrição:* espaço vão sair regras essa é uma estratégia frequente quando eu tenho muitos itens vocês imaginam o supermercado que tem lá o parafuso de furo x o parafuso de furo Y parafuso de comprimento Z o parafuso de comprimento H é o parafuso com cabeça parafuso sem cabeça né então muitas vezes o que eu quero saber é se itens de ferragens são comprados juntos com frutas e se eu encontro essa relação aí eu detalhe essa relação sabendo bom que tipo de fruta junto com tipo de item de ferragem vale a pena para eu botar esses itens de ferragem ali junto das frutas ou eu botar sua unha em promoção né então exemplificando por exemplo besta mas basicamente é assim que a gente lida tem alguns algoritmos antigos da fique justamente fazem isso eles funcionam com valores generalizados a gente dá uma hierarquia para eles eles funcionam valores generalizados e se Eles encontram padrões generalizados assim baa com limão aí eles vão ver se algum tipo de limão específico algum tipo de baa específica Então esse é um outro jeito de lidar com a espasticidade e tudo isso gente é o que hoje esse convencional chamar de engenharia de dados são transformações que eu tenho que fazer outras coisas que eu tenho que me preocupar com a priori é ele só funciona com dados categóricos tem algumas implementações acadêmicas que funcionam para dados de intervalos mas elas não vingaram apesar da sua utilidade o o priori só funciona com dados catagóricos a coisa que eu mais encontra é aluno querendo aplicar coisas em dados que são numéricos pega salário pega o tamanho do pé pega Sei lá o número de filhos é a gente tem que discretizar porque ele não funciona a menos que eu tenha um algoritmo que funcione para isso não é especificamente tem um parâmetro lá que a gente diz encontro os valores de discretizar esses valores e aí você sai com

- *Corpus ID:* 4443
- *Score:* 0.8286195993423462
- *URL:* oculto
- *Início:* 01:31:51
- *Fim:* 01:33:59
- *Transcrição:* aquelas experiências lá então ele não precisa né interagir com o ambiente toda vez para atualizar a rede ele pode usar experiências que ele já viveu anteriormente para atualizar a rede de novo ele ter um ganho de tempo nisso aí né ele não precisa passar revisitar o mesmo estado duas vezes né ele pode simplesmente resgatar lá do replay de experiências Então isso é uma das ideias tá Outra ideia esse o target aqui versus comportamento ele tem uma rede que controla o agente Então essa rede fica ela não é treinada né E outra rede ela hã digamos assim ó ele começa controlando a o agente com uma rede E aí toda vez que tem um erro aqui ao invés de atualizar essa rede ele atualiza uma cópia dela n atualiza uma cópia dela e vai essa rede digamos a rede cópia vai sendo atualizada E aí de tempos em tempos ele substitui a rede que controla o agente né a cópia passa a controlar o agente E aí ele né continua atualizando em cima aqui e aqui embaixo tá mais estável né Essa é a ideia tá a cada eh intervalo de tempo lá eh essa rede que controla o agente ela demora mais para trocar né ele vai atualizando aqui Vai acumulando as atualizações aqui numa outra rede cópia E aí depois usa né Essa Rede Mais modificada para controlar o agente essa é uma ideia para ter um pouco de estabilidade né que se ele atualizasse a mesma rede que controla o agente poderia ficar muito errático Talvez né então ele acumula uma série de atualizações aqui em cima de tempos em tempos vai lá e faz uma troca né pro pro agente hã para ter essa estabilidade Então esse é outro truque aqui de implementação outra coisa muito importante tá em especial no Breakout ali a gente vê ó que e eu tô chamando aqui de empilhar né os frames anteriores porque por exemplo aqui ó Nessa situação né a gente sabe se a bola tá Subindo ou


**Resposta gerada pelo LLM**: Não é possível responder com o contexto fornecido


**Anotação manual**: I

**Answer Relevance (AR)**: 0.00

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. A RESPONSE that does not answer the question or provides no relevant information should receive a score of 0. Supporting Evidence: The RESPONSE states that it is not possible to answer with the context provided, which indicates a refusal to engage with the PROMPT. This does not provide any relevant information about the concept of "garbage in, garbage out," nor does it attempt to explain or define it. Therefore, it fails to address the PROMPT entirely.  Score: 0


---
