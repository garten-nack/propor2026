**Pergunta 151**: O que são tensores variáveis e como eles funcionam?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 6825
- *Score:* 0.8492540121078491
- *URL:* oculto
- *Início:* 01:08:11
- *Fim:* 01:10:11
- *Transcrição:* exemplo mas aqui seria uma tipo assim um mecanismo mais simples Ok bom pessoal então agora a gente vai voltar então pro Firefox aqui paraa d03 para dar andamento antes da nossa essa pausa só para falar rapidamente então sobre eh tensores e operações Ok eh que é nossa próxima atividade dirigida tá então o objetivo dessa atividade didática só a gente entender como é que funciona esses tensores do tensor Flow assim que é basicamente um arranjo multidimensional muito parecido com o num p e pra gente saber assim tipo como que a gente pode usar esses tensores para se eventualmente a precisar modificar alguma coisa certo ã dentro de um modelo fazer alguma alteração lá dentro cer e eh isso claro normalmente a gente não faz mas é importante a gente saber como é que funciona essa essa parte mais básica de tensões Ok então pros para esse para esse aqui a gente vai então Eh importar o tens Flow e importar o nump também pra gente ver por exemplo correlações que existem essas entre essas duas ferramentas S vou aguardar aqui a instanciação pronto eh e vou terminar a conclusão aqui da do da carga e para pros tensores Então a gente vai basicamente fazer então só essa parte de da da criação indexação operações e usando o backend do keras como é que a gente faz para vir partido do pai e algumas questões sobre tipos conflitantes que não são convertidos implicitamente e tensores variáveis Ok porque tensores normalmente são constantes eh então assim D criação né a gente pode criar tensores a partir de valores de uma constantes de uma matriz nesse caso caso que a gente passa uma matriz usando essa sintaxe do Python E aí o que que a gente observa quando a gente executa esse código quer dizer que a gente tem Ah uma um shape notem bem a semelhança com lpie né o tipo de dado é float 32

- *Corpus ID:* 6839
- *Score:* 0.8425372242927551
- *URL:* oculto
- *Início:* 01:31:05
- *Fim:* 01:33:16
- *Transcrição:* Inclusive a gente instancia com TF constants aqui tão vendo mas a gente pode ter tensores variáveis também tá eh nesse caso daí a gente não vai instanciar com TF P Constant que são constantes Inclusive imutáveis a gente não consegue mudar então fixos né mas durante o treinamento se peguem eh qualquer modelo que a gente já treinou até agora na disciplina n disciplinas anteriores vocês sempre vão trabalhar com tensores que vão sendo atualizados ao longo da execução a gente não quer que ficar criando tensor imutável né ao longo de um treinamento né mas para isso então a gente vai ter um custo a maior né para emprego disso aí a gente tem que usar então um um Construtor diferente que é o TF pver com esse V aqui maiúsculo tá diferente do constante que o c é minúsculo E aí nesse caso a gente vai ter então um objeto que é diferente dos objetos anteriores ó anteriormente a gente tinha TF P tensor como tipo do objeto agora a gente vai ter um TF P variable Esse vai ser o tipo onde aqui vai ter o nome dessa dessa variável vai ter o o resto é tudo igual o shape o dtype e qual que é o objeto interno né então esse tensor dito variável né deixa eu corrigir aqui português ele funciona como um tensor constante tem todas as mesmas operações A diferença é que a gente pode fazer assign nele a gente pode mudar ele Então nesse caso aqui a gente tá pegando esse tensor V que a gente acabou de instanciar variável né a gente tá multiplicando eh por dois nota que aqui acho que tem a ver com aquela pergunta a gente tá fazendo dois vezes e aí aqui é uma multiplicação simples vamos dizer assim Element Wise tá não vou nem me me não vou não vou usar o termo multiplicação de matriz porque não é tá eh simplesmente tu pegar uma matriz toda ela propagada para ser 2 por 3 também cheia de do e fazer a multiplicação Element Wise com v Ok E aí nesse caso a resposta dessa operação ela é a a aabs

- *Corpus ID:* 7021
- *Score:* 0.8415356874465942
- *URL:* oculto
- *Início:* 00:13:22
- *Fim:* 00:15:35
- *Transcrição:* porque a gente chama de operações Raízes que são essas folhas que aparecem no grafo eh que elas não dependem de ninguém seria assim a origem né de uma informação então notem que aqui nós no nosso gráfico de tensores nós temem temos nós que na realidade dependem de outros nós então esses não são raiz porque eles precisam eles eles têm uma dependência eles precisam primeiro resolver a dependência deles para eles poderem ser executados então por exemplo o f aqui pro F executar eu preciso ter o abde então o f eu não consigo começar por ele já o a o b e o c posso começar por eles porque não Eles não dependem Depende de ninguém né então o que que ele vai fazer ele vai detectar essas operações raízes e vai escalonar isso né ou ou em CPU ou em GPU em função da da da disponibilidade né E aí o que que acontece acontece em do uma operação de eh eh entre trads e intra trads também então essa essa esse essa operação esse esse a por exemplo ele vai ser enfilado isso vai ser executado eh no numa trad e o que pode acontecer é que essa operação ela é quebrada mais uma vez né para ser executada por mais trads digamos assim a gente precisa de uma aceleração Extra como se a gente usasse mais de uma trade para fazer uma tarefa a chama isso de tarefa eh com múltiplas trads Ok e do lado da GPU a mesma coisa acontece né a gente tem um enamento de tarefas no caso a c aqui foi mapeada para executar na GPU E aí tem a c dnn lá dentro né que vai receber essa tarefa e vai fazer o que tem que fazer lá dentro eventualmente quebrando também em pedaços né vai usar os vários tensor cores ou os vários cud acores Então existe essa ideia de duas filas né Especialmente na na na na tanto em CPU quanto em GPU E aí eh essa primeira fila é a fila para tarefas do grafo então a gente en filera e depois a outra fila é pros pedaços de uma tarefa né então existe esses dois tipos de paralelismos né seria o paralelismo

- *Corpus ID:* 6702
- *Score:* 0.8385221362113953
- *URL:* oculto
- *Início:* 00:42:10
- *Fim:* 00:44:26
- *Transcrição:* assim de quão básico é esse negócio e esse canal é um cara que faz vídeos de matemática Ele criou Então esse essa playlist sobre neural Networks e O legal é que é super visual assim aparece só a voz dele né super visual ele cria programas em Python para representar a a ideia certo então aqui por exemplo el tem quatro capítulos Ele explica o que que o que que é uma rede neural então né Bem bem assim elementar digamos assim para qualquer pessoa mundana não precisa nem ter um background de computação conseguir entender mas logo em seguida tu precisa um background um pouco matemático mas ele explica os detalhes que é como que as máquinas aprendem né O que que é a decida do Gradiente O que que significa ok então e como que a gente usa né como é que a gente implementa essa descida do Gradiente que é o aprendizado da máquina ok que na realidade é uma função ali que tu vai querer descer na função para achar o ótimo digamos assim e o mecanismo que que que tá por trás disso é o back propagation certo que é exatamente o que o treinamento faz né então o treinamento na realidade é uma de operações de forward back propagation Então como que isso funciona e aí ela termina só para vocês terem uma ideia com esse último capítulo aqui que é o cálculo do back propagation Qual que é a operação de álgebra linear que é feita para que aconteça o cálculo do back propagation E aí lá tá multiplicação de matrizes soma de vetores que essa operação fundamental Então as gpus né da NVidia né só para abrir um par rápid que a gente já usou na disciplina anteriores A a NV hoje se vende como uma empresa de Inteligência Artificial Porque nas placas eles têm os tensor cores eles TM os cores normais os cuda cores e eles têm os tensor cores o que o que que é esse tensor Core é basicamente multiplicação de matriz e vetor feita num passo de tempo então para tu usar eles tem que usar lá

- *Corpus ID:* 6351
- *Score:* 0.8380411863327026
- *URL:* oculto
- *Início:* 00:29:54
- *Fim:* 00:31:53
- *Transcrição:* representados pela cor branca aqui na nossa entrada mas também esses esses dados marcados em azuis que são as bordas que a gente não que esse que as threads desse bloco não são responsáveis por calcular mas precisam daquele dado para calcular então esses limites aqui né no caso a posição zero a posição 1 e a posição dois Ok e também as últimas posições aqui né então no Passo dois a gente calcula os elementos da saída né então esses são representados por essas Flechas pretas e aqui na cor rosa tá representando Qual é o raio mediação dessa primeira trad que tá calculando essa posição zero aqui do vetor de saída Então a gente vai precisar ler todas essas posições para calcular a média das sete posições e calcular esse vetor de saída E aí depois que a gente fez esse cálculo né esse cálculo ele vai estar eh eh representado num vetor talvez local né e a gente precisa Então levar esses dados paraa memória Global Então para que a saída né esteja efetivamente na memória Global porque é somente da memória global que que o anfitrião né a máquina host consegue copiar os dados essa memória compartilhada ela acessível de do anfitrião Então como que a gente faz isso né a gente vai primeiro eh marcar então o nosso estêncil 1D ên 1D recebe a nossa entrada e a nossa saída a nossa entrada aqui lembram bem é uma referência para os dados que estão na memória Global da placa a mesma coisa essa saída E aí nós vamos ter então uma variável temporária que é representando por essa variável temp aqui que ela vai ter o tamanho do bloco mais duas vezes o raio Para justamente se preocupar com essas bordas né E ela é marcada então com shared para dizer então que essa variável temporária a gente quer que ela fique não em um registrador ela a gente quer que ela fique na memória compartilhada OK depois o que que a gente vai ter então a a gente vai ter um

- *Corpus ID:* 6840
- *Score:* 0.836502730846405
- *URL:* oculto
- *Início:* 01:32:40
- *Fim:* 01:35:00
- *Transcrição:* por dois nota que aqui acho que tem a ver com aquela pergunta a gente tá fazendo dois vezes e aí aqui é uma multiplicação simples vamos dizer assim Element Wise tá não vou nem me me não vou não vou usar o termo multiplicação de matriz porque não é tá eh simplesmente tu pegar uma matriz toda ela propagada para ser 2 por 3 também cheia de do e fazer a multiplicação Element Wise com v Ok E aí nesse caso a resposta dessa operação ela é a a aabs para V Ok então como a gente sobrescrever daí os valores de v Então porque eles são mutáveis esses valores né então eu executei uma vez se eu executar de novo executar de novo percebam bem que o v vai mudando tão vendo ali os valores e aumentando ó porque a gente tá mudando o v ok a gente pode também modificar fatias usando o mesmo regramento de fatiamento do nump Então nesse caso aqui a gente pega V not tem bem aqui o V ó a se eu faço v01 eu tô pegando só um determinado elemento nesse caso aqui é o 512 que é a linha zer a coluna 1 né eu posso fazer um assign naquela naquela posição ou seja um ass de uma única posição do meu do meu tensor variável E aí eu ponho 42 lá no lugar do 5 por exemplo posso fazer também assign de vetores então um pouco mais sofisticado também a mesma coisa pode falar Jan qual seria o uso da Constante então iia só usar a variável a constante é assim para um uso assim de de de operações a gente usaria mais eh assim tipo fazer transformações né se a gente quer fazer in Place a gente usa variáveis né mas as constantes são úteis também notem nota bem que tu pode fazer operações também com com as constantes né que nem a gente fez aqui né Deixa eu achar aqui os exemplos aqui ó a gente fez operações também T + 10 levou elevou ao quadrado só que aqui nesse caso a cada execução tu tá criando um novo objeto entendeu tu não tá fazendo inl né

- *Corpus ID:* 6989
- *Score:* 0.8344425559043884
- *URL:* oculto
- *Início:* 01:14:03
- *Fim:* 01:16:12
- *Transcrição:* que são todas as variáveis explicativas oriundas dessa linha vai vai vai remover a média desse valor e vai dividir pelo desvio padrão então com isso a gente faz uma normalização baseada no desvio padrão e a gente retorna então uma tupla vejam bem a primeiro elemento dessa tupla é um vetor com as variáveis explicativas e o segundo elemento é o y que é a variável de resposta do nosso aprendizado supervisionado Ok Então essa é a função de pré-processamento do usuário alguma dúvida pessoal estão absurdamente silenciosos é eu tava esperando professor que em algum momento a gente I iria aplicar aquele né a função né mas agora ficou Claro ali que foi feito manualmente nessa função tu diz o a a normalização isso a normalização usando algum método lá do standard scaler mas isso a gente é lá a gente a gente é a gente tá fazendo essa normalização aqui de um jeito talvez não muito possível de fazer né porque para eu obter esse valor médio de todas as colunas né que a gente viu antes né eu precisava da integralidade dos dados né imagina como é que tu vai tem 50 GB de dados né como é que tu vai obter essa essa ess essa média né para tu fazer essa normalização vai ser bem Custoso né Mas tu vai ter que fazer isso de maneira prévia ainda mas isso é é o pré-processamento que tu tem que fazer nos seus dados antes de fazer o treinamento tu vai fazer uma vez só né também né tu vai testar depois vários modelos tu vai fazer essa essa essa normalização uma única vez bom então agora só testando né Essa função de pré-processamento né a gente tá passando aqui então uma string os nossos nove Campos e Podem perceber que ele retorna uma tupla onde o primeiro elemento dessa tupla é esse aqui de cima é um tensor com oito elementos e o segundo elemento eh é um um tensor escalar com a variável de resposta variável do nosso aprendizado de

- *Corpus ID:* 4454
- *Score:* 0.833968997001648
- *URL:* oculto
- *Início:* 00:05:48
- *Fim:* 00:08:06
- *Transcrição:* ambiente que existe naquele né nessa biblioteca aí a gente pode olhar aqui nessa nesse repositório né e e vê as as eh a descrição dele hã por exemplo Tá o que que é o Como é que é o que que a gente consegue observar nesse ambiente são quatro valores aqui né não quer dizer que é um número quatro tá é o número de coisas que a gente tem que observar que são a velocidade do carro e tem a descrição dos ângulos né embaixo Aqui a gente vai ver melhor e as ações são e é um espaço discreto de ações e São só duas ações né ação de de empurrar o carrinho pra esquerda e pra direita é só isso tá e ou seja não tem aceler ação aqui a ação ela Ele vai pra direita no máximo e vai pra esquerda no máximo também ã e todas as esse High e low aqui são os Opa teve uma falha ali depois a gente olha no colab esse High aqui é o os maiores valores tá pro pro para cada um do dos dos quatro números da observação então o primeiro número o maior valor é 4.8 o segundo número número é infinito o terceiro número é 0.42 e o quarto número é mais infinito também e os low aqui quer dizer que cada um dos quatro números tem né esses valores aqui são os mínimos deles - 4.8 menos infinito menos p42 menos Infinito ou seja se eu quiser saber né o primeiro a primeira grandeza aqui da observação ela vai de 4.8 ou até -4.8 né a segunda de mais infinito até menos infinito assim por diante ã e aqui a descrição das ações né ação número zero é empurrar pra esquerda ação número um empurrar pra direita ou seja as duas ações estão descritas aqui e as observações ó primeiro quando a gente né observ o estado desse desse ambiente aqui tem quatro é um vetor com quatro elementos o primeiro elemento né no índice zero é a posição do carro no índice um a velocidade no índice dois o ângulo da da vareta né da vassourinha e no três a

- *Corpus ID:* 3788
- *Score:* 0.8334605097770691
- *URL:* oculto
- *Início:* 01:18:33
- *Fim:* 01:20:51
- *Transcrição:* e identifica também essas quebras desse acoplamento E aí onde há essa quebra que ele ele faz o corte entre os classes e para fazer isso né imagine com muitas dimensões montar gráficos é meio complicado então isso todo uma matemática baseada em Matriz né então cria matrizes de singularidade entre os elementos para poder identificar essa essa vizinhança e essa Matriz então é decomposta não é então posso ter várias dimensões e ele tem uma matriz tem só duas dimensões E aí usa aquela técnica de cálculo de Alto valores que vão representar os elementos não é numa dimensão menor não é uma quantidade dimensões menores e por isso então espectral ele cria esse impedem essa representação do espectro de Vizinhança em duas dimensões somente E aí ele ele usa esse aí todas as técnicas de Matriz decomposição de matrizes para poder fazer a identificação então dessas desses trechos que são mais acoplados e essa separações um trechos que são menos acoplados eu vou depois colocar alguns vídeos e materiais para vocês que são interessantes de vocês olharem né para entender melhor como funciona isso né mas eu trouxe um dos vídeos ele tem uma explicação bem interessante de como essa essa análise não é de usando os alto-falantes autovetores funciona então imagine aqui do desse lado esquerdo pegar o apontador esse lado esquerdo aqui alguns elementos que foram onde foi calculada não é a vizinhança entre eles se montou o gráfico né de acoplamento e vejam que existe um outro conjunto aqui embaixo também de elementos que são altamente acoplados eventualmente Existe alguma conexão entre esses dois grupos Mas ela é fraca né porque além das distâncias serem

- *Corpus ID:* 6353
- *Score:* 0.8328906893730164
- *URL:* oculto
- *Início:* 00:32:58
- *Fim:* 00:35:05
- *Transcrição:* zero mas na realidade essa trad aí ela tem que ser responsável pela posição do raio então é 0 1 2 e 3 ela deve ela deve ser responsável por essa posição então por isso que o local index ele tem esse mais o raio aqui de ação Ok para que a trad ela comece efetivamente deslocada né de rádios vezes pra direita né então para que a gente faça com que o que essa trade vai calcular seja realmente o miolo do bloco temp que vai estar na memória compartilhada isso tudo porque a gente tem esse raio de ação né que a gente não quer calcular a gente só quer ler ok a gente só quer ler essas áreas vermelhas a gente não quer calcular o novo valor delas até porque nem daria para fazer isso porque a gente precisaria de uma posição que nem vai estar aqui ok tanto do lado esquerdo quanto do lado direito depois que a gente fez Então essa locação da variável tempo e e o cálculo dos índices a gente precisa procar Então copiar os dados da memória Global paraa memória local Então isso é relativamente simples porque a gente vai simplesmente pegar então do nosso vetor eh que está da nossa referência que está na memória Global a gente vai usar o o o índice global e vai copiar essa posição para variável temp no índice local então isso tá sendo representado aqui ó por essas Flechas eh aqui certo então cada uma das trads vai fazer a cópia dentro desse bloco né a gente tá falando aqui só no universo de um bloco que é quem tá executando esse código aí a gente sempre tem que pensar assim que é o o código daquele nosso kerno executado por um bloco claro que a gente vai ter um monte de bloco Tá mas a gente imagina que tem só um a gente vai executar eh as as todas as trads vão fazer essa cópia ao mesmo tempo da memória global para essa variável temporária que está na memória compartilhada do SM então isso acontece simultaneamente e eh A grande questão que surge é que pra gente efetivamente


**Resposta gerada pelo LLM**: Tensores variáveis, diferente dos tensores constantes (imutáveis), podem ser modificados.  Eles são instanciados com `tf.Variable` (com 'V' maiúsculo), ao contrário de `tf.constant` (com 'c' minúsculo).  Possuem as mesmas operações que os tensores constantes, mas permitem o uso de `assign` para alterar seus valores, inclusive fatias, permitindo modificações in place.  As constantes são úteis para transformações, mas criam um novo objeto a cada execução, enquanto as variáveis modificam o objeto existente.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining what tensors variáveis (variable tensors) are and how they function. It distinguishes them from constant tensors, describes their instantiation using `tf.Variable`, and explains their ability to be modified through operations like `assign`. This comprehensive explanation covers both aspects of the PROMPT, providing a clear understanding of variable tensors and their functionality.  Score: 3


---
