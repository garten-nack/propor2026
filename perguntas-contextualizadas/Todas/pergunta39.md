**Pergunta 39**: O que é tokenização?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 3980
- *Score:* 0.8556876182556152
- *URL:* oculto
- *Início:* 00:12:45
- *Fim:* 00:15:03
- *Transcrição:* vocês decidem que o esquema de organização é outro tem que ser adaptadas para corresponder a isso ou talvez não façam sentido só que hoje elas fazem mais sentido quando o top é uma palavra e assim essa unidade Token para quem trabalha com processamento da língua natural ela não é simplesmente o termo né não é simplesmente a sequência de caracteres que constituem uma palavra os tokens eles têm tipos também né então eu posso um nível mais simples dizer que os tokens são são palavras e aí tudo que que ele encontra que separado por espaço é uma palavra e eu posso dizer que os pontos são irrelevantes Mas eu posso dizer para ele também olha para mim é importante porque eu quero simpaticamente manter isso manter os símbolos também então ele vai me dizer olha isso aqui é um token parentes É um toque é um parente do tipo específico que significa né início de uma frase ou início de alguma de alguma coisa especial né ele pode dizer que vínculo é um toque um toque de separação de frases né um determinado parágrafo Ah eu posso dizer que que não tem nenhum número aqui eu posso dizer que um número é um toque no médico e aí na hora de analisar o texto pode ser importante o diferenciar os tipos de toques né então eu posso onde eu quero chegar eu posso ter de tipos diferentes de tokens alguns são tokens estruturais Alguns são tokens que dizem o tipo da palavra né uma palavra é o número posso ir adiante ter um tokenizador mais inteligente que diz olha isso aqui é uma é uma deixa eu pegar aqui algum exemplo mais uma entidade né eu posso tirar isso aqui é um nome de uma pessoa né então identifique se é um toque em pessoa se é um toque em lugar se é um toque em data Então ele pode ser bastante complexo Então toca representa essa unidade de análise e aqui nós vamos usar o tipo mais simples de organização que só separa as palavras a sequências né

- *Corpus ID:* 3999
- *Score:* 0.8550693988800049
- *URL:* oculto
- *Início:* 00:47:05
- *Fim:* 00:49:27
- *Transcrição:* que que eu fiz isso porque eu quero agregar ao processo de tokenização a identificação dos radicais né dos sistemas então é uma função Não muito complexa né que ela antes de depois ela ter identificado o token Eu usei o mesmo código ou como base o código original do tocanizador eu pedi para ele para depois de fazer a identificação do Token ele processar o sêmero dele então ele vai retornar a versão extermizada do Token que foi identificado então isso já tá integrado aqui nesse script E além disso eu peço para ele remover assentos digamos assim ele vai devolver o código em Asque não em um nicode já tirando as variações todas E aí cria o Data Frame Então agora vocês têm aqui essa versão já vai já vai ver já Como já não tem acento né ela vai simbora aqui por exemplo antes o símbolo tá um símbolo com acento e outro sem agora ele juntou tudo num só né então vocês podem usar nos exercícios essa versão aqui que é uma versão que já elimina muitas variações o seguido que eu fiz foi só tentar avaliar se aquela aquela comportamento não é que o zift mencionava ocorre aqui também esse texto é muito pequeno para isso mas dá para perceber alguma coisa então fiz um histograma aqui das frequências e dá para ver aqui é claro que aqui acabou fazendo em tanques em grupos mas dá para ver aqui as mais frequentes daí tem um grupo que a frequência é a mesma aqui né mas enfim o ranking de frequência é mais ou menos Segue o comportamento daquela Power rola da dos if que é muito frequente depois é basicamente acho que dois perdeu um terço aqui né Depois mais um texto assim por diante ele também mais

- *Corpus ID:* 8264
- *Score:* 0.851255476474762
- *URL:* oculto
- *Início:* 00:15:05
- *Fim:* 00:17:27
- *Transcrição:* tenho peças que tem duas letras três letras quatro letras uma letra e aí eu vou montando o o quebra-cabeça com com as peças que que se encaixam ali para representar aquela palavra mas aí com isso a gente resolve o problema das palavras fora do do vocabulário não não não é simplesmente o caracter né prof profess assim porque e a frequência desses caracteres juntos vai definir né então ali na palavra seres tu tem es atrás como um token né então não é não é por caracter é eu poderia ter a palavra ser inteira como um token se ela fosse frequente o suficiente Ela poderia ser um token aí eu juntaria o token ser mais o token es s é que esse é é um mini exemplo Então tem um exemplo mais detalhado aqui no no nesse link do do T data Science e aqui na na huging face também tem tem um exemplo da tokenização a gente vai ver na prática depois isso que se repetem muito no final das palavras né de várias exatamente elas vão ter vão ter uma representação muito Possivelmente se elas forem bastante frequentes então Endo and ir a r or No final da palavra são são coisas muito frequentes e vão vão invariavelmente vão ter vão ter uma representação que seja um toque e palavras que são frequentes o suficiente também vão ter uma representação inteira então a palavra bom a palavra mal a palavra ruim todas as Stop Words elas vão ter representações inteiras porque elas vão ser bem frequentes o Gar tem uma pergunta então se eu tivesse treinado né ou ou ou feito né Essa essa codificação e chegasse uma palavra Sei lá tá aqui uma uma uma uma conjugação do verbo ser lá seris né então ele ainda ia cair naquele caso que aí não tem né o que fazer né vai ser um no porque ele não tem I nem nada nesse vocabulário base né É mas é o vocabulário base que a gente vai fazer

- *Corpus ID:* 8061
- *Score:* 0.8499840497970581
- *URL:* oculto
- *Início:* 00:30:12
- *Fim:* 00:32:25
- *Transcrição:* crítica quando a gente eh se utiliza de nuvens públicas onde a cobrança se dá né pela quantidade de de tokens Então se a gente não fizer um pré-poo antes a gente pode ter uma continha ali no cartão de crédito bem salgada ao final do mês sim tu tá falando por exemplo do GPT né aham é ele é um tokenizador diferente desse aqui a gente vai ver ele na semana que vem porque ele não separa por palavras ele ele separa por sub palavras Então realmente isso que tu tá falando é uma coisa a considerar especialmente se tu tá num se tu vai rodar um Bet ali de de proms e tu pode ter uma surpresa porque é é o preço é todo em cima dos tokens Mas vocês vão ver na na próxima semana que é uma forma diferente de tokenizar as palavras e existe uma razão para isso porque aqui a gente vai ver que palavras que não estão no no no conhecimento do do modelo elas acabam eh ficando eh né ela fica uma palavra desconhecida mas isso a gente vai falar mais na semana que vem teu comentário tá correto é a tokenização ela mesmo esses modelos modernos ela é diferente mas ela faz parte ela é é uma parte importante do processo tá então então agora nós estamos falando de uma coisa mais tradicional onde separamos palavras normalmente token aqui é palavra tá é uma pontuação também tá então ele separa pontuação Então tokem é isso aqui pros modelos mais modernos do estado da arte já muda um pouquinho e a gente vai ver isso mais profundamente professora ten mais uma pergunta eh tem alguma função método ali que ele Liste o que ele considera como separador de Tod ou a gente precisa olhar na documentação meso É tem que dar uma olhada na documentação porque eh cada cada tokenizador é um então aqui o que eu sei te dizer é eu utilizo esse cara aqui sempre então eu nem não não nem sei direito como é que ele foi treinado ó tri Bank Word tokenizer tá então ele tem ele tem bases da onde ele

- *Corpus ID:* 8262
- *Score:* 0.8495962023735046
- *URL:* oculto
- *Início:* 00:11:33
- *Fim:* 00:13:53
- *Transcrição:* gente tinha mal males Lar lares mas não tinha seres tinha ser mas não tinha seres E aí se o nosso vocabulário base é só esse aqui como é que a gente vai tokenizar seres a gente vai tokenizar a gente o um S E aí eu tenho o e o r e o es quer dizer ele foi tokenizado e ele foi separado em em quatro tokens e esse símbolo aqui ele significa que eh é para para ser a que que esse token deve ser aglutinado com com o que vem antes então indica que a sub palalavra vai ser unida para formar um toque Viviane diga eh esse tipo de situação por exemplo Então se no texto tem uma palavra que não existe no no no dicionário português e foi digitada ela vai acabar sendo mapeada por conta dessa diversidade de tens exatamente e no pior caso se for uma palavra que realmente não tem nem pares de caracteres que que foram representados como tokens ela vai ser representada por cada letra da palavra e tirando a primeira letra todas vão ser precedidas desse desse símbolo Entendi então com isso eu consigo representar qualquer palavra Professor vocabulário básic tá chamando aí é como se fosse um alfabeto na verdade né não é um voc como se fosse um alfabeto novo é é o alfabeto que a gente tem só que ele vai ter aqui não vai ser um vocabulário que tem 30.000 tokens ou 50.000 tokens quanto a gente definir eu tô falando 30.000 porque ele é um número meio meio padrão então eu vou eu vou consegir eu vou usar esses 30.000 tokens para representar todas as palavras do do teste que vão aparecer e eu vou conseguir representar porque eu tenho todos os símbolos base que são as letras do alfabeto o os caracteres de pontuação espaço separadores professora se estou entendendo corretamente então se eh Se for se o modelo de eding for criado numa

- *Corpus ID:* 3978
- *Score:* 0.8493068218231201
- *URL:* oculto
- *Início:* 00:09:26
- *Fim:* 00:11:37
- *Transcrição:* as unidades linguísticas são chamadas de tokens é cada unidade de análise uma unidade de análise chamada de token Então existe um processo que nós vamos usar primeiro deles que é de tokenização que é eu pego um corpos é um conjunto de documentos eu eu processo esse conjunto de documentos para identificar os tokens né que são os termos as palavras válidas né de serem consideradas na análise aqui nós não vamos explorar diferentes formas de tocarinização lá na de PLN a senhora vai fazer isso eu vou considerar que a unidade de análise são as palavras as palavras os termos individuais que aparecerem mas o processo de organização ele ele é bem amplo né então eu posso dizer para ele que um token é um pare de palavras é um trio de palavras eu posso dizer para ele um toque é uma sequência de símbolos tem estudos que mostram que ao invés de eu trabalhar no nível de palavras o mesmo nível que a gente compreende eu trabalhando no nível de de sequências de letras eu consigo representar-se com desses estudos né que mostram isso melhor o conteúdo do texto Então eu digo para ele um tom que é uma sequência de duas letras então ele ele ignora os espaços ele junta tudo que a palavra ou mesmo considera o espaço Mas enfim ele vai pegando sequências de duas letras e aí tem uma janela uma janela deslizante que pega por exemplo né se eu fosse processar esse próprio essa própria o bloco de notas né e dissesse para ele que os token são sequências né Elas chamam de gramas ou n gramas ele é a quantidade de letras ou palavras que são que são usadas na análise né então se digo que é migrama seria de duas em duas palavras ou de duas em duas letras Então se fossem duas palavras ele

- *Corpus ID:* 3981
- *Score:* 0.8490864038467407
- *URL:* oculto
- *Início:* 00:14:28
- *Fim:* 00:16:52
- *Transcrição:* posso ir adiante ter um tokenizador mais inteligente que diz olha isso aqui é uma é uma deixa eu pegar aqui algum exemplo mais uma entidade né eu posso tirar isso aqui é um nome de uma pessoa né então identifique se é um toque em pessoa se é um toque em lugar se é um toque em data Então ele pode ser bastante complexo Então toca representa essa unidade de análise e aqui nós vamos usar o tipo mais simples de organização que só separa as palavras a sequências né em toques diferentes e aí Isso serve para explicar um funcionamento e depois Como eu disse naquelas em outras opções as Stop Words Então são são tokens e que eu quero desconsiderar durante o processo né Aí tem aqueles artigos todos ali então que eu vou deixar para vocês verem os efeitos disso para clasterização normalmente nós tiramos fora com esse objetivo né de tornar uma quantidade de dimensões menor e também porque eles são frequentes ou bastante para não discriminar nada e mas se eu mantivesse você viu ali que existem maneiras como tfdf que que acabam entendendo que essas palavras são são relevantes ou são pouco discrimites e mudam o peso delas para que isso seja refletido né então já que existe uma um mecanismo que faz isso eu poderia seguir adiante mas esse mecanismo faz isso processando mais palavras né tendo que fazer um cálculo maior Então se a gente já sabe que o mecanismo depois vai fazer isso para nós e já sabe que elas são inúteis nesse caso específico Então a gente vai tirar fora aí o nosso o nosso maior problema é é que eu preciso ter uma lista né uma lista de quais são essas palavras né E aí aqui é o mecanismo mais simples Eu tenho um dicionário de Stop World Então nada em Python existem bibliotecas que foram construídas ao longo dos anos pela comunidade que definem conjuntos de

- *Corpus ID:* 8104
- *Score:* 0.8483508825302124
- *URL:* oculto
- *Início:* 00:07:44
- *Fim:* 00:10:04
- *Transcrição:* coisa lâmpadas convencionais né é outra coisa seria pod interpretar sim poderia interpretar se tu quer reconhecer o que que é eh tu quer reconhecer por exemplo seria mais útil tu ter uma Fit que diga lâmpada LED do que só led ou do que só lâmpada porque aí não não discrimina então esse problema da da gente identificar o que que são as as as unidades relevantes ou identificar o que que seriam termos compostos qualquer coisa que é composta por mais de um token também é é é relevante e tem toda uma área de de pesquisa pra gente identificar algumas pessoas chamam de compostos nominais sintagmas ais eh expressões multipal tem tem tem várias tem bastante trabalho nesse sentido não é um problema resolvido eh 100% resolvido ainda mas existem abordagens para fazer essa identificação hã e pra gente tratar em vez de olhar palavra por palavra tentar olhar assim unidades com um contexto maior que possam dar uma representação melhor sobre o que que sobre o que que essa sentença tá falando mas hoje Por simplicidade a gente vai separar em tokens por espaço ou por caracter separador né pode ser espaço pontuação numeração então a gente normalmente vai ter que descartar algumas coisas então números e pontuação normalmente a gente descarta outra questão é que ah a tokenização é específica para cada idioma por exemplo em alemão é o contrário do problema que a gente tava falando aqui que era de reconhecer que dois tokens separados são uma coisa só aqui em em alemão ou e e esses outros idiomas que a gente chama de aglutinativa sem espaço então eles eh é identificar que a gente tem um token só que corresponde a duas palavras esse aqui a gente consegue identificar o que que é né computer linguistic esse outro aqui eu não uso não ouso nem a pronunciar é um exemplo assim eh exagerado Mas é uma palavra que existe

- *Corpus ID:* 3979
- *Score:* 0.8464982509613037
- *URL:* oculto
- *Início:* 00:11:04
- *Fim:* 00:13:16
- *Transcrição:* aí tem uma janela uma janela deslizante que pega por exemplo né se eu fosse processar esse próprio essa própria o bloco de notas né e dissesse para ele que os token são sequências né Elas chamam de gramas ou n gramas ele é a quantidade de letras ou palavras que são que são usadas na análise né então se digo que é migrama seria de duas em duas palavras ou de duas em duas letras Então se fossem duas palavras ele pegaria Corpus conjunto como uma delas depois conjunto de como outras ele vai deslizando aqui né E vai seleciodo esses esses elementos como como toques de processamentos no nível de palavras tem uma opção que pega por letras não pegaria co aqui como uma palavra né um token depois o r o outro token depois o RP outro e assim vai indo e com isso esses estudos mostram que a repetição desses elementos representa melhor né o conteúdo mas eu vou trabalhar no nível de palavras e também tem a questão de compreensão depois do resultado então vocês vão explorar melhor isso depois Como eu disse é mais para vocês entenderem que a tua organização permite diferentes formas de representação e que elas têm um determinado Impacto específico no resultado isso vai ser alvo de outra disciplina independente do tipo de token que vocês escolheram o restante do processo é o mesmo é claro que por exemplo Stop Word se vocês decidem que o esquema de organização é outro tem que ser adaptadas para corresponder a isso ou talvez não façam sentido só que hoje elas fazem mais sentido quando o top é uma palavra e assim essa unidade Token para quem trabalha com processamento da língua natural ela não é simplesmente o termo né não é simplesmente a sequência de caracteres que constituem uma palavra os tokens eles têm tipos também né então

- *Corpus ID:* 8060
- *Score:* 0.845323383808136
- *URL:* oculto
- *Início:* 00:28:25
- *Fim:* 00:30:45
- *Transcrição:* em inglês agora eu vou fazer um exemplo em português onde eu tenho aqui o nome de uma conta com @ links números vou usar os mesmos comandos que eu usei lá em cima veja como eles se diferenciam E aí que nós vamos começar a ver que a gente precisa Talvez pré-processual para esses algoritmos que nós vamos ver na próxima aula daquela onda que a professora Viviane comentou do desses modelos estatísticos e mais tradicionais Então veja aqui que o Word tokenize ele também separa essa conta ele não resolve esse problema eu não quero saber dessa ara aqui que ela entre separado né o link também acontece alguns problemas então isso tudo a gente vai ter que tratar eh fazendo um pré-processamento disso tem mais aqui acho que o guarda-chuva ó guarda-chuva no no no Word tokenize Ele separa Ai desculpa ó o Word tokenize ele mantém tá o Ou seja é uma é um token só veja que naquele to esse tokenizador do punct aqui que é é agnóstico a língua ele separa e ele trata o if como um token e assim tem particularidades então a tokenização o método que a gente vai utilizar ele é importante especialmente nesses algoritmos que a gente vai ver na próxima aula então a gente tem que analisar esses resultados antes de decidir por um tokenizador por favor interrompo Caso vocês tenham dúvidas ou comentares professora Ah e essa e essa questão da tokenização Ela acaba ficando bastante crítica quando a gente eh se utiliza de nuvens públicas onde a cobrança se dá né pela quantidade de de tokens Então se a gente não fizer um pré-poo antes a gente pode ter uma continha ali no cartão de crédito bem salgada ao final do mês sim tu tá falando por exemplo do GPT né aham é ele é um tokenizador diferente desse aqui a gente vai ver ele na semana que vem porque ele não separa por palavras ele ele separa por sub palavras Então realmente isso que tu tá falando é uma


**Resposta gerada pelo LLM**: Tokenização é um processo que identifica tokens, unidades de análise em um conjunto de documentos.  Um token pode ser uma palavra, uma sequência de símbolos, um par ou trio de palavras, ou sequências de letras.  Existem diferentes formas de tokenização, sendo a mais simples a separação de palavras por espaços.  Em modelos mais modernos, a tokenização pode ser por subpalavras.  A tokenização é importante, especialmente em algoritmos que cobram por quantidade de tokens.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the PROMPT by defining tokenização and explaining its significance. It provides a clear explanation of what tokens are, the different forms of tokenization, and its importance in algorithms. This comprehensive coverage of the topic aligns well with the PROMPT's request for information about tokenização.  Score: 3


---
