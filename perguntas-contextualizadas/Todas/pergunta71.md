**Pergunta 71**: Apesar do grande avanço e aplicação do Deep Learning, qual é a principal dificuldade que os especialistas enfrentam em relação ao funcionamento dessa tecnologia? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8174
- *Score:* 0.8740357756614685
- *URL:* oculto
- *Início:* 00:42:50
- *Fim:* 00:44:59
- *Transcrição:* algoritmos de redes neurais né e depois Deep learning Por que eles conseguem representar melhor essas palavras do que isso então eu te diria uma resposta paraa classificação não não funci não acho que funcionaria tá deixa eu te fazer uma outra pergunta e com relação ao desbalanceamento de classes como é que a gente lidaria com isso olha a minha tese de doutorado foi teve algumas coisas sobre isso tá então hoje tem uma tendência muito grande da gente falar de aumento de dados porque nada mais é que tá te faltando dado de algumas classes então tu imagina aqui nós estamos num ambiente de revisão Pode ser que um determinado produto ele seja tão revisão de produtos né Pode ser que um determinado produto seja tão maravilhoso que ele não tenha quase revisões sabe E aí tu vai fazer o qu né quando tu quiser entender mas pior imagina um cenário de saúde onde eu tenho uma classe muitas vezes que ela é muito pequena que é identificação de uma uma imagina no prontuário um texto sabe e o coisa urgente ou grave que está descrito no prontuário mas isso ocorre em 1% dos casos e eu ainda tenho mais uma dificuldade que é nesse nesse domínio muitas vezes o o dado não tá acessível Eu não eu consigo só um determinado número de coisas lá porque não tem permissão tá é uma um dado muito sensível assim pra gente ter acesso aí eu tenho que fazer o quê eu tenho que aumentar esses dados com o que eu tenho e aí tem técnicas a gente não vai ver ao longo do curso aqui mas por exemplo eu posso pegar uma estância uma coisa bem simples eu utilizo um tradutor então eu traduzo a Estância para o árabe por exemplo E aí eu volto ele para português nessa ida e vinda ele introduz variações naquela Instância aí eu tenho uma Instância a mais sabe is isso é bem simples de fazer e bastante estabelecido assim eu posso pegar sinônimos por exemplo de uma de algumas palavras por exemplo eu posso

- *Corpus ID:* 7170
- *Score:* 0.8656269311904907
- *URL:* oculto
- *Início:* 01:28:32
- *Fim:* 01:30:43
- *Transcrição:* Deep learning isso era bem Inicial né então vocês podem esperar muito mais do que isso para hoje tá aqui a gente tem um outro sistema do metp que é um sistema comercial e eles T outros sistemas outros dispositivos de captura né eventualmente alguns dispositivos capturam informação tridimensional também outros capturam só cor e e a partir das informações de cor a gente pode reconstruir né uma ou fazer uma reconstrução tridimensional do ambiente também e Navegar eventualmente né Por esse ambiente virtual eh usando esse tipo de ferramenta tá isso aqui se tornou bastante Popular especialmente fora do Brasil não sei como é que tá aqui dentro para eh em Sistemas de Imobiliária né Então a gente tem uma imobiliária tem um apartamento que a gente quer alugar Quer comprar uma casa e ao invés de me deslocar ir lá e e ver que eu não gostei né antes de tudo dá para fazer uma navegação virtual e bom se eu achei que é legalzinho de repente eu eu pago né o Uber pago o ônibus pago a gasolina para ir até lá ver tá mas mas fica um sistema de screening assim né para determinar se eu devo ou não devo investir naquela compra naquele aluguel tá eh e hoje em dia a gente tem soluções aqui é um dispositivo específico aqui é outro dispositivo específico para captura eh mas a gente tem soluções que tão mais portáteis né mais na palma da mão do que do que nunca então sei lá eu eu vou falar do iPhone embora não tenha um iPhone eu vou falar do iPhone porque eu sei que tem aplicativo para isso nele eh em que a gente de fato faz esse tipo de reconstrução eventualmente não é tão boa quanto né desses exemplos que foram dados mas ele consegue de uma forma ou de outra fazer um mapeamento de ambientes pequenos e e e te ajudam né a interagir com esses ambientes tá Ahã Professor

- *Corpus ID:* 2625
- *Score:* 0.8624821901321411
- *URL:* oculto
- *Início:* 00:17:22
- *Fim:* 00:19:12
- *Transcrição:* interessante porque assim falando aqui nessa questão né de questões mais enfim bancárias ficeiras então eles dava um conjunto de dados com milhares de indivíduos né que tinha aspectos históricos de crédito e informando se o indivíduo não pagou empréstimo tá então tinha informações e basicamente a tarefa era criar um modelo caixa preta tá para prever a inadimplência de empréstimos em seguida explicar a caixa preta então o desafio diz assim você tem que criar um modelo caixa preta para prever esse risco de inadimplência e propor um método uma forma de explicar essa caixa preta tá E aí basicamente o que a maioria dos times que participaram desse desafio pensou o seguinte se a competição exige que se cria um modelo caixa preta que normalmente são modelos mais complexos por exemplo uma rede neural profunda né amor a própria rede neural tradicional enfim usbm o problema é realmente demanda o uso de tal modelo né para poder ser resolvido E aí teve um timeverse que ele era coordenado pela professora Cinthia ruden que observou o seguinte fizeram um treinamento de um modelo de aprendizado profundo né baseado em redes neurais e o modelo de regressão linear clássico que tem a questão da interpretabilidade que a gente comentou agora tá da questão dos coeficientes e observou entre os dois modelos menos de 1% de diferença na acurácia nessa tarefa de prevenir de preferência tá E sendo que o modelo representado profunda era muito mais complexo não é interpretável então basicamente o que eles observaram isso é que um modelo interpretável aqui representado pela regressão era tão bom quanto esse modelo não interpretável e ele se depararam a situação que é ou eu sigo as regras do desafio e apresento o meu modelo de aprendizado profundo e apresenta uma estratégia para tentar explicar o que esse modelo tá prevendo que tem técnicas para isso Ou eu

- *Corpus ID:* 6257
- *Score:* 0.8622807860374451
- *URL:* oculto
- *Início:* 01:05:35
- *Fim:* 01:07:37
- *Transcrição:* um monte de outras perguntas agora então passar pro Denis aqui V seguir a fila aqui Denis Opa bom dia professor vamos lá ah tudo então ah no mundo real né a gente vai ter problemas do tipo sei lá eh vou implementar uma solução de biometria baseada em Deep learning que tem ali uma rede neural complexa com sei lá dezenas centenas ali de de camadas e para isso naturalmente né as gpus elas surgem como uma opção interessante mas a minha pergunta é a gente já tem um nível de complexidade elevado lá na na na questão né de concepção de de eh das redes neurais naquela coisa toda e para eu usar a GPU eu vou necessariamente entrar nesse nível aqui de de de eh de baixíssima abstração ou tem alguma camada intermediária que Vai facilitar a minha vida tem tem camadas intermediárias que facilitam enormemente a vida mas eu insisto que a gente não consegue entender essas camadas intermediárias sem ter ideia concreta de como é que funciona a placa entendeu então a gente vai ver na próxima aula por exemplo numba que vai permitir a gente programar em Python essas placas entendeu então tipo assim eh só que daí a gente na hora de lançar o kerno a gente vai precisar especificar por exemplo Eh quantas tras esse tipo de coisa né então continua ess em outro nível mas tem eh níveis de Denis de abstração ainda mais elevado né que é por exemplo todo o Framework rapids da NVidia ou mesmo por exemplo se vocês pegarem o dask o dask tem os workers que a gente viu lá no dask eles eram workers CPU vocês lembram né a gente lançava lá Desk worker então existem hoje em dia desks GPU workers entendeu onde um worker do Desk é na realidade é uma GPU entendeu então quando a gente faz o cálculo lá em npai ou em pandas entende e vai acontecer e que um pedaço daqueles blocos lá daí agora tô falando daqueles blocos lá do dask né eles serão mapeadas para um worker GPU e e o nosso nível de

- *Corpus ID:* 3216
- *Score:* 0.861117959022522
- *URL:* oculto
- *Início:* 00:05:39
- *Fim:* 00:07:54
- *Transcrição:* conjunto de dados mais desafiador né o início é muito muito simples e o que acontece é o seguinte ó se você tem em algum algoritmo alguma rede neural que não tá indo bem no éministe quer dizer que tá indo bem no início não quer dizer que a rede neural o algoritmo seja bom mas se ela tá indo mal não é ministro quer dizer que tem algum problema né É o primeiro digamos teste de sanidade né o CNPJ Inicial que se passar não quer dizer que tá tudo bem mas se não passar quer dizer que tem um problema né Então vale a pena né usar como a primeira primeiro ponto aí de medição da no desempenho né enfim da validade de uma rede neural não é o ponto final é o ponto final aí tem que ser um conjunto de dados mais desafiador mas isso que eu tô dizendo é para coisas do mundo real né então você tá tentando uma nova arquitetura alguma coisa assim ah vamos ver se tá funciodo aí rodando no início e você para o trabalho como Ministro é simples tá aí pode ser né pode usar o limite na sua implementação se for aquela modalidade de implementar o algoritmo de treinamento mesmo tá se for a modalidade de usar o tensor igual nós usamos aqui e não é ministro tá muito simples né isso aqui já quase resolveu o problema já deu quase 90% decorar essa imagina se eu botar os dois tantos lá do enunciado já talvez já chega no 95%, então não não vale né usar o menisco e quando for usar o tensor né para aí por isso que tem outras outras datas certas recomendados lá ok pessoal então esse né tá demonstrado aqui como que se usa o Multilaser percebe então Multilaser então rede neural aquela que a gente já conhece é um problema de processamento de imagens e o data 7 bem simples que é o ministro agora aqui deixa eu ver se eu pego esse aqui né como que funciona uma rede neural assim né no conjunto em imagens o que a gente tem né vamos voltar problema de tentar deletar um gato aqui né na

- *Corpus ID:* 7702
- *Score:* 0.8610653877258301
- *URL:* oculto
- *Início:* 00:55:56
- *Fim:* 00:58:47
- *Transcrição:* com o o desenhinho que tá ali eu não sei qual dos dois que tá discrepante com relação ao ao artigo original tá bom E aí de novo né seguindo essa Professor Oi uma uma curiosidade eh eh eh naquela última a gente teve uma redução ali de 45 para para 37 né na POA a questão do do erro e essa última aí melhorou a quanto mais ou menos Ah eu já não sei de cabeça aqui eu posso até olhar aqui vamos ver deixa eu ver aqui aqui eu tenho um artigo de ó esse artigo aqui é de 2018 tá então eu não sei exatamente qual técnica que tá sendo ilustrada aqui mas dá para ver a evolução né então tem aqui 30 não esses erros aqui estão menores do que eu mostrei antes então talvez não seja exatamente o mesmo dataset tá Ah então de novo pessoal isso tá sendo gravado então vou deixar bem claro eu acabei de baixar esse artigo aqui eu não sei exatamente o que que eh que que ele tá representando tá mas aqui mostra a curva da queda do erro né então pré Deep learning tá ali 20 27 25 cai para 16 aí depois no ano seguinte já cai para um pouco abaixo 10 depois a partir de 2015 já baixa para menos de 5% tá E aí tem uma coisa muito legal que essa barra vermelha eu tô assumindo que é confiável isso aí que é exatamente a taxa de erro humano tá então tem algumas imagens que elas são bem delicadas de classificar tá então o humano também erra né Daqui a pouco eu tô dou uma imagem que eu não sei se daqui a pouco é um lobo ou se é um rusk sei lá tá então é as técnicas tu vê 2015 Já faz quase 10 anos né que a taxa de erro já atingiu basicamente a taxa do erro humano tá então o Google net é de 2000 e o vgg é de 201 tá Talvez seja aquela que já baixou de 5% não sei tá Não não vou afirmar Com certeza porque eu realmente não sei tá

- *Corpus ID:* 6688
- *Score:* 0.861062228679657
- *URL:* oculto
- *Início:* 00:19:24
- *Fim:* 00:21:39
- *Transcrição:* tenha sido treinada né Tem gente trabalhando nisso né explicar assim ah como por que que ela funciona né Por que que essa rede neural consegue identificar os objetos ou classificar os objetos por né a pessoa tenta olhar visualizar os valores dessa rede neural muito difícil explicar né então nessas situações não se a gente precisa que o modelo seja explicável né daí a gente não vale a pena usar PR profundo porque isso é uma coisa que tá ninguém sabe explicar ainda né Tem gente estudando isso aí mas isso ainda é um um processo tá acontecendo digamos assim bom então falando um pouco sobre o aprendizado profundo né Eh a gente sempre tem essas duas fases uma fase de treinamento e uma fase de inferência né então normalmente né em situações eh corriqueiras digamos assim o treinamento é uma parte que deve acontecer junto com a massa de dados que a gente tem tem PR processar esses dados e esse treinamento normalmente ele é lento né então ele envolve a gente ler todos os dados várias vezes que é o que a gente chama de épocas né e em cada uma dessas épocas a gente vai embaralhar os dados né para que eles fiquem bem misturados para que não se crie vícios né E a gente vai empregar então uma estratégia de distribuição desses dados de maneira que a gente possa treinar usando gpus tpus cpus então uma vez que esse treinamento Se conclui e a gente vai ter um modelo e esse modelo a gente pode salvar é um arquivo e esse modelo depois a gente pode usar no que a gente chama de eh no deployment né tipo assim que seria o uso do modelo e essa segunda parte que é a parte de inferência ela é considerada rápida sobretudo em relação à Ótica do treinamento ela sempre vai ser mais rápida que o treinamento porque ela envolve somente a gente fazer fornecer um dado na entrada e fazer aqueles dados estimular a rede Até chegar na saída né então isso é rápido né comparado com o tempo de treinamento

- *Corpus ID:* 6694
- *Score:* 0.8608824610710144
- *URL:* oculto
- *Início:* 00:29:17
- *Fim:* 00:31:29
- *Transcrição:* conclusão Olha só esse meu modelo é tribom muito da da da teu intelecto foi eh dedicado em escolher Quantas camadas tinha como é que é conexão entre as camadas quais são os tipos de camadas Qual é a sequência Ok então por isso que normalmente o pessoal não disponibiliza o modelo né quando esse modelo foi foi investido um tempo muito grande para conceber ele depois para treinar Claro T um tempo muito grande e paraar mas aí mas aí tipo Google da vida uma essas grandes esses grandes players eles vão ter um parque tecnológico bem grande e então eles têm eles vão ter tempo eles têm poder de processamento até encontrar de repente uma rede eles vão por exemplo sempre tá na frente de uma pessoa que pode ter até um intelecto melhor para produzir isso mas ele não tem tempo nem tem infraestrutura suficiente para gerar essa rede é né e é é bem vender alguma coisa por exemplo é tem é realmente bem complicado porque não basta tu ter boas ideias para tu construir a rede né e ah não essa minha rede neural é bem inovadora ela que tem essa essa configuração de camadas os tipos são desse se depois para tu avaliar tu vai precisar treinar ela né e treinar é um tu precisa de um parque computacional muito grande né para então acaba acontecendo realmente que quem ganha é quem tem mais recurso computacional para fazer o treinamento para testar né tipo assim para chegar numa determinada versão de rede neural né provavelmente teve muito teste antes que falhou né que foi treinado treinado depois não ficou bom E aí muda Enfim então tem isso tem toda uma área de de pesquisa sobre isso aí né bom então a gente vai usar o tensor Flow né uma biblioteca para Prado profunda ela é de código aberto desde 2015 escrita em C mais mais ela foi iniciada dentro do Google né e depois ela foi Tornada pública digamos assim pra comunidade

- *Corpus ID:* 7147
- *Score:* 0.8605907559394836
- *URL:* oculto
- *Início:* 00:48:48
- *Fim:* 00:51:09
- *Transcrição:* ã e ver se aquilo tá casando com as orientações que o sistema tá dando né E aí isso eventualmente autentica uma pessoa ou determina que aquela pessoa é uma pessoa de Fato né não é um vídeo gerado ou uma máscara ou qualquer coisa nesse sentido tá então no caso do Golf BR professor é É passivo já foi ativo né mas isso gerava muito problema de acessibilidade né então algumas pessoas não conseguiam fazer os movimentos que ali eram pedidos então havia um como a gente tá falando de 150 5 milhões de contas já hoje então havia uma limitação muito grande e hoje a gente já tem um um processo de liveness passivo né então ele só vê ali a profundidade da imagem são outros atributos e não o movimento da pessoa a pessoa basta ficar parado na posição claro que exige um pouquinho melhor de qualidade de de imagem mas ele ele é passivo tá Não beleza Beleza então são são duas duas frentes aí né Eh bom é uma área que para quem trabalha com ela acho que eh isso aqui é verdade né Eh de certa forma é apaixote né aqui tá atrativo para para diminuir um pouco o impacto da frase mas eh tem muita coisa que especialmente nessa era de Deep learning né Tem muita coisa que funciona e tá como assim né como que funciona claro que eh a gente tem algumas diretivas Gerais na hora de construir eh uma arquitetura de rede na hora de determinar uma loss function em detrimento de outra eh com base em evidência né de Treinamento prévio por exemplo consigo determinar se a minha rede tá curta demais ou se ela tá longa demais Deep demais né Eh mas assim eh o como essa coisa toda funciona no final das contas eh é algo bastante eh né bastante interessante assim né quer dizer de novo retomando o livro do

- *Corpus ID:* 5415
- *Score:* 0.8605055809020996
- *URL:* oculto
- *Início:* 00:14:54
- *Fim:* 00:17:01
- *Transcrição:* até eu não tinha me preparado muito Denis mas eh a gente tem um parque eh computacional de Alto desempenho a gente prioriza ter as máquinas físicas aqui para ter controle 100% da pilha entendeu tento que porque a gente trabalha com o ensino né Então vem o aluno ali a gente precisa ensiná-lo a trabalhar desda como configurar como instalar a máquina né tipo assim as coisas mais elementares de enfim de da energia da necessidade de energia e tudo mais a a parte da bios né que div bastante importante porque a gente precisa ali conf várias coisas né Eh por exemplo parte de processador ali tem várias coisas que a gente Conseguir configurar na Bios né E que a gente Talvez não tenha acesso através do sistema opcional e depois toda a pilha de software né que existe uma gama absurdamente enorme de possibilidades né então Eh claro que aqui a gente prioriza uma determinada configuração paraa Nossa sanidade digamos assim então assim a gente prioriza a máquina física certo mas porque a gente tá num ambiente de ensino se a gente for para um ambiente de produção né onde que isso aqui não é um ambiente de produção né o pcad é um ambiente de teste prototipagem como eu já falei já no ambiente de produção onde o nosso workload é relativamente estável existe muita gente estudando justamente o uso de nuvens né então nós temos aí as nuvens da da Amazon aws o azure da Microsoft enfim vários outros a Google também tem o Google Engine enfim existe uma gama grande aí né de empresas que elas a virtualiza ciclos de processamento né e tem muita gente indo para isso né pela simplicidade né porque tu paga ali uma hora tu não precisa se preocupar em instalar as máquinas esse tipo de coisa eh eu vejo como vantagem essa característica assim das da ser eh maleável digamos assim a desvantagem que eu vejo e aí eu vejo mais pelo lado assim de cientista né do ponto de vista de análise de desempenho é que normalmente as coisas não são tão


**Resposta gerada pelo LLM**: A principal dificuldade é a falta de explicabilidade do funcionamento das redes neurais profundas, tornando difícil entender por que elas funcionam e o que elas estão prevendo.  Ainda há pesquisas em andamento para solucionar esse problema.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address the main difficulty faced by specialists in relation to Deep Learning. Supporting Evidence: The RESPONSE directly addresses the PROMPT by identifying the principal difficulty, which is the lack of explainability in deep learning models. It explains why this is a challenge for specialists, thus providing relevant context to the entire PROMPT. Additionally, it mentions ongoing research efforts to address this issue, further enhancing its relevance.  Score: 3


---
