**Pergunta 149**: Como o paralelismo de dados é implementado no TensorFlow?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 7036
- *Score:* 0.8865324854850769
- *URL:* oculto
- *Início:* 00:36:35
- *Fim:* 00:39:00
- *Transcrição:* espelhamento ela realmente pega ela realmente faz isso que eu acabei de descrever ela ela replica ela espelha o modelo e faz com que cada placa atue sobre betes diferentes e depois tem uma uma uma fase de suavização né uma combinação Então essa é uma forma de implementar existe uma outra forma que são através de parâmetros centralizados a gente ter um servidor dedicado para gerenciar os parâmetros o que que são os parâmetros é o modelo em si com seus pesos né e Bis Ok E aí no caso dessa segunda estratégia a gente tem uma atualização síncrona ou aí então vamos pras perguntas que parece que apareceu bastante né Jan Carlo Pode falar a sua Av visação dos dados é pegar os modelos que estão nos devices e e ajustar os baias e pesos para todos ficem iguais exatamente tu vai pegar os os baias e os pesos das placas diferentes né que foram que divergiram né E tu vai fazer uma operação de suavização para que tu tenha só uma modelo como se tu estivesse fazendo uma combinação e essa combinação daí tem várias formas de fazer pode fazer uma média por exemplo entendeu seria Exatamente isso Antônio Fagner pode perguntar Bom dia Lucas Bom dia eh a minha pergunta é assim é uma questão técnica do seguinte a gente tá estudando essas técnicas aí eh um p tort o eh tensor Flow cu e eh dnn né todos esses frameworks esse eles oferecem maneiras de a gente fazer isso utilizar essas técnicas que a gente tá estudando na prática tá porque você vai vamos vamos trabalhar em cima disso mas qual desses te oferece uma melhor te oferece a os algoritmos essa divisão ah de ess ess quebrar esses dados né paralelizar por processo paralelizar por dados qual desses é melhor pra gente utilizar entendi ah não existe uma resposta definitiva para essa pergunta porque

- *Corpus ID:* 7022
- *Score:* 0.8804945945739746
- *URL:* oculto
- *Início:* 00:15:00
- *Fim:* 00:17:12
- *Transcrição:* que tem que fazer lá dentro eventualmente quebrando também em pedaços né vai usar os vários tensor cores ou os vários cud acores Então existe essa ideia de duas filas né Especialmente na na na na tanto em CPU quanto em GPU E aí eh essa primeira fila é a fila para tarefas do grafo então a gente en filera e depois a outra fila é pros pedaços de uma tarefa né então existe esses dois tipos de paralelismos né seria o paralelismo Inter é entre tarefas e o paralelismo intra é internamente numa tarefa Ok e a vantagem de usar GPU é que a paralização é mais simples porque essa parte intra aqui ela é gerenciada pela própria biblioteca se dnn né tipo assim não não tem não não existe do tensor Flow nenhum mecanismo para fazer esse tipo de coisa já já na parte de CPU como tem que ser feito à mão né aí o tensor Flow implementa daí esses dois essas dois enfer aí bom alguma dúvida até agora pessoal por enquanto não Professor certo então assim eh Quais são as alternativas de paralelização que nós temos Então a gente tem várias situações Diferentes né H uma primeira alternativa seria por exemplo a gente treinar vários modelos em paralelo e botar um modelo por GPU Então como a gente tá fazendo normalmente uma procura bem exploratória de qual é o modelo ideal para nós a gente vai testar vários modelos Imaginem a gente variando os hiper parâmetros ali Quantas camadas tem quais são os tipos de camadas esse tipo de coisa né a gente cria um um um espaço de configurações disso aí e vai explorar esse espaço de configurações sendo que em cada uma dessas configurações a gente precisa fazer a integralidade do treinamento né e testar né se se se se aquela se aquele modelo específico daquele espaço de configurações tá indo bem ou Malé a

- *Corpus ID:* 7054
- *Score:* 0.8774480223655701
- *URL:* oculto
- *Início:* 01:04:31
- *Fim:* 01:06:32
- *Transcrição:* algoritmos bem eficientes para isso também então no tensor Flow depois a gente vai olhar agora logo em seguida aí né Essa estratégia é é é a principal ela chama TF distributed mirror strategy tá tá aqui a documentação então se eu abrir ela aqui para vocês a gente dar uma olhada rapidinho Ela tá aqui na documentação Então ela é a principal estratégia de PR de dados tá ela erda de estratégia que vocês podem ver que é uma uma é uma super classe aqui que que todo mundo tem quear que é fazer para de dados E aí que tem um monte de detalhamento a gente vai entrar então um pouco mais a fundo nisso aí se a gente for eh olhar para para esse espelhamento e quer trabalhar com tpus do Google daí a gente vai usar o TPU strategy é uma outra classe né e depois a gente vou mostrar a gente vai fazer umas comparações entre GPU e TPU no no colar Ok ã então assim só pra gente terminar esse conjunto de slides bem rapidinho né Então essa é uma estratégia tá uma estratégia que é a estratégia principal ok que é a estratégia des espelhamento do modelo com uma operação al redu para fazer depois a média Tá mas existe uma outra que é uma estratégia alternativa que funciona também que é a estratégia de parâmetros centralizados aqui funciona diferente a gente vai ter ter também essa cópia do modelo inteiro em cada device que nem antes mas eh os parâmetros do modelo eles são centralizados Então a gente vai ter um uma única CPU né uma único computador por exemplo que vai receber eh os vários modelos que divergiram E e essa única máquina vai calcular a média do modelo vai atualizar vai gerar novos parâmetros e esses novos parâmetros serão informados paraas várias gpus para que eles atualizem né instituam um modelo com esses novos parâmetros Ok Então essa é uma outra estratégia uma estratégia de que a gente chama de parâmetros

- *Corpus ID:* 7024
- *Score:* 0.8764493465423584
- *URL:* oculto
- *Início:* 00:18:08
- *Fim:* 00:20:12
- *Transcrição:* basicamente um arquivo csv que descreve cada modelo de alguma forma né E aí vai ter um mecanismo que vai pegar uma dessas dessas configurações e vai pegar Ah tá qual GPU tá livre vou treinar nessa GPU depois ele vai pegar a próxima configuração vai botar na outra GPU e vai fazendo isso enquanto tem GPU disponível e como é que tu força para que eh eh cada script veja só uma GPU daí tem uma variável de ambiente chamada kuda visible devices que ela é respeitada pelo tensor Flow E aí a gente diz no cuda visible devices Qual que é o device ID eh da GPU que a gente quer usar naquele naquela execução de tensor Flow né então com isso daí eles conseguem automatizar esse mecanismo de procura bem manual né a gente vê isso bem manual embora existam mecanismos mas eh eficientes para fazer isso talvez já mais prontos assim né mas essa seria a forma manual de fazer Então essa é uma alternativa né tu tem várias eh gpus tu testa um modelo GPU Ok ah uma outra uma outra possibilidade de paralelização né que essa Inclusive a gente já viu é usar o prefet né então a gente vai ao mesmo tempo que tá treido com uma GPU né a gente pode usar CPU pra gente fazer pré-processamento né então por exemplo aqui nessa nossa nesse nosso grafo de tarefas essas tarefas que aparecem aqui em CPU podem ser de pré-processamento né para isso a gente vai usar o dataset prefet que a gente viu na aula passada e aí a gente põe o paralelismo que a gente precisa ter nesse dataset esse paralelismo vai ser um paralelismo de CPU e a gente usa todas as cpus para preparar os dados para que a GPU possa fazer o treinamento de maneira eficiente então com isso a gente vai ter um paral ismo em CPU e um paralelismo em GPU Ok tem uma outra alternativa também que é a gente usar para aqueles modelos que são de

- *Corpus ID:* 7009
- *Score:* 0.8756489157676697
- *URL:* oculto
- *Início:* 01:48:02
- *Fim:* 01:50:05
- *Transcrição:* a gente paralelização uma né várias gpus para fazer o treinamento entendeu então acho que eu acredito que agora a gente tem uma maturidade suficiente Porque a gente já sabe como é que alimenta os datasets toda a preocupação que tem que foi inclusive o questionamento ali da Caroline de como que a gente coloca os dados na memória da GPU aí isso tá beleza aí agora a gente vai atacar realmente a questão do treinamento paralelizado em GPU entendeu então a gente vai ver primeiro eh múltiplas gpus isso na próxima aula múltiplas gpus no mesmo nó ou seja numa único computador com várias gpus e também vamos vamos ver como é que isso funciona em vários nós vamos supor que a gente tem um cluster de gpus como que a gente prepara o código para o usufruir desse cluster tá inclusive teve agora da turma um não sei se vocês quiserem acompanhar vai ter as defesas não sei se são abertas essas defesas mas ter as defesas de TCC da turma um e Teve um aluno que foi o eh foi o Rodrigo é que o Rodrigo ele basicamente fez esse esse esforço Ele usou não usou gpus né mas usou daí o tensor Flow distribuído ã para usar o cluster para fazer o treinamento entendeu Não só um único nome se vocês quiserem acompanhar aí vocês me avisem que daí talvez eu eu eu passo para vocês aí o o dia ali da Defesa Ok G eu eso acompanhar se possível Professor Beleza então tá eu vou eu vou passar para vocês depois a a o dia da defesa e aí o no dia da Defesa eu posso aí divulgar pra turma de vocês aí o o a apresentação para vocês até terem uma ideia Acho que até importante vocês olharem para vocês terem uma ideia assim do que que são os tccs né tipo assim que tipos de TCC estão acontecendo e tudo mais sei que tá um pouco longe para vocês mas daí vocês já podem ter uma ideia também ah eu até não tá longe não é não tá longe não V Tá longe não eh bom então isso que isso para resumir

- *Corpus ID:* 6740
- *Score:* 0.8753256797790527
- *URL:* oculto
- *Início:* 00:29:33
- *Fim:* 00:31:43
- *Transcrição:* camadas Qual é a configuração de cada uma delas entendeu E com o tempo Acho que vai mais uma experiência entendeu vai ficando mais fácil mas no início é tentativa e er e daí a importância né de saber usar o paralelismo porque leva tempo né leva tempo PR esperando tá joia é o que ocorre assim por exemplo só para dar um exemplo concreto né tinha uma aluna da professora Viviane que acho que até foi professora de vocês também que ela tava ali acessando uma máquina ali né usando o tensor Flow para fazer os treinamentos dos várias camadas e tal aí Aí como eu ajudo a administrar o Class um dia fui olhar ali tipo ah para ver o que que ela tava fazendo né Tipo esse cara fica sempre curioso né e basicamente tinha três gpus naquela máquina ela tava usando só uma entendeu daí tipo mandei e-mail Olha só tu tem três gpus aqui ó não tem por tu tá usando testor Flow ele é multi GPU entende essas duas três linhas aqui que a gente vai ver logo em seguida aí e tu consegue acelerar entendeu Tipo e usa também os callbacks esse tipo de coisa para tu poder descartar o mais rapidamente possível um um eventual modelo que tu te equivocou ali colocou configuração claramente não deu certo então realmente o paralelismo tem que ser aproveitado para para tu poder iterar mais rápido entendeu professor fala Alexandre eu queria complementar a pergunta é em relação ao Bet a gente tem um modelo que ele tem uma entrada desenhada para uma imagem né então assim internamente ele multiplica os modelos várias 32 vezes ou ele faz uma como é que ele faria essa mágica aí ele ele ele basicamente ele ele ao invés de tu trabalhar assim o modelo continua sendo aquele ali entendeu é 28 por vezes 28 entradinhas entendeu a diferença é que eh em vez de tu estimular a rede com uma única imagem é como se tu botasse na entrada dessa dessa Rede ao mesmo tempo 32 imagens entende e tu faz todo o processo de

- *Corpus ID:* 6981
- *Score:* 0.8752256035804749
- *URL:* oculto
- *Início:* 01:01:41
- *Fim:* 01:03:38
- *Transcrição:* diferentes estão vendo Então simplesmente fazendo esse intercalo já garante um certo embaralhamento tá vindo de arquivos de fontes diferentes tá o fato é que esse interl né só para mostrar uma feature extra aqui é que ele vai ler sequencialmente esses arquivos então ele vai ler um depois vai ler o outro Valeu outro então a gente pode tornar esse esse esse processo um pouco mais rápido usando essa função Num paralel caos nesse caso aqui a gente tá especificando com três threads então daí a gente consegue eh e dizer que daí a gente consegue ler em paralelo né Então olha só agora a gente redefiniu esse esse dataset né vou mandar de novo aqui 20 elementos são os mesmos 20 elementos tá hã a gente pode também especificar em vez de três especificar esse etf data experimental autotune para fazer dizer pro tensor Flow decidir dinamicamente a maior quantidade de trads para ler aqueles dados né Isso pode ajudar a a ele regular a quantidade de trads para maximizar o desempenho Ok então meio que confia no tensor Flow na escolha dele mas se caso a gente quiser a gente pode especificar aqui uma quantidade de trads grandes também para tornar parael então normalmente se a gente usa Any readers né o num paralel caos aqui talvez seja interessante botar também igual a Any readers pelo menos né pra gente ter que a gente tem cinco arquivos abertos ao mesmo tempo que a gente tem cinco leituras paralelas ao mesmo tempo né para ficar não seja mú ou que o número ou que o número de reader seja múltiplo do número de calls ou sei lá ou vice versa porque depende da quantidade de cor sei lá que tá disponível para isso tu pode ter por exemplo 40 arquivos abertos mas ter só 10 cores né daí não cor não não vale a pena tu ter Igual daí né porque daí tu não vai ter cores suficiente para ler aqueles 40 arquivos

- *Corpus ID:* 7705
- *Score:* 0.8746445178985596
- *URL:* oculto
- *Início:* 01:02:05
- *Fim:* 01:04:54
- *Transcrição:* elas são sequenciais eu tenho uma camada depois outra depois outra depois outra depois outra e assim por diante essa visualização já me indica que eu tenho uma certa camada que ela simultaneamente ela alimenta quatro blocos que são depois combinados em um só tá então essa rede aqui esse bloco ele não é um bloco sequencial n porque essa mesma camada previous layer aqui ela alimenta quatro módulos distintos tá então o que que eles propõem eles propõem pegar uma camada o primeiro bloco que é um por um Ele simplesmente faz o ajuste no número de canais o segundo bloco ele faz um ajuste no número de canais e aplica convolução 3x TR o terceiro bloco faz esse ajuste no número de canais e aplica convolução em paralelo de tamanho 5 por5 tá e a última layer paralela ali ela primeiro faz um Max pulling e depois aplica soluções um por um tá e Oi desculpa então eh isso aí É como se você tivesse três eh redes três redes convolucionais n em paralela digamos e que a partir desse filtro aí eh Seria a mesma a mesma ou não eh eu entendi que ele consegue fazer isso aí em paralelo para depois pegar o resultado final dessas três dessas três aí junta tudo no mesmo filtro podia fazer cada um numa rede diferente isso é um único bloco então eu pego uma camada e essa camada em todas as redes anteriores uma camada ela é propagada para apenas outra camada nesse caso essa camada aqui que é a de baixo ela é propagada para quatro outras camadas em paralelo e depois a gente junta o resultado dessas quatro numa única só tá e E aí isso em termos de de codificação ele vai ter Impacto porque a gente não pode mais usar o modelo sequencial lá do tensorflow bar keras tá então Aqueles bloquinhos um por um eles aparecem em paralelo Então são quatro blocos Paralelos que acabam aparecendo Ah nesse módulo aqui tá E aí essa é uma

- *Corpus ID:* 6783
- *Score:* 0.8745914101600647
- *URL:* oculto
- *Início:* 00:03:20
- *Fim:* 00:05:20
- *Transcrição:* desempenho entre CPU e GPU só para você terem uma ideia como que o tensorflow pode ajudar para você vocês para atividades cotidianas que não necessariamente envolve treinamento de uma rede profunda tá E no final da aula hoje a aula vai ser um pouco diferente porque a gente vai terminar a aula falando com slides né emz de falar terminar a aula com atividades dirigidas a gente vai falar sobre dados aí eh como é que a gente eh cria um objeto que é o alimentador de dados pro treinamento tá que é o o a interface TF pdata Então essa é uma é um aspecto de tem bastante aspecto de paralelismo também tá então isso a gente vai ver eh no final da aula em preparação pra próxima aula onde a gente vai realmente aí ver como é que isso implementa tá então hoje a gente só vai discutir conceitos e na próxima aula a gente vai ver realmente atividade dirigida não deu tudo para não deu para colocar tudo na mesma aula tem um outro ponto também então que hoje só falando da estrutura da aula né a gente vai começar com esse com essa parte de callback extensor boards depois nós vamos ter três atividades dirigidas e o que eu queria mencionar é que a primeira atividade dirigida no meu computador não funcionou muito bem no Firefox que é o navegador que eu uso de normalmente né então para esse ad02 nós vamos utilizar Então o Google Chrome aqui que o Google Chrome eu percebi que funciona melhor para quando a gente for instanciar o tensor board dentro do cabbi ok então essa é a estrutura da aula de hoje mas antes de começar a aula só queria comentar uma uma uma um assunto que tem bastante a ver com essas duas disciplinas que a gente tá abordando agora que é um assunto que eu tenho me envolvido bastante eh faz um certo tempo já que que envolve obviamente todas essas ferramentas de ciências de dados

- *Corpus ID:* 7025
- *Score:* 0.8738381862640381
- *URL:* oculto
- *Início:* 00:19:42
- *Fim:* 00:21:44
- *Transcrição:* viu na aula passada e aí a gente põe o paralelismo que a gente precisa ter nesse dataset esse paralelismo vai ser um paralelismo de CPU e a gente usa todas as cpus para preparar os dados para que a GPU possa fazer o treinamento de maneira eficiente então com isso a gente vai ter um paral ismo em CPU e um paralelismo em GPU Ok tem uma outra alternativa também que é a gente usar para aqueles modelos que são de redes convolucionais que trabalha com imagens né E vamos supor que a gente cria um modelo que que tenha como entrada não uma imagem não seria um modelo de classificação seria um modelo por exemplo de de por exemplo comparar duas imagens identificar idades entre elas né a gente teria que treinar o modelo e tudo mais né aí esse modelo por exemplo teria teria duas imagens de entrada para as quais a gente teria que fazer algum tipo de convolução Independente de cada uma delas antes de chegar no Coração lá no modelo que junta esses essas essas informações que foram convolucion né Então nesse caso a gente poderia empregar mais de uma GPU para fazer essas convolução de maneira paralela tipo assim tu teria na entrada duas imagens tu usaria duas gpus processar essas imagens ISO seria uma possibilidade também então ã se a gente olhar olhar para todas essas alternativas aqui a gente tá usando a GPU sempre para problemas distintos né então eh tanto no ponto dois aqui que poderia Talvez quear uma dúvida tipo aqui a gente tá usando uma GPU né então a gente tá se a gente tem mais de uma GPU a gente teria que fazer também né tipo assim trabalhar como se fosse nessa nesse primeira situação aqui então o que que a gente vai ver agora a gente vai ver agora uma situação diferente nessas aqui né que são bem básicas Essas aqui que permitem a gente na realidade eh usar várias gpus para treinar um modelo certo que seria o é o que eu quero


**Resposta gerada pelo LLM**: O TensorFlow oferece diferentes estratégias para paralelismo de dados.  A principal é a `TF distributed mirror strategy`, que espelha o modelo em cada placa, processa batches diferentes e depois combina os resultados. Uma alternativa é a estratégia de parâmetros centralizados, onde um servidor gerencia os parâmetros do modelo, atualizando-os de forma síncrona.  Existem também outras formas de paralelização, como treinar vários modelos em paralelo, um por GPU, ou usar o `dataset.prefetch` para pré-processamento paralelo em CPU enquanto a GPU treina.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to all parts of the PROMPT regarding how data parallelism is implemented in TensorFlow. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining how data parallelism is implemented in TensorFlow. It mentions specific strategies such as the `TF distributed mirror strategy` and centralized parameter strategies, which are relevant to the implementation of data parallelism. Additionally, it discusses other methods like training multiple models in parallel and using `dataset.prefetch`, which further elaborates on the topic. Since the RESPONSE comprehensively covers the implementation of data parallelism in TensorFlow, it is relevant to the entire PROMPT.  Score: 3


---
