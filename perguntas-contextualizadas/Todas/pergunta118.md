**Pergunta 118**: Qual a função da camada de saída na rede U-Net?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 7961
- *Score:* 0.837855339050293
- *URL:* oculto
- *Início:* 00:12:01
- *Fim:* 00:14:48
- *Transcrição:* Vou dobrando né 64 128 256 512 tá E aí o bottleneck que que é o bottleneck o bottleneck É exatamente a final final do Encoder é é versão codificada da entrada tá E aí se aplica um outro bloco de convolução com 124 canais então em tese Esse cara tem que ter dimensão 16 por 16 por 1000 24 Ok chegou a parte de final do U agora vou começar a subir a outra perninha do u Como é que eu faço isso bom a minha rotina upsample and residual Block ela faz duas coisas ela recebe a camada anterior que no caso é o Bottom ne recebe a conexão do skip e faz a concatenação desse cara aqui tá então basicamente o que que ele vai fazer com esse cara são para 512 canais tá então ele vai receber esse cara aqui que tem 512 esse de 1024 ele reduz para 512 e concatena e vai fazer isso tá cada vez aumentando a resolução espacial e diminuindo o número de filtros tá então tipicamente o Decoder é uma versão espelhada do Encoder tá e finalmente Qual é a camada de saída vai ser uma camada com três saídas né então aqui eu tô usando convolução um por um né então basicamente eu só tô reduzindo o número de canais da saída para três canais porque são três categorias tá com ativação soft Max certo e aqui eu tô criando o meu modelo né basicamente o que que eu fiz aqui eu criei uma camada de saída fui plugando essa camada desculpa criei uma camada de entrada fui criando várias conexões entre a camada de entrada e camadas intermediárias no final das contas eu conecto essas camadas intermediárias numa camada de saída tá então esse processo vai gerando o meu grafo de conectividade das camadas quando eu chamo o modelo Model aqui do keras basicamente ele vai achar ele já vai pegar as conexões que ligam os inputs aos out Ok e Relembrando o que que o Model summary dá ele me dá um resumo tá das ã um resumo das eh camadas né Eh que a gente foi construindo ao longo da rede

- *Corpus ID:* 7721
- *Score:* 0.8328610062599182
- *URL:* oculto
- *Início:* 01:33:58
- *Fim:* 01:36:35
- *Transcrição:* Ah então o que que eu vou fazer esse cara aqui ele pega basicamente o ú a última dimensão do tensor de entrada né que corresponde a um número de canais tá no tensor Flow a última dimensão de um tenso Era exatamente o número de canais então ch que é o número de canais é a última dimensão do meu tensor tá aí eu vou pegar ah o tensor de entrada Vou fazer o Global Average pulling né que é exatamente essa ideia de ã de calcular a a média né ao longo de cada fatia espacial em cima desse cara eu aplico uma camada densa com número de neurônios e dado por essa esse fator né que é o número de canais pela razão certo e eu vou te dizer que eu acho que faltou um cara aqui tá acho que eu ratei porque tem uma outra tem duas fully connected tá tá certo eu eu que viajei Tá certo isso aí tá então ten uma densa o que eu viajei aqui é que a ativação e a camada densa estão na mesma chamada tá então até poderia criar uma camada densa com ativação linear ou seja sem ativação e depois criar uma camada específica para ativação relu Tá mas dentro da camada densa eu já posso indicar qual é a ativação função de ativação usado então na primeira camada é densa com relu e a segunda é densa com sigmoide e depois eu faço a multiplicação do tensor de entrada que é o x por por esse bloco X1 aqui que é exatamente esse Quiz and exide beleza até aí pessoal beleza tá eh eu quero ver se eu fecho Isso aí até umas 10:15 pra gente fazer o nosso Break Tá mas só para não não perder a lógica eh tem todo todo uma outra categoria de redes que Visa reduzir o custo computacional tá então como eu falei para vocês convolução são pesadas né Por

- *Corpus ID:* 7718
- *Score:* 0.8321090340614319
- *URL:* oculto
- *Início:* 01:27:34
- *Fim:* 01:30:37
- *Transcrição:* camada ah a saída de cada bloquinho aqui ela conecta com todas as demais saídas tá Ah e a outra diferença entre a a a rede ã residual e uma den net é que a maneira de combinar essa informação não é por soma Mas é por concatenação Ok bom então assim como as resnet também tem diferentes maneiras de combinar esses blocos densos tá então tem lá dennet 121 169 201 e novamente nada impede vocês criem a dence net customizada de vocês com o número de canais que vocês acharem adequado tá então Aqueles D ali são basicamente ah os os blocos densos de uma ah dennet 12 21 bom em 2018 uns tem uns caras que propuseram as Tais de Squeeze and excitation Networks eh qual é a grande sacada desses caras ou a contribuição desses caras aqui tá é exatamente explorar as interdependências entre os canais tá então a verdade é que uma um bloco convolucional implicitamente ele mistura todos os canais Tá mas vamos assumir que eu tenha tá uma um um tensor de entrada que tenha dimensões H por w né que seria altura largura e seu número de canais tá Qual é a ideia bsica do dess bloco sque and exite primeiraa que ele faz é basicamente agregar informação espacial desse tensor tá fazendo um pulling espacial um Average pulling então para cada fatia H por w eu calculo a média desses caras aí tá então a dimensão espacial vai ser condensada num único valor então eu vou gerar um ã um tensor que tem tamanho um por 1 por C certo e aí eu vou ter tal e e esse camado aqui essa camada que é basicamente uma camada fully connected que vai combinar todos os canais com eles próprios então uma camada fully connected faz isso né todo mundo conversa com todo mundo então eu vou criar uma camada fully connected com a

- *Corpus ID:* 2069
- *Score:* 0.8317285180091858
- *URL:* oculto
- *Início:* 00:30:41
- *Fim:* 00:32:46
- *Transcrição:* relação possível entre a saídas a combinação de saídas dos meus modelos base e a minha saída real tá então claro que a gente aqui do ponto de vista né de definição conceitual de stecking você fala em camadas sucessivas interligadas né ou seja tem um conjunto de classificadores a saída desses classificadores passam para outros classificadores e assim por diante mas na prática isso é feito só em dois níveis tá Quais são esses dois níveis Eu tenho meus classificadores bases que são esses modelos individuais que vão aprender a partir dos dados de Treinamento originais certo então enfim eu tenho aqui um conjunto de modelos tá que foi treinado com base nos dados de Treinamento que a gente tem e tem um nível 1 que seria o corredor o combinador é um modelo que aprende a combinar esses a saída desses modelos então na prática a gente normalmente tem somente esses dois níveis porque provavelmente não sei exatamente uma referência de cabeça assim mas provavelmente para chegar nessa conclusão que a gente pode usar dois níveis alguns estudos devem ter mostrado que não tem tanto ganho de desempenho fazendo múltiplas combinações e múltiplas camadas e isso com certeza aumentaria muito custo tá então A ideia é isso eu tenho os meus dados treino um conjunto de ter classificadores né E aí ou regressores e aí as saídas desses modelos passam por um segundo modelo que é treinado com algoritmo de aprendizados previsionado tá então qual é a ideia que se eu treinar um modelo para aprender essa combinação esse modelo pode aprender padrões muito mais robus sofisticados que uma votação né então eventualmente assim qual é o padrão aqui que faz com que eu combine essas entradas que seria sair dos modelos para acertar o máximo possível em relação a saída esperada então de repente a votação majoritária funciona em alguns casos e outras regras funciona

- *Corpus ID:* 7960
- *Score:* 0.8305196166038513
- *URL:* oculto
- *Início:* 00:10:01
- *Fim:* 00:12:47
- *Transcrição:* transposto com número de filtros reduzidos tá com tamanho 3 por3 e Stride 2 beleza bom ah com base nessas funções básicas eu vou construir a minha unet tá vejam que a unet também não pode ser construída por um modelo sequencial né então dentro do queras eu posso criar um modelo com sequential E aí eu boto uma camadinha atrás da outra por que que esse cara não pode ser sequencial exatamente pel aqueles atalhos de concatenação Tá mas a montagem da rede é super simples né então eu crio uma camada de entrada né que é com com uma função de input lá na layer de input tensor Flow com a dimensão desejada tá E aí o que que eu vou fazer eu vou aplicar a primeira o primeiro bloco de Down sample com ã a camada de entrada e 64 filtros tá que é também é diferente do que tava lá acho que começou com 32 lá né naquele exemplinho da tela tá vejam que eu também tô armazedo a cópia do bloco que eu vou precisar depois pro decoder aí em cima do X que é o caminho para baixo eu vou aplicar outro bloco de de de Down sample e vou manter o a a outra skip Connection vou fazer isso de novo e vou fazer isso de novo então na minha unet eu tenho uma profundidade de quatro níveis tá então é 128 a resolução espacial baixa para 64 baixa para 32 e baixa para 16 Essa é resolução espacial da parte de baixo da minha unet tá e o número de canais Eu Vou dobrando né 64 128 256 512 tá E aí o bottleneck que que é o bottleneck o bottleneck É exatamente a final final do Encoder é é versão codificada da entrada tá E aí se aplica um outro bloco de convolução com 124 canais então em tese Esse cara tem que ter dimensão 16 por 16 por 1000 24 Ok chegou a parte de final do U agora vou começar a subir a outra perninha do u Como é que eu faço isso bom a minha rotina upsample and residual Block ela faz duas coisas ela recebe a

- *Corpus ID:* 7959
- *Score:* 0.8302853107452393
- *URL:* oculto
- *Início:* 00:08:00
- *Fim:* 00:10:41
- *Transcrição:* é exatamente o cara que vai ser concatenado lá do lado tá então na hora que eu fizer o Down sample é importante que eu mantenha uma cópia do tensor tá antes de fazer o Down sample que é o que vou ter que alimentar a hora do do Decoder depois então essa função da um sample Block que que ela faz ela aplica o filtro convolucional tá Ah vejam aqui que o pading same né nesse bloc a ele pega a cópia PR conexão residual faz o maxing e aqui tá usando um drop Relembrando Qual é a moral do Drop desativar alguns neurônios isso e qual qual é o objetivo n Por que a gente faz isso ex qual eito você ou mexer no parâmetro de dropout e ver o que que acontece tá então às vezes até a gente coloca mas o efeito não é o desejado bom Aqui tem o o o bloco básico pro upsample tá então como é que funciona um bloco básico de OP sample eu vê eu vem o bloco né que que veio do Decoder ou da camada anterior n ou desculpa eh ou o final do Encoder ou o bloco que veio da camada anterior do Decoder tá então o que que eu tenho que fazer eu vou fazer uma eh com 2D uma convolução 2D transposta vejam que na con transposta eu seto o número de filtros né Então lemam aquela dúvida lá que pô eu tinha 64 baixou para 32 Por quê Tá exatamente Porque durante a chamada da rede como a gente vai ver aqui na na na construção da eonet né a gente fez a chamada do bloco com 2D transposto com número de filtros reduzidos tá com tamanho 3 por3 e Stride 2 beleza bom ah com base nessas funções básicas eu vou construir a minha unet tá vejam que a unet também não pode ser construída por um modelo sequencial né então dentro do queras eu posso criar um modelo com sequential E aí eu boto uma camadinha atrás da outra por que que esse cara não pode ser sequencial exatamente pel

- *Corpus ID:* 3421
- *Score:* 0.8297540545463562
- *URL:* oculto
- *Início:* 00:41:57
- *Fim:* 00:44:12
- *Transcrição:* pesos muito próximos a zero a rede acaba não conseguindo fazer nada e se eu boto um lambda muito pequeno lâmpada Zero anula isso aqui né a rede normal ali com o overflite em máximo Então aí que aí que ajudar na pergunta de um milhão de dólares né o efeito do lambda é isso aqui mesmo né é ele controla o quanto que eu que eu quero né restringir o treino da rede é o ajuste dos pesos E aí então eu tenho que encontrar né sei lá 0.1 0.3 tem que testar valores aqui ok então assim né que é a regularização assim que eu uso né a regularização adiciono ela dentro da camada então eu posso ter algumas camadas com regularização outra sem Enfim pode ser pode ser mesclado aqui né Umas com L1 umas com L2 Mas normalmente né o treino toda a rede com todas as camadas com a mesma regularização né talvez com o mesmo lambda tem né para talvez é ajuda né Um pouquinho de elimina uma parte da complexidade né que imagina quantas combinações possíveis né se eu tenho cinco camadas aí eu boto a primeira com ela em uma segunda com L2 o terceiro com a L1 aí depois eu tenho que trocar aí testar outra configuração então é muito é muita possibilidade e acaba não tendo como testar [Música] Então isso é regularização OK agora vou falar do Drop out se não ficou Nenhuma Dúvida tá ficou dúvidas me avisa drop out significa em inglês né tradução aproximada que eu abandono e a gente professor universitário morde de medo disso aqui na evasão né drophalt em inglês né evasão escolar nas universidades para nós aqui o abandono né no nosso caso aqui nós vamos o drop out não vai ser uma coisa ruim né Porque isso é uma coisa boa para treinar a rede porque qualquer ideia tá é que numa rede normal aqui né todos os neurônios estão sempre ativados do tempo

- *Corpus ID:* 7742
- *Score:* 0.8290673494338989
- *URL:* oculto
- *Início:* 00:22:29
- *Fim:* 00:24:56
- *Transcrição:* uma função matemática e tem tanto no nump quanto no tensor Flow que ela exatamente me retorna dado um vetor por exemplo ele me retorna Qual é o elemento deste vetor que retornou o maior valor tá então o Max de um vetor é o valor máximo o arg Max é a posição onde esse valor máximo foi atingido tá então como eu tô interessado na posição né que é exatamente qual é o neurônio que gerou o máximo a gente usa essa de saída argmax para isso aí beleza bom na alexnet aqui tá então o que que dá para perceber tá que a última camada sobretudo mudou de 10 para 1000 neurônios tá eh eles também colocaram camadas de drop Out para tentar diminuir o overfitting tá então a questão do overfitting também é algo muito delicado é muito difícil ver quando vai ter overfitting ou quando não vai ter Tá mas o dropout é uma estratégia usada exatamente visando reduzir o overfitting né E aí como eu tinha dito antes né eu tenho duas camadas ocultas de 4096 antes da final tá que que acontece se eu mudar de duas para uma ou mudar de 4.096 paraa metade ou pro dobro sei lá tem que testar eu não sei tá então é tudo muito difícil as coisas são muito pladas entre a natureza do dataset o tamanho do dataset e e e basicamente a arquitetura da rede tá então o que que é natural é a última camada de novo sem porque eu tenho 100 classes e softmax porque é um problema de classificação mas do que isso só tem um rótulo né uma classificação competitiva tá bom tem uma camada que ela apareceu naquele contexto do Squeeze and excite que é o Global Average pulling tá E na verdade nesse histórico que a gente fez com os back bonus eu não sei se vocês chegaram a reparar mas ela aparecia em alguns deles também tá o que que o Global Average pulling faz tá ele basicamente dado um tensor w por H por C

- *Corpus ID:* 7658
- *Score:* 0.8290160298347473
- *URL:* oculto
- *Início:* 01:15:20
- *Fim:* 01:17:46
- *Transcrição:* menos né a a Aquelas nossas saídas Então eu tenho basicamente um eh um filtro para cada uma daquelas nossas entradas né E aí a a a conta que a gente faz para saber quantos pesos e quantos bases a gente vai ter é similar a anterior né a gente tem assumindo aqui zero pading né ou algum tipo de pading eh a gente vai ter ã o número de canais de entrada vezes o número de canais de saída vezes o tamanho do Kernel né pesos e mais o número de camadas de saída biases tá ã então isso aqui no final das contas acaba sendo bem bem comum né nas nossas aplicações tá esse negócio do canal de saída aí poder ser menor é quando você achata deixaa ela toda cinza né Isso então isso na verdade isso aqui é bem é bem comum em em em aplicações de segmentação por exemplo né em aplicações de segmentação geralmente a gente tem como como entrada né uma imagem colorida e no final uma máscara de segmentação e essa máscara de segmentação é uma é como se fosse uma imagem grayscale né em que cada cor do Pixel daquela imagem representa um rótulo né ah é pessoa é eh calçada é isso e aquilo então sim é bastante comum nesse sentido [Música] Tá bom a gente comentou que tem o o pulling né Então as nossas camadas de convolução ali até Deixa eu voltar nesse exemplo isso aqui é razoavelmente comum tá pessoal Geralmente as nossas camadas de convolução tem essa cara eu começo com número de canais tá pode ser um pode ser três não sei da minha entrada né e Gero um conjunto de saídas que tem mais canais que a minha entrada tá então é comum eu aumentar o número de de de canais de saída tá nota que se eu fizer só isso e aumentando o número de canais de saída eu vou ter eu vou acabar tendo

- *Corpus ID:* 7709
- *Score:* 0.8290058374404907
- *URL:* oculto
- *Início:* 01:10:05
- *Fim:* 01:12:39
- *Transcrição:* propuseram isso também não não não sabiam tá então o que que acaba acontecendo é cara eu vou testar uma arquitetura uma camada funcionou deu resultado bom aí eu vou tentar inventar uma explicação para esse cara aí tá com relação a lógica de implementação tá claro como é que se faz gente alguma dúvida Nisso aí a lógica sim tá beleza então é bem tranquilo mesmo né se eu quiser criar um bloco né Um bloco que não apresenta camadas sequenciais é extremamente tranquilo de fazer isso é uma função inha no Python que recebe uma entrada e essa mesma entrada ela é acessada Ah pelos quatro caminhos Paralelos pelo caminho um pelo caminho dois pelo caminho três pelo caminho quatro tá E neste bloco aqui esses quatro caminhos são concatenados no final que é exatamente a saída da função OK tá aí depois do inception aí os caras inventaram outras loucuras lá extensão dos módulos inception E aí vem uma coisa bem legal que é o tal do uso de conexões residuais tá eh Vocês já viram já ouviram falar em conexão residual eu não sei se o Anderson chegou a comentar isso para vocês eu não não tá então eh tem um backbone super conhecido ele foi proposto há quase 10 anos ele ainda é bastante Popular hoje em dia tá então o que que são as Tais resnet tá então as resnet elas foram a ideia básica das redes Eh desculpa as conexões residuais elas formam a ideia básica de uma uma rede residual tá Qual é a ideia básica tá a ideia básica é que eu vou criar um atalho n Então vamos assumir que isso aqui é uma rede tá então ela tá indo ao contrário dos exemplos anteriores de cima para baixo tá Então nesse exemplo


**Resposta gerada pelo LLM**: A camada de saída da U-Net possui três saídas, usando convolução 1x1 para reduzir o número de canais para três (uma para cada categoria), com ativação SoftMax.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately describe the function of the output layer in a U-Net architecture. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining the function of the output layer in the U-Net model. It mentions the use of a 1x1 convolution to reduce the number of channels to three, which corresponds to the categories in a segmentation task, and it correctly identifies the activation function used (SoftMax). This information is relevant and complete in the context of the PROMPT.  Score: 3


---
