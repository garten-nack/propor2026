**Pergunta 86**: Por que a rede U-Net não pode ser construída com um modelo sequencial?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 7959
- *Score:* 0.8566197156906128
- *URL:* oculto
- *Início:* 00:08:00
- *Fim:* 00:10:41
- *Transcrição:* é exatamente o cara que vai ser concatenado lá do lado tá então na hora que eu fizer o Down sample é importante que eu mantenha uma cópia do tensor tá antes de fazer o Down sample que é o que vou ter que alimentar a hora do do Decoder depois então essa função da um sample Block que que ela faz ela aplica o filtro convolucional tá Ah vejam aqui que o pading same né nesse bloc a ele pega a cópia PR conexão residual faz o maxing e aqui tá usando um drop Relembrando Qual é a moral do Drop desativar alguns neurônios isso e qual qual é o objetivo n Por que a gente faz isso ex qual eito você ou mexer no parâmetro de dropout e ver o que que acontece tá então às vezes até a gente coloca mas o efeito não é o desejado bom Aqui tem o o o bloco básico pro upsample tá então como é que funciona um bloco básico de OP sample eu vê eu vem o bloco né que que veio do Decoder ou da camada anterior n ou desculpa eh ou o final do Encoder ou o bloco que veio da camada anterior do Decoder tá então o que que eu tenho que fazer eu vou fazer uma eh com 2D uma convolução 2D transposta vejam que na con transposta eu seto o número de filtros né Então lemam aquela dúvida lá que pô eu tinha 64 baixou para 32 Por quê Tá exatamente Porque durante a chamada da rede como a gente vai ver aqui na na na construção da eonet né a gente fez a chamada do bloco com 2D transposto com número de filtros reduzidos tá com tamanho 3 por3 e Stride 2 beleza bom ah com base nessas funções básicas eu vou construir a minha unet tá vejam que a unet também não pode ser construída por um modelo sequencial né então dentro do queras eu posso criar um modelo com sequential E aí eu boto uma camadinha atrás da outra por que que esse cara não pode ser sequencial exatamente pel

- *Corpus ID:* 7960
- *Score:* 0.8549192547798157
- *URL:* oculto
- *Início:* 00:10:01
- *Fim:* 00:12:47
- *Transcrição:* transposto com número de filtros reduzidos tá com tamanho 3 por3 e Stride 2 beleza bom ah com base nessas funções básicas eu vou construir a minha unet tá vejam que a unet também não pode ser construída por um modelo sequencial né então dentro do queras eu posso criar um modelo com sequential E aí eu boto uma camadinha atrás da outra por que que esse cara não pode ser sequencial exatamente pel aqueles atalhos de concatenação Tá mas a montagem da rede é super simples né então eu crio uma camada de entrada né que é com com uma função de input lá na layer de input tensor Flow com a dimensão desejada tá E aí o que que eu vou fazer eu vou aplicar a primeira o primeiro bloco de Down sample com ã a camada de entrada e 64 filtros tá que é também é diferente do que tava lá acho que começou com 32 lá né naquele exemplinho da tela tá vejam que eu também tô armazedo a cópia do bloco que eu vou precisar depois pro decoder aí em cima do X que é o caminho para baixo eu vou aplicar outro bloco de de de Down sample e vou manter o a a outra skip Connection vou fazer isso de novo e vou fazer isso de novo então na minha unet eu tenho uma profundidade de quatro níveis tá então é 128 a resolução espacial baixa para 64 baixa para 32 e baixa para 16 Essa é resolução espacial da parte de baixo da minha unet tá e o número de canais Eu Vou dobrando né 64 128 256 512 tá E aí o bottleneck que que é o bottleneck o bottleneck É exatamente a final final do Encoder é é versão codificada da entrada tá E aí se aplica um outro bloco de convolução com 124 canais então em tese Esse cara tem que ter dimensão 16 por 16 por 1000 24 Ok chegou a parte de final do U agora vou começar a subir a outra perninha do u Como é que eu faço isso bom a minha rotina upsample and residual Block ela faz duas coisas ela recebe a

- *Corpus ID:* 7029
- *Score:* 0.8531036972999573
- *URL:* oculto
- *Início:* 00:25:50
- *Fim:* 00:27:49
- *Transcrição:* sequencialmente abordadas Então existe uma sequencialidade Então tem que imaginar que tem o back propagation também aqui a gente tá fazendo só o for né né mas a gente tem que imaginar que uma vez chegou lá no final a gente sabe qual a resposta a gente tem que fazer o o o a propagação reversa aí né para Então vai ter eh reajustes eh sendo propagados do final para o início também da nossa da nossa rede né Então essa seria uma abordagem possível envolve comunicação e como é que a gente lida com essa questão do da da sequencialidade aqui das camadas a gente vai fazer que enquanto a primeira GPU processou passou paraa segunda a primeira GPU já pega mais dados para chegar já fazer a próxima etapa certo Alexandre marqu pode pode perguntar Professor eh nessa questão de para ter eficiência realmente paralelismo essas camadas T que ter uma quantidade absurda de neurônios né porque isso como uma uma camada espera outra não não assim não tem um o paralelismo fica bem limitado então assim ele tá ele só faz sentido se a camada for em extensão muito grande sem muitos neurônios não é isso isso isso isso é um ponto importante que tu comentasse essa camada tem que ser bem grande realmente para fazer valer a pena isso aqui né então tem um outro detalhe também Ach mar que que acho que talvez tu tenha te enganado aí um pouco que assim eh tem essa sequencialidade né mas nota bem vamos supor que tu tem imagens aqui na entrada aqui embaixo então tu fornece a primeira imagem para essa GPU ela processa e passa pra próxima quando a próxima estiver trabal olando a primeira GPU já recebe a próxima imagem entendeu então na realidade tu pode ter assim um paralelismo fazendo com que todas as gpus estejam trabalhando ao mesmo tempo entendeu desde que tu tenha uma alimentação de dados na entrada bastante eh eficiente Ok gem pode perguntar

- *Corpus ID:* 7717
- *Score:* 0.8519083857536316
- *URL:* oculto
- *Início:* 01:25:36
- *Fim:* 01:28:23
- *Transcrição:* inception que eu fazia concatenação nesse bloco aqui eu vou fazer a adição Tá mas exatamente a mesma coisa tá em termos de de de estrutura de código tá E aí criada essa função eu posso criar vários blocos desses e pegar um bloco conectando no outro tá então esse bloco eu posso usar dentro de um modelo sequencial por exemplo bom o que que são as dens nets tá sem entrar em maiores detalhes a resnet ela pega uma uma uma um tensor e faz uma skip Connection paraa frente o que que a dens NET vai fazer ela vai pegar um tensor e ela vai propagar para vários blocos da frente tá então a ideia de uma dennet é basicamente uma resnet turbinada né então a resnet ela Dá saltos tá de tantos em tantos o que que oet vai fazer eventualmente a saída da primeira camada vai ter uma conexão lá no final da rede tá então essa ideia de propagação de formação do fim pro início ela vai acontecer até de maneira mais rápida né porque eu vou pegar a primeira camada né Vocês estão vendo essa linha aqui vermelha que conecta a a primeira primeiro bloco aqui com a saída né então todo esse bloco aqui quando eu a informação chegar aqui de cara eu já vou propagar pra entrada tá Talvez eu tenha exagerado quando eu disse que a saída vai conectar a entrada vai que tá direto na saída mas isso é válido dentro de um bloco denso tá então dentro de um Bloc de um bloco denso eu tenho conexão de todos para todos n então cada camada ah a saída de cada bloquinho aqui ela conecta com todas as demais saídas tá Ah e a outra diferença entre a a a rede ã residual e uma den net é que a maneira de combinar essa informação não é por soma Mas é por concatenação Ok bom então assim como as resnet também tem diferentes maneiras de combinar esses blocos densos tá então tem lá dennet 121 169 201 e novamente

- *Corpus ID:* 7031
- *Score:* 0.850906491279602
- *URL:* oculto
- *Início:* 00:28:52
- *Fim:* 00:30:58
- *Transcrição:* ser sequencial mas o processamento das imagens para do B inteiro para I que que pode ser um custo bem grande né aí tu consegue fazer de maneira pipeline Entendeu Tá certo então essa é uma abordagem né Eh outra abordagem né Eh seria fazer da da da da última forma aqui né Eu realmente quebrar Ah o meu modelo no meio nesse caso que tô usando duas gpus abrindo no lado direito aqui ã o o que que acontece aqui né é que ã tu vai ter por exemplo comunicação entre as camadas né Eh e tipo assim e entre as e entre as duas gpus de maneira bastante massiva né vai ter realmente muita comunicação nesse caso então Eh acaba tendo uma um custo eh elevado isso aqui porque tu vai ter que fazer decisões talvez vai ficar desequilibrado esse tipo de coisa né Aqui também tá desequilibrado né porque a última GPU que só tem um neurônio para processar então tipo assim é bem desequilibrado então h existem vários desafios e esses desafios já aparecem quando a rede neural é totalmente conectada tá no caso de redes neurais convolucionais né onde algumas camadas estão parcialmente conectadas é o exemplo desse aqui né a gente tem uma entrada grande aqui que seria por exemplo duas imagens né E uma e um pedaço dessa imagem vai para um lado e out ou uma imagem vai para um lado e o outro e outra imagem vai para outro lado ou se a gente imaginar que só tem uma imagem na entrada essa imagem vai se quebrada em dois pedaços e aí nós temos ã eh camadas que são elas são são no mesmo nível Mas elas estão separadas eh tem tem pedaços dela como a gente pode ver aqui na figura nesse caso né a gente pode fazer o paralelismo da forma que a gente viu por último antes né E aí a gente vai ter menos comunicação né Porque aqui no início vai ter uma comunicação entre as gpus mas depois as duas gpus podem avançar ainda que sejam

- *Corpus ID:* 6793
- *Score:* 0.850694477558136
- *URL:* oculto
- *Início:* 00:19:23
- *Fim:* 00:21:27
- *Transcrição:* vez instanciado né para executar o teu workflow E aí tu tem certeza que sempre vai funcionar entendeu É eu tô vendo aqui que tem pacotes para instalar você pode instalar só binário só pacote tem a distribuição né Isso é tu pode instalar o sistema teu todo pode ser um sistemax também entende em vez de usar Debian em vez de usar ubun pode serx entendeu Mas a mas a gente não tem usado ass aparentemente tem imagem aliás tem algumas imagens do di no docker legal beleza é fica é fiquem aí só para saber que existe isso aí também né seria o que a gente tem considerado melhor assim para parte de reprodutibilidade então a gente se vocês tiverem interesse a gente ministrou uma um minicurso sobre wicks agora numa Escola Regional que teve aqui em Porto Alegre no primeiro semestre aí eu posso passar esse material posso recuperar esse material esse aí eu acredito que não esteja ainda disponível Tá mas eu posso passar daí o pdf E aí é um tutorial handson assim realmente quais são os comandos que tem que executar para colocar a mão na massa Ok ã bom acho que vamos paraa aula então Eh o tempo urge Então nós vamos começar então a aula de hoje então né ã e sobre o tensorboard cbexs Então nós vamos começar então falando sobre o tensor Bard de uma maneira geral mas antes de entrar realmente falando tensorboard nós vamos comentar Então como é que funciona os callbacks tá isso a gente já já falou rapidinho ali na aula passada Então nós vamos seg seguir falando sobre isso ok então ã só para relembrar o fluxo habitual né do modelo sequencial lá de treinamento de redes Profundas a gente Define um modelo esse modelo vai ser um modelo sequencial porque envolve camadas né Essas camadas são sequenciais a gente vai configurar o modelo com compile E aí depois a gente vai fazer o treinamento com Fit e essa aqui é a parte que em geral vai demorar

- *Corpus ID:* 5736
- *Score:* 0.8491381406784058
- *URL:* oculto
- *Início:* 00:48:47
- *Fim:* 00:50:51
- *Transcrição:* sequencial el sequenci é tudo sequencial eh foi rápido né nosso texo porque éa tudo coisa pequena né mas se tu tivesse trabalhando com milhões de de matrizes e com milhões de linhas e inúmeras colunas ficaria tu não teria um paralelismo assim o paralelismo que nem eu falei antes né gar o paralelismo que tu terias seria por causa das bibliotecas que o que o num pai usa não por causa do num pai entendeu Então ele continua sendo um pacote puramente sequencial entendeu ele era do paralelismo de outras bibliotecas né mas mas ele sozinho não tem paralelismo Hum tá certo Entendi obrigado então a a ideia agora é que esses algoritmos que a gente vai que a gente já tá usando né tipo assim ah aquelas operações que a gente tá usando as funções universais ali por exemplo tipo Essas funções universais que a gente tá usando ela ou mesmo fatiamento ou a propagação automática tudo isso vai acontecer de maneira transparente certo que a gente vai est usando um um mider que vai pegar aquele fatiamento em vez de aplicar sobre todos os dados ele vai pegar esse fatiamento que a gente especifica ficou e fazer o fatiamento considerando os vários blocos que a gente tem então eu gosto de usar sempre o exemplo de filtragem né Vocês Imaginem um arranjo muito muito grande separado em 100 pedaços eu quero fazer um filtro né aplicar uma máscara né que a gente que nem a gente explicou ali no final Então essa máscara ela vai também ter que ser quebrada em 100 pedaços para existir uma correspondência com os dados né E aí ela é aplicada em paralelo em cada um dos pedaços isso vai então me retornar seleções e essas seleções Então são concatenadas para eu ter a filtragem final o resultad do filtro final né geral então isso pode parecer assim pô mas eu vou ter que fazer isso à mão né na realidade não porque o dask já faz isso para nós então ã só para ilustrar um pouco assim como é que isso funciona essa questão do dos algoritmos pro Bloco

- *Corpus ID:* 6694
- *Score:* 0.8482552170753479
- *URL:* oculto
- *Início:* 00:29:17
- *Fim:* 00:31:29
- *Transcrição:* conclusão Olha só esse meu modelo é tribom muito da da da teu intelecto foi eh dedicado em escolher Quantas camadas tinha como é que é conexão entre as camadas quais são os tipos de camadas Qual é a sequência Ok então por isso que normalmente o pessoal não disponibiliza o modelo né quando esse modelo foi foi investido um tempo muito grande para conceber ele depois para treinar Claro T um tempo muito grande e paraar mas aí mas aí tipo Google da vida uma essas grandes esses grandes players eles vão ter um parque tecnológico bem grande e então eles têm eles vão ter tempo eles têm poder de processamento até encontrar de repente uma rede eles vão por exemplo sempre tá na frente de uma pessoa que pode ter até um intelecto melhor para produzir isso mas ele não tem tempo nem tem infraestrutura suficiente para gerar essa rede é né e é é bem vender alguma coisa por exemplo é tem é realmente bem complicado porque não basta tu ter boas ideias para tu construir a rede né e ah não essa minha rede neural é bem inovadora ela que tem essa essa configuração de camadas os tipos são desse se depois para tu avaliar tu vai precisar treinar ela né e treinar é um tu precisa de um parque computacional muito grande né para então acaba acontecendo realmente que quem ganha é quem tem mais recurso computacional para fazer o treinamento para testar né tipo assim para chegar numa determinada versão de rede neural né provavelmente teve muito teste antes que falhou né que foi treinado treinado depois não ficou bom E aí muda Enfim então tem isso tem toda uma área de de pesquisa sobre isso aí né bom então a gente vai usar o tensor Flow né uma biblioteca para Prado profunda ela é de código aberto desde 2015 escrita em C mais mais ela foi iniciada dentro do Google né e depois ela foi Tornada pública digamos assim pra comunidade

- *Corpus ID:* 7705
- *Score:* 0.8477652072906494
- *URL:* oculto
- *Início:* 01:02:05
- *Fim:* 01:04:54
- *Transcrição:* elas são sequenciais eu tenho uma camada depois outra depois outra depois outra depois outra e assim por diante essa visualização já me indica que eu tenho uma certa camada que ela simultaneamente ela alimenta quatro blocos que são depois combinados em um só tá então essa rede aqui esse bloco ele não é um bloco sequencial n porque essa mesma camada previous layer aqui ela alimenta quatro módulos distintos tá então o que que eles propõem eles propõem pegar uma camada o primeiro bloco que é um por um Ele simplesmente faz o ajuste no número de canais o segundo bloco ele faz um ajuste no número de canais e aplica convolução 3x TR o terceiro bloco faz esse ajuste no número de canais e aplica convolução em paralelo de tamanho 5 por5 tá e a última layer paralela ali ela primeiro faz um Max pulling e depois aplica soluções um por um tá e Oi desculpa então eh isso aí É como se você tivesse três eh redes três redes convolucionais n em paralela digamos e que a partir desse filtro aí eh Seria a mesma a mesma ou não eh eu entendi que ele consegue fazer isso aí em paralelo para depois pegar o resultado final dessas três dessas três aí junta tudo no mesmo filtro podia fazer cada um numa rede diferente isso é um único bloco então eu pego uma camada e essa camada em todas as redes anteriores uma camada ela é propagada para apenas outra camada nesse caso essa camada aqui que é a de baixo ela é propagada para quatro outras camadas em paralelo e depois a gente junta o resultado dessas quatro numa única só tá e E aí isso em termos de de codificação ele vai ter Impacto porque a gente não pode mais usar o modelo sequencial lá do tensorflow bar keras tá então Aqueles bloquinhos um por um eles aparecem em paralelo Então são quatro blocos Paralelos que acabam aparecendo Ah nesse módulo aqui tá E aí essa é uma

- *Corpus ID:* 7728
- *Score:* 0.8472241759300232
- *URL:* oculto
- *Início:* 01:47:34
- *Fim:* 01:50:03
- *Transcrição:* computacional mas ela tem uma perda muito pequena na curá tá então a dica é Ah vocês têm recurso computacional limitado bom então pensem numa mobilet ou em alguma versão que segue ainha da mobilenet que é uma rede ah reduzida tá então a partir da mobilenet tem a versão V2 E aí se bobear já existe V3 eu confesso que eu não sei tá mas a ideia básica é exatamente aquela de convolução separável tá ã então para ficar bonitinho o tempo aí do Break eu vou esperar 10:20 daí a gente volta 10:35 tá tem um uma um outro tipo de rede que é bem interessante que na verdade não é uma rede é uma as efficient nets elas são uma maneira de escalar uma rede baseline tá então vamos supor que eu passo para vocês uma rede qualquer uma VG 16 E aí eu digo cara tá muito grande eu tenho que reduzir Esse Cara o que que vocês vão cortar vocês vão cortar o número de canais vocês vão cortar o número de pulling vocês vão diminuir a resolução de entrada qualquer opção dessas é válida Tá mas as efficient nets elas propõem um modelo escalável que de maneira combinada ela Ajusta a resolução de entrada com o número de canais com a profundidade da rede para aumentar ou diminuir o tamanho de uma rede padrão tá então efficient nets Na verdade são uma família de redes ã a partir das quais eu posso até aumentar ou diminuir tá Então essas são várias estratégias E aí o que que eles propõem cara eu quero aumentar uma rede eu vou fazer meio que proporcionalmente o aumento da resolução do número de filtros e da profundidade da rede tá então eles propõe lá um conjunto de nove redes eu acho da b0 até B9 até V8 e cada uma delas maior que a outra E aí esse tradeoff entre custo computacional e e


**Resposta gerada pelo LLM**: Porque existem atalhos de concatenação.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 0.33

**Anotação automatizada**: P

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should provide a complete answer to why the U-Net architecture cannot be built using a sequential model. Supporting Evidence: The RESPONSE mentions "atalhos de concatenação" (concatenation shortcuts), which is relevant to the U-Net architecture's design. However, it does not fully explain why these shortcuts prevent the use of a sequential model. The answer lacks depth and does not address the entire PROMPT comprehensively.  Score: 1


---
