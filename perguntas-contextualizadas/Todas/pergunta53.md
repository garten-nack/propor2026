**Pergunta 53**: Como a redução de dimensionalidade pode ajudar a identificar conglomerados de dados? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 4033
- *Score:* 0.8667526841163635
- *URL:* oculto
- *Início:* 00:11:24
- *Fim:* 00:13:52
- *Transcrição:* esse problema bastante conhecido e discutido na literatura que é a maldição da dimensionalidade né Então esse foi o fenômeno digamos assim que que acontece quando nós temos que processar uma quantidade grande não é disso de colunas de atributos E aí eles cita aqui no artigo uma série de casos onde isso acontece não é E aí explica o problema então é interessante E aí explica também algumas técnicas né então amostragem entre outras em especial no nosso caso aqui hoje a ideia de reduzir Então as dimensões para que nós possamos lidar com esse com esse problema tem tem um pesquisador que trabalhou Trabalhou muito com isso eu acho que depois eu eu procuro direitinho o artigo dele trabalhava muito na área de administração também mas o ele foi um dos que estudou muito e criou uma uma regra também uma regra whisky que diz que nós seres humanos eles compreendem 7 mais ou menos duas dimensões não é de análise que numa empresa no caso que era administração ali é também é importante a gente tentar reduzir para 7 mais ou menos duas para poder compreender né o contexto isso é usado diferentes áreas na economia assim por diante depois eu trago o artigo dele com mais certeza e coloco ali no mundo não tem várias várias pesquisadores que estudaram essa questão né E aí na matemática na estatística então foram desenvolvidas essas técnicas para que nós possamos Diminuir a quantidade dimensões mas ainda assim temos uma quantidade representativa aí de elementos né de atributos que permitam análise como se fosse feita em cima dos as dimensões originais digamos assim então os dois objetivos aqui que nos interessam é são facilitar a visualização né Nós já vimos ali o caso

- *Corpus ID:* 4034
- *Score:* 0.8667374849319458
- *URL:* oculto
- *Início:* 00:13:20
- *Fim:* 00:15:43
- *Transcrição:* estatística então foram desenvolvidas essas técnicas para que nós possamos Diminuir a quantidade dimensões mas ainda assim temos uma quantidade representativa aí de elementos né de atributos que permitam análise como se fosse feita em cima dos as dimensões originais digamos assim então os dois objetivos aqui que nos interessam é são facilitar a visualização né Nós já vimos ali o caso então do MDS tem 1567 ou mais atributos 30 50 eu vou reduzir isso para três para duas permitindo nos visualizar o resultado porque mais de três dimensões a gente não consegue compreender direito né E permitir com que eu consiga processar os dados no computador é muito grande aí O interessante é isso a gente fica imagido como é que eu posso pegar 50 dimensões acende dimensões que algo que às vezes acontecem muitos estudos né e reduzir para duas 13 ainda assim ter representatividade né então o segredo tá na matemática das coisas não necessariamente duas outras dimensões vão ser suficientes em todos os casos né porque Justamente por isso a gente sabe que existem perdas Mas se nós fizermos processamento corretamente e usando algumas eurísticas que nos apoiam nós vamos ter uma perda limitada não é uma perda nesse caso aqui a ideia é criar não é esses essas dimensões que são correlacionadas digamos assim de alguma maneira com as dimensões originais e aí que que tem captar né representar a variabilidade daquelas dimensões que estão sendo representadas né E aí a gente consegue usando essas técnicas então tentar ficar dentro aí de um limite né de 15% que é o que é o estimado de perda então pode ser menos E prefere que seja

- *Corpus ID:* 8589
- *Score:* 0.8658187389373779
- *URL:* oculto
- *Início:* 00:40:29
- *Fim:* 00:42:59
- *Transcrição:* submodelos personalizados com os parâmetros que a gente escolher Então vamos começar com a redução de dimensionalidade essa redução a gente viu discutiu aqui sobre generalização Então ela é importante e normalmente ó muitas dessas em inglês T 384 de comprimento a gente viu e a gente já viu dificuldade né uma solução é reduzir para um espaço Dimensional viável por exemplo C isso aqui é o valor de que tinha lá então não não verifiquei se ele Manteve cinco mas tu não precisa fazer cinco tu pode fazer 10 né mudar então ele automaticamente o ber Topic vai usar o umap e ele vai reduzir para CCO por quê Porque nos experimentos dele ele achou que aquilo é melhor né então fica esse Def ó o que que é o umap pessoal quem gosta de Ática assim vai nisso aqui porque a pessoa vai se divertir ela não vai conseguir largar é bem bem bem matemática né então assim como ele tantas coisas que tem aqui dessa ela é uma das mais ah usadas é de todas é a que lida com de redução de dimensionalidade ela lida melhor com datasets grandes às vezes dá uns pau tá Dá um já vi casos assim que o problema era a biblioteca Por incrível que pareça eu acho que ela não usa GPU ainda essas soluções assim desses matemáticos eu vejo que eles não exploram o ambiente de GPU sabe não sei se não não não tem ganho talvez precisa muito memória e a GPU não tem memória não sei porquê mas ela é Ela captura um espaço de Alta Dimensão eh e tanto no nível local e Global então assim isso quer dizer que ele não perde quando ele vai reduzindo ele consegue manter muita coisa sabe vocês vão ver na na na na na visualização mas tem outras até o PCA tá que dá para tentar pode às vezes funcionar porque o PCA funciona muito bem quando tu tem tudo muito definido né e datas menores às vezes pode ter uma solução melhor então aqui é possível por exemplo mudar o umap então

- *Corpus ID:* 3918
- *Score:* 0.8631729483604431
- *URL:* oculto
- *Início:* 01:09:06
- *Fim:* 01:11:21
- *Transcrição:* se vocês quiserem né Essa é a mais simples porque cria intervalos que vocês mesmos definem mas assim agora que nós já sabemos fazer Em algumas situações o pessoal recomenda discretizar por plastilização porque ou castelização ou discretizar por alguma distribuição né você pode fazer um histograma e verificar quais se ela é bimodal se não é enfim Aí talvez fazer algum corte baseado nessa distribuição ou ainda fazer transferência então poderia fazer um Camélias por exemplo só nessa dimensão que vai pegar suavidades que aí usando o SSD descobrir enquanto os enquanto os grupos ele ele poderia dividir E aí provavelmente nesse caso aí ele vai ele vai dizer que tem dois grupos né uns próximos 30 e mais os outros são mais acho que quase que outlines né tem 42 ou 40 Talvez ele dissesse que o ideal é dois clusters no máximo três aí então vocês podem descretizar pelo pela pelos classes até pelos centros que ele vai só que é uma técnica um pouco mais mais complexa né ela cada cada conjunto de dados teria o seu os seus os seus intervalos né esse intervalo seriam pelos câncer de elementos Em algumas situações Eu já vi o pessoal fazendo isso o problema de fazer isso é que ele se adapta é a vantagem que ele se adapta por coruja dados em especial conjunto de dados que tenham concentração de números né então como nós vimos Nesse caso a maioria a maioria não todos ficaram como como adultos e aí enfim acaba não separando ninguém e nessas situações É bem interessante usar um algoritmo desses porque ele ele tenta se adaptar ao conjunto de dados e de distribuir melhor esses valores né mas o ponto negativo é que se você está comparando

- *Corpus ID:* 3572
- *Score:* 0.8631666898727417
- *URL:* oculto
- *Início:* 00:52:11
- *Fim:* 00:54:33
- *Transcrição:* Faz parte dessa análise exploratória antes de masterizar digamos assim nós usarmos alguma técnica de feitios selection fecho até a gente pode eventualmente forçar fazer uma Fiat Uno de nível acho que vocês têm disciplinas específicas depois sobre isso já não tiveram Mas a gente pode criar novas vídeos ou modificar alguma feature tentando extrapolar né alguma propriedade para que ela fique mais aparente mais para o final de semana nós vamos ver algumas técnicas que fazem isso elas digamos elas distorcem o espaço eu consigo primeiro eu consigo reduzir de n dimensões 10 15 20 ou mais para três ou para duas e ao fazer tem técnicas hoje que ao fazer essa redução de n dimensões para duas ela consegue distorcer o espaço fazendo com que os elementos que estão próximos fiquem mais próximos ainda e os que são distantes fiquem mais distantes ainda uns dos outros então essa distorção ela é feita para que eu possa perceber de maneira mais clara os conglomerados então Originalmente Eles não têm essa configurações espacial mas eu crio essa distorção eu provoco essa distorção para que que eu perceba e o algoritmo né o modelo perceba mais facilmente as diferenças né e a semelhanças que eles têm internamente então é muito muito interessante muito útil inclusive mas foi ótima colocação então seguindo adiante como eu falei nós temos várias famílias não é e basicamente a taxonomia mais simples não é mais geral é essa eu posso ter ou usar algoritmos que que são hierárquicos Ou seja eu vou ter um cluster e esse cluster se divide em dois ou até em mais mas normalmente a gente faz divisões 2 a 2 e cada um deles

- *Corpus ID:* 4127
- *Score:* 0.8630052208900452
- *URL:* oculto
- *Início:* 00:07:19
- *Fim:* 00:09:31
- *Transcrição:* que ela é uma técnica de redução de dimensões Assim como nós vimos na aula passada o uso de PCA nos jornais fatorial né como dimensões alternativas as dimensões originais também uma vez que eu detecte tópicos E ela como se fosse uma técnica de imbedem né Para quem para quem já viu e depois seus homens mais adiante com isso eu já mencionei né tem essas técnicas mais atuais né é uma das primeiras técnicas de imbede em que existe eles não chamavam de imbedem na época Mas o que ela faz é isso ela ela Analisa as relações entre as palavras e ver as palavras que estão mais relacionadas E aí cria e cria esses conjuntos esses classes de palavras que possuem alguma relação E aí é isso o pessoal não mente da o nome de tópico e eles fizeram vários vários experimentos e viram que não deixam de representar tópicos num conjunto de documentos e aí então vocês teriam os tópicos que são mais salientes no conjunto de dados e a ideia a opção seria vocês alguns ao invés de usarem as palavras originais do vocabulário vocês usassem esses tópicos esse conjunto de tópicos que foi detectado E aí não só vocês vão ter uma uma representação que semanticamente é mais rica mas também vão ter uma quantidade dimensões menor para processar e a alternativa não dois que eu coloquei aqui também é Vocês poderiam clasterizar os documentos da maneira tradicional e ao final vocês para cada cluster que vocês geraram vocês aplicariam Lda desculpem Essa svd é para vocês terem né identificados Quais são os tópicos salientes dentro daquele subconjunto de documentos Isso daria uma noção então dos assuntos que estão sendo mencionados dentro dele dentro deles no caso e aí eu preparei Então essa aula complementar coloquei lá no mundo onde vocês tem um exemplo aqui que carrega alguns textos que eu que eu disponibilizei no

- *Corpus ID:* 3765
- *Score:* 0.8614655137062073
- *URL:* oculto
- *Início:* 00:27:26
- *Fim:* 00:29:40
- *Transcrição:* aí vocês vão tendo concentrações menores não é E aí vocês vejam que aqui as configurações são diferentes né dos conjuntos necessariamente eles precisam ter a mesma a mesma digamos o mesmo desvio padrão ou mesmo o mesmo valor de média cada um tem o seu então o que ele vai tentar fazer é se eu digo que tenho que tem três Nesse caso tem três não ele vai tentar fazer o ajuste dessa função que representa a curva glauciana para que ele tenha esse desvio padrão que é adequado para isso como te dados enquanto para esse provavelmente vai ter um outro e para esse um outro ainda então ele maximiza essas esses parâmetros fazendo o ajuste automaticamente para nós e assim imagine que ele tem então uma curva de probabilidade para cada para cada cluster não é então eu tô pegando uma dimensão só ele vai calcular a probabilidade que determinado ponto vamos supor tem um ponto aqui um determinado valor não esse ponto e mais probabilidade pertencer a essa curva aqui ou a essa outra aqui né essa outra porque aqui o valor é maior então é basicamente esse tipo de decisão que ele que ele faz ele ele escolhe aquela que tem a probabilidade maior dele pertencer então aqui se vocês perceberem nesses pontos também ele vai calcular né qual é a probabilidade desse ponto específico aqui pertencer a esse conjunto maior vermelho ou esse azul e aquela que tiver uma habilidade é que ele escolhe é isso se dá pelo ajuste dessas dessas funções Matemáticas e pela então pela distribuição dos elementos que estão ao redor ele consegue perceber que ele mas a dele tá mais adequado pertencendo ao outro do que do que é esse né até para entender né porque assim esses esses conglomerados que estão mais juntinhos você vai como desvio padrão menor você coloca um círculo o círculo

- *Corpus ID:* 2444
- *Score:* 0.8605455160140991
- *URL:* oculto
- *Início:* 00:31:22
- *Fim:* 00:33:23
- *Transcrição:* dessas para cidade ou dessa separação dos dados E aí quando a gente tá falando de redução de mensalidade o que a gente quer fazer então é reduzir o número de atributos ou melhor falando reduzir o número de dimensões tá porque a gente vai ver que dependendo da Estratégia a gente de fato reduz atributos e outras a gente reduz o número de dimensões por combinações dos atributos e quando a gente faz isso a gente quer Claro manter a estrutura do meu problema ou seja o que caracteriza cada classe os padrões caracterizam cada classe porque isso é essencial para que eu aprenda a por exemplo separar bem as classes ou se for um problema de regressão que eu aprendo bem a estimar o valor da saída a partir daquelas entradas tá então ou a gente quer manter Às vezes a gente consegue até aprimorar o desemprego desempenho preditivo sem mudar essa estrutura interna dos dados que é essa esse padrão subjacente a cada classe ou subjacente é uma relação né de entrada e saída no caso de regressão Então a gente vai discutir duas estratégias principais né que são as que as que existem dentro de cada estratégia tem uma série de métodos possíveis né eu selecionei alguns dos mais utilizados para a gente discutir uma delas é seleção de atributos então na seleção de atributos o que a gente faz é a partir dos atributos originais a gente descarta alguns atributos que a gente avalia e percebe que não são relevantes para aquela tarefa Então essa questão de descartar os atributos a gente vai ver formas de identificar o quão relevante um atributo para minha tarefa e verificando isso eu consigo por exemplo rankear os atributos e tirar aqueles que não são relevantes que não estão melhorando na minha no meu problema de aprendizado supervisionado tá então a ideia que é manter os melhores preditores para cada classe né de classificação ou para mim a função né

- *Corpus ID:* 3820
- *Score:* 0.8600409626960754
- *URL:* oculto
- *Início:* 00:44:45
- *Fim:* 00:47:22
- *Transcrição:* algoritmo para as luas para os círculos das duas classes então de maneira geral né resumindo dá para dá para verificar que o cluster em espectral ele é uma é uma família que tem uma boa amplitude uma boa cobertura para diferentes formas de conglomerados mais difícil dele é justamente identificar o melhor parâmetro é quantos grupos tem e o algoritmo de vizinhas não é muito fácil e é bem complexo de calcular a vizinhança e vocês viram ali não tem uma médica de qualidade pronta implementada vocês podem usar a silhueta e o mais recomendado é usar o GAP o GAP né a distância dos alto valores e eu tô devendo eu vou trazer para vocês a implementação mas ela tem naquele link ali também e daria para tentar automatizar isso né um algoritmo que avaliasse diferentes parâmetros né mas aí vejam que ele já é mais complexo por natureza de calcular E aí vocês botam num processo interativo que avalia diferentes parâmetros e tem mais de um parâmetro isso pode demorar bastante né então opção que a gente tem de a gente tem que avaliar se tem essa essa disponibilidade de recursos computacionais e o outro é esse aqui o outro que também tem uma amplitude boa é esse aqui de udb Scan o DB Scan ele Talvez amplitude dele não seja tão grande quanto a do espectral né se a gente tivesse que fazer um ranking né o espectral seria o que tem uma amplitude maior depois eu colocaria o de densidades E aí por último do kérmélias mas nós tivemos que considerar outras dimensões também né em termos de complexidade Então tá médias ele é muito simples é muito fácil de rodar várias vezes e ele funciona muito bem para para grupos né que são isométricos né que muitas vezes é algo

- *Corpus ID:* 4029
- *Score:* 0.8599596619606018
- *URL:* oculto
- *Início:* 00:03:54
- *Fim:* 00:06:21
- *Transcrição:* atributos e verificar se esses atributos assim se não não podem ser agrupados de alguma maneira então são técnicas de usar elas normalmente para reduzir a quantidade de menções talvez você já tenha estudado algumas delas nós vamos trabalhar Mais especificamente com três duas que são técnicas de transformação linear e outra que é uma técnica que combina diferenças abordagens mais modernas que chamam eles chamam de unifood vocês vão ver depois porque que duas lineares porque são duas bastante utilizadas né o PCA e análise fatorial e depois essa outra é o mapa e o nome e é uma das que tem atualmente uma das mais modernas que existem ela não só preserva ordem original como nós vimos lá no MDS a gente usou MDS numa das aulas e eu acho que eu mostrei na aula anterior o tsne essa é uma mais moderna ainda do que essas outras então vocês vão ter uma boa Ampla Gama aí uma boa Gama de algoritmos que vocês podem usar para representar os elementos uma quantidade menor de dimensões e isso ajuda e muitas situações né tanto em termos de processamento porque aí computacionalmente vocês não precisam processar todo o conjunto de dimensões isso é extremamente útil né quando tem um volume muito grande dados mas também porque especial se elas conseguirem né esse objetivo representar por não te dados originais sem perder muita informação Vocês conseguem também eventualmente compreender melhor o resultado né seja por usar uma visualização em duas ou três dimensões porque vocês dependendo da técnica usam que a gente chama de variáveis latentes né que não variáveis


**Resposta gerada pelo LLM**: A redução de dimensionalidade, ao reduzir o número de atributos ou dimensões, permite visualizar melhor os dados, facilitando a identificação de conglomerados.  Técnicas como o MDS reduzem a dimensionalidade para duas ou três dimensões, permitindo a visualização. Outras técnicas, como o UMAP, preservam a ordem original dos dados e distorcem o espaço para que elementos próximos fiquem mais próximos e os distantes mais distantes, tornando os conglomerados mais aparentes.  Isso facilita a percepção de conglomerados, mesmo que originalmente não tivessem uma configuração espacial clara.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should address how dimensionality reduction aids in identifying data clusters. Supporting Evidence: The RESPONSE directly addresses the PROMPT by explaining how dimensionality reduction helps in visualizing data, which is crucial for identifying clusters. It mentions specific techniques like MDS and UMAP, detailing how they function to enhance the visibility of data clusters. This comprehensive explanation covers the entire scope of the PROMPT, making it highly relevant.  Score: 3


---
