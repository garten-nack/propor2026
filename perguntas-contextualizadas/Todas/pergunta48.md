**Pergunta 48**: Quais são os otimizadores usados para treinar redes convolucionais?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 3432
- *Score:* 0.8621202707290649
- *URL:* oculto
- *Início:* 01:00:45
- *Fim:* 01:03:07
- *Transcrição:* né então se a gente poderia usar o tensor sem o que eras né só que aí teria que consultar o manual do tensor e usar as construções dele lá e aí para redes neurais mais comuns né rotineiras como Multilaser percebem e redes convolucionais as construções do que elas uniformizam né e deixa mais simplificado para a gente é legal ajudou bastante beleza Carol Professor essas técnicas contra o versitem overfiting São excludentes Eu poderia usar regularização e andropaute pode usar não são excludentes o que tornam a coisa um pouquinho pior né que eu posso usar uma as duas ou nenhuma as combinações né uma outra as duas nenhuma as quatro possibilidades aí e vai ter mais vai ter o Beth normalization que eu vou falar ainda mais em seguida o que pode ser feito não é depois eu complemento não sei ia perguntar se tem que tipo assim se tem alguma restrição eu posso colocar tipo né que nem tá ali a densa aí vem o andropaude na sequência já encaixa uma regularização antes da próxima ou não pode ser verdade e enfim não tem restrição e por ser Talvez uma diretriz aqui né sem sem pretensões tá gente de ser uma recomendação totalmente validada mas um jeito de fazer é eu testo o primeiro sinal testa dropauts peço minha rede crua nessa em nada aí deu ovo é fit eu vou e testo drop out ver se melhorou aí eu além do Drop vou e adiciono regularização e vejo se melhorou e além dessas duas eu vou adicionar o bett normalization eu vejo se melhorou né E aí eu posso tentar de maneira incrementar assim né para ver se as coisas vão melhorando ou não né E aí entra também junto disso o ajuste de parâmetros né o texto Dropbox 0.2 0.3 e aí eu vou vou pilotando né vou medindo né Cada modelozinho ali ó Qual é a curasse a final né acurácia ou a

- *Corpus ID:* 3334
- *Score:* 0.8612328767776489
- *URL:* oculto
- *Início:* 00:30:27
- *Fim:* 00:32:36
- *Transcrição:* poderia ser outra rede né o poder de ter mais camadas poder ele não ter dropado te poderia ter outro ativação aqui que não é líquido enfim né Poderia ter customizado de outras formas aqui é uma forma de fazer o discriminador o que o que é obrigatório é que a entrada primeira camada tem que aceitar isso aqui como entrada e a saída tem que ter um neurônio tá só isso E aqui então tem as camadas convolucionais não tem não tô usando maxpune também tá eu só peguei a mesma rede que tava lá no no tensor não tô usando Max bullying enfim e vamos ver o que acontece né aqui é a chata né a saída das convoluções para conectar no não tem camadas ocultas aqui ó nessa parte e sai lá o valor da classificação agora o gerador ele tem que ter uma coisa diferente ó que ele vai pegar um vetor de números aleatórios e vai gerar uma imagem tão diretor de pra imagem ele tem que fazer o contrário de uma convolução e fazer convulsão invertida com evolução inversa enfim de conversão né o nome técnico é convolução transposta tá para gerar imagens a partir de um ruído aleatório e aqui então essa é a primeira camada né do gerador o formato de entrada é 100 então ele pega um vetor de 100 números aleatórios tá e vai reformata para ter convoluções tá aqui tem umas outras coisas que eu vou explicar também Beth normalization vai ter uma aula para isso é mas por enquanto vamos ignorar é como se só pegasse esse número de neurônios aqui ó número de elementos então ele pega sem números aleatórios transforma nesse número aqui de elementos né aí pega esse número de elementos para fazer com evolução para fazer convulsão então aqui ele tá gerando uma coisa 7 por 7 né Por 256 valores eu acho pois eu vejo

- *Corpus ID:* 3285
- *Score:* 0.8574963808059692
- *URL:* oculto
- *Início:* 00:42:33
- *Fim:* 00:44:50
- *Transcrição:* Digamos um paradigma de processamento de imagens muito forte mas agora depois que vieram uns Transformers para texto Vieram também os Transformers para imagem né então visual transforma e algumas tarefas relativas a processamento de imagens Transformers né visuais tem tido melhor desempenho do que redes convolucionais né então tem uma disputa entre as duas dois paradigmas aqui né E a gente vai ver o Transformers para texto e vocês vão ter uma disciplina só de processamento imagens né então nela vocês provavelmente vai ter a chance né de entrar nessa nessa coisa mais avançada aqui dos Transformers visuais Ok obrigado fazer o lisinho aqui ó versos né versus redes neurais comvolutivas Então tá tendo a coisa mais recente é o estado da arte tá aqui né aqui a gente já pode dizer que é clássico né redes convolucionais a primeira rede convolucional testada em imagens foi dos anos 90 92 alguma coisa assim naquele data certa pelo Ministro e o cara que fez isso foi o Ian né que foi um dos vencedores lá do prêmio turing né do Nobel da Computação em 2018 né pelo pioneirismo dele então isso aqui já é meio que clássico e aqui já é o moderno né então redes comvolutivas nos primeiros usos aqui e quem tava orientando o iannecã foi o outro vencedor do túnel World que era o Jofre hinton que por acaso é neto lá do George burro da aquele negócio de tudo que a gente vê né em computação Bullying é boleando né então é um nome lá do cara George Bully que era até ta ta taravô lá do rinton Então tem um histórico aí na de genética né computação que transferiu né os genes computacionais dele para generalidade

- *Corpus ID:* 7679
- *Score:* 0.8571200370788574
- *URL:* oculto
- *Início:* 00:13:05
- *Fim:* 00:15:41
- *Transcrição:* Treinamento eventualmente escolhe uma dessas aí tá e como eu tinha comentado anteriormente a gente pode fazer isso offline né Na hora de construir o meu data Loader eu já posso fazer esse processo aí e aí a rede já começa treido ou eventualmente eu posso introduzir Ah todos ou alguns deles como camadas não treináveis em um backbone Tá bom então basicamente o que que um backbone faz tá ele recebe uma imagem como entrada né de novo né mais genericamente ele recebe um conjunto de dados né de entrada pode ser áudio pode ser sei lá pode ser até um conjunto de imagens né quando a entrada for um vídeo certo e aí ele cospe né um um tensor de saída tá E aí novamente dependendo do Objetivo né o número de canais e a resolução de saída do backbone podem variar Bastante como é que esse backbone é composto essencialmente é um conjunto de camadas né e e eu creio que a essa altura do campeonato Vocês já viram né basicamente eh um bom conjunto de camadas que podem ser usados eu já comentei aqui alguns deles a camada convolucional que é e de de fato building Block de uma rede convolucional né mas também existem camadas de pulling né para reduzir a resolução da imagem exatamente para dar essa ideia de multiresolution como uma camada fully connected tá elas basicamente aplicam uma transformação linear né Elas multiplicam os dados de entrada por constantes e adicionam Bias tá e a função de ativação pode potencialmente introduzir uma não linearidade tá então a relu é uma função de ativação né que tem sido muito popular aí nos últimos na última década pelo menos tá porque ela facilita o treinamento tá existem alternativas para ela se vocês olharem lá o tensor Flow vocês vão ver que tem uma penca né de funções de ativação tá a função de ativação ela também serve para reescalar os dados eh produzidos por uma

- *Corpus ID:* 7735
- *Score:* 0.856012225151062
- *URL:* oculto
- *Início:* 00:09:17
- *Fim:* 00:11:52
- *Transcrição:* gente vai usar em geral as cnns redes convolucionais algumas delas tendem a funcionar melhor tá eh é só para mostrar como tem muito achismo e muito ismo né e eu comentei Eu acho que eu errei esse aqui não é 23 eu acho que é 24 tá é recente mas não tanto tem um artigo lá que o título é a batalha dos backbones né que ele faz uma comparação exaustiva de vários backbones para várias tarefas tá e a conclusão que ele chega que no final das contas Quem ganhou não foi um Transformer foi aquela com H connex que eu mostrei que eu comentei no slide anterior tá então é é muito difícil de realmente tu chegar categoricamente dado um problema dizer não eu garanto que essa rede vai funcionar melhor é muito difícil para não dizer impossível tá b então eu eu eu comentei para vocês que a maioria desses backbones pelo menos praticamente todos convolucionais que a gente viu até agora eles foram propostos no contexto de classificação de imagens n que que significa isso significa que bom e agora se eu tiver um outro problema eu eu eu tenho que criar outro backbone como é que a gente faz tá E aí o que que acaba acontecendo eh uma da um dos achados aí já bem antigos aí Eh sobre essas essas redes Profundas tá é que eh mesmo que eu treine um backbone para uma certa tarefa tá as as features aprendidas sobretudo nas camadas iniciais Opa tá elas são comuns para outros datasets e para outras tarefas então se eu treinei um backbone por exemplo para classificação da imagens e daqui a pouco eu quero fazer detecção de objetos eh vale a pena explorar esse backbone pré treinado num dataset de classificação e de novo D uma atualizada nele para Ah pra tarefa que eu quero e pro dataset novo que eu quero certo Ahã Então esse processo de reuso parcial ou total é chamado de transferência de conhecimento transfer learning tá então

- *Corpus ID:* 3288
- *Score:* 0.851044774055481
- *URL:* oculto
- *Início:* 00:48:01
- *Fim:* 00:50:16
- *Transcrição:* não sei se tem que colocar o depois eu confio tá a palavra-chave se é extraído mesmo depois eu vejo direitinho mas é isso né tem que configurar qual qual o tamanho da janela 2 por 2 e qual o número de Passos aqui e depois do maxpring ou seja só tem uma camada convolucional com Reluz só tem um Max bullying aí eu achato a imagem para entrar para já sair as classes aqui ó então a chata e já sai as classes né com ativação sofre demais então aqui já é a classificação é os dígitos lá cada dito vai ser é uma das 10 classes colocadas e aqui agora strass mesmo né o Alexandre conferiu ali ó então o colab tava completando certinho para mim extrair isso deixa eu ver tem uma mão levantada é dúvida deles sem problemas tá E aqui então tá definida a rede neural uma sequência né de camadas algumas fazem processamento efetivamente outras acessório né Para chata a imagem é um tipo de processamento também né mas não é não tem pesos aqui né a camada e no final a classificação e aí eu mando né compilar quer dizer definir né o que como que vai vai otimizar então otimizado ou esfoliada essa loja aqui né a gente viu né na aula passada que não dá para tirar o espaço aqui ó Então tem que ser espaço entropia cruzada categórica tem que ser essa aqui para quando o problema tem mais de uma de duas classes né tem que ser essa função de custo aqui e a métrica que eu vou medir eu vou medir a curasse né então ao longo do treinamento da rede a gente vai ver o custo e a curasse e aqui o resumo né Vamos executar aqui para ver o que que tem essa nossa rede neural aqui ó que que ela tem uma camada convolucional como a gente definiu uma Max bullying achatar e uma dessa e aqui o número de parâmetro só a camada

- *Corpus ID:* 3248
- *Score:* 0.851039707660675
- *URL:* oculto
- *Início:* 01:02:28
- *Fim:* 01:04:40
- *Transcrição:* Maps Diferentes né e se eu tiver outra camada comvolucional que combine né Essas coisas Vai acender na presença das duas juntas né pode ser muito interessante isso E aí vou ter um filtro meio que detecta isso aqui junto né uma orelha pode ser vai elaborando vai vai aumentando o nível de abstração das coisas a serem infectadas é provável que já tenha sido feita essa explicação mas eu perdi um pouco nessa camada comvolutiva a gente tem três padrões eu vou chamar de três padrões como é mesmo que esses padrões Eles foram apreendidos como é que se chegou nesse trem tá esses aqui não foram apreendidos tá essa minha rede aqui eu eu já matei a charada e sabia né que era esses aqui que era os padrões bons para detectar o x Tá mas numa rede neural normal né convolucional esses números aqui começam tudo aleatório E aí com base nos erros a gente vai ter um momento para discutir isso dos erros vai ter uma predição aqui né provavelmente na primeira vez ela vai errar E aí vai ter o beck propagation aqui e vai ter mesmo para cá também para mexer no valor desses filtros aqui ó cada filtrozinho é um peso também o cara filtro não cada elemento do filtro é um peso então começa aleatório mas vai refido aqui para chegar nos valores que fazem né as características legais as características boas serem detectadas então a minha rede convolucional digamos essa aqui já tá treinada né já tá treinada já tá certa é mais uma rede normal vai vai ter que ser treinada do zero e funciona né A gente vai ver depois até em código que como que funciona também ah e uma coisa importante né eu não falei das ativações aqui agora chegou a hora de falar das ativações é uma ativação não linear até para poder ter né aquela questão da de fugir né do reino linearidade a gente vai ter uma ativação aqui também né em

- *Corpus ID:* 7680
- *Score:* 0.8510391712188721
- *URL:* oculto
- *Início:* 00:15:01
- *Fim:* 00:17:25
- *Transcrição:* constantes e adicionam Bias tá e a função de ativação pode potencialmente introduzir uma não linearidade tá então a relu é uma função de ativação né que tem sido muito popular aí nos últimos na última década pelo menos tá porque ela facilita o treinamento tá existem alternativas para ela se vocês olharem lá o tensor Flow vocês vão ver que tem uma penca né de funções de ativação tá a função de ativação ela também serve para reescalar os dados eh produzidos por uma camada para uma faixa de valores adequada então é muito comum também a gente ter uma função de ativação customizada lá no final eu vou falar isso depois quando eu tiver falando na head da rede neural tá então como eu tava dizendo né boa parte quer dizer boa parte das camadas de uma rede convolucional vai ser formada por camadas convolucionais tá E aí como eu falei para vocês antes né pô eu vou montar uma rede convolucional quantos filtros eu uso Quantas camadas eh qual é o tamanho do Kernel quantos pulling eu vou ter o céu é o limite tá então assim eh uma tendência é que a medida que a resolução diminui ou seja quando eu vou agregando pulling eu aumento o número de canais tá E até tem uma explicação computacional para isso né porque se eu tenho uma imagem de entrada muito grande Tá ou ou grande com o número de canais grandes eh grande o custo computacional é alto né porque uma camada convolucional ela basicamente pega a dimensão espacial o número de canais que é um volume e para cada querne o convolucional ela pega o volume inteiro né então o custo computacional de uma operação de de convolução numa rede neural é o tamanho espacial do Kernel multiplicado pelo número de canais tá então aquela análise que a gente fez lá no início da disciplina n no curso computacional da convolução da filtragem linear A lógica é a mesma A grande diferença que agora também vai entrar o

- *Corpus ID:* 7732
- *Score:* 0.8505421876907349
- *URL:* oculto
- *Início:* 00:04:00
- *Fim:* 00:06:23
- *Transcrição:* dataset que a gente tem para treinar tá a gente tem que levar em consideração e o o custo computacional que a gente quer a gente tem problema de custo ou não tem né basicamente posso assumir que eu tenho máquina infinita ou não tá E aí isso obviamente que impacta o a escolha de vocês e isso que guia o desenvolvimento de novos back Bon tá então tem algumas lá shufflenet é uma outra rede compacta tem uma outra um outro backbone que é muito usado para detecção de objetos tem uma versão V2 deficient net tá e tem uma rede que é connex tá que curiosamente o ano passado saiu um artigo comparando vários back Bon tá e um dos findings é que essa connex é um um dos uma dos back que atinge melhor resultado bom eh aqui vai ficar só um Pointer para vocês tá eh provavelmente vocês já ouviram falar em Transformers ainda a a disciplina acho que da Viviane vem logo depois tá E aí certamente vocês vão ver isso aí todo mundo já ouviu falar pelo menos esse termo Transformer não sim sabem o que que é é um filme né o robozinho que muda lá o carro também ém também é horrível pr pra rede eu acho horrível Tá mas tudo bem Tá então assim toda a moral dessa da dos Transformers tá eh é assim ó eh talvez vão fazer o contraponto entre uma rede convolucional e uma rede baseada em Transformer convolucional A ideia é que eu tenho Kernel e o tamanho do Kern limita quem vai falar com quem né então 3 por TR significa que eu tenho um bloco de nove pixels que vai conversar entre si certo ah então essas todas as operações são locais tá a ideia do transformer é usar uma camada que não é local potencialmente todo mundo conversa com todo mundo tá então A ideia é que todo mundo vai conversar com todo mundo em tese o

- *Corpus ID:* 3458
- *Score:* 0.8505069613456726
- *URL:* oculto
- *Início:* 00:12:51
- *Fim:* 00:15:18
- *Transcrição:* assim a batata pequena eu vou treinar mas eu não vou treinar pasta congelada né eu vou e faço o Fit Quando eu fizer ele vai pedir para edição então a imagem vai passar por toda a rede vai sair a previsão final e o back propagation vai fazer executar até aqui né o os erros aqui ó vão ser executados até aqui vão atualizar só essa parte da rede ou seja vai ser muito mais rápido eu vou aproveitar toda o conhecimento codificado nessa rede já treinada e eu vou só vou só aprender o que for necessário aqui para a minha aplicação específica né então isso é o transforme a transferência de aprendizado eu peguei tudo que já foi aprendido e tô né criando conhecimento novo aqui com novas camadas Ok dúvida Antônio pessoa uma dúvida sobre este uso né Se eu tiver usando uma rede para treinar para reconhecimento passear digamos né certo e classifica mas tem um desempenho ruim quando determinado tipo de categoria de de fez sons parciais né esse esse tipo de treinamento né seria poderia ser é feito uma camada ardência Para poder melhorar performance nesses pinscher específicos não tem restrição tá as novas camadas aqui ó as novas camadas vou até fazer de vermelho pode ser só densa né pode ser as camadas só densas aqui nesse caso me parece mais sensato carregar né o vgg aqui por exemplo com aquele incluente top igual a falso porque eu não vou pegar o classificador do vgg eu não vou pegar o classificador dele o PC entrou nele ou pegou usar o meu então as minhas novas camadas eu posso colocar com convolucional não tem restrição né E também eu vou ter que eu preciso ter um classificador no fim das contas né mas aqui ó a parte convolucional é eu posso colocar também que ela pode pegar o que


**Resposta gerada pelo LLM**: O texto menciona o uso de otimizador estocástico por descida de gradiente (esfoliada),  e a função de custo entropia cruzada categórica.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: I

**Answer Relevance (AR)**: 0.33

**Anotação automatizada**: P

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant context to the question about optimizers used for training convolutional networks. Supporting Evidence: The RESPONSE mentions "otimizador estocástico por descida de gradiente," which is a relevant optimizer for training convolutional networks. However, it does not provide a comprehensive list of optimizers or fully address the prompt, which asks for various optimizers used in this context. The mention of "função de custo entropia cruzada categórica" is related to loss functions rather than optimizers, which detracts from the overall relevance to the prompt. Therefore, while there is some relevant information, it does not fully answer the prompt.  Score: 1


---
