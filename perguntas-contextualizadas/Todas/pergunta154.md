**Pergunta 154**: Quais são as decisões que precisam ser tomadas durante a preparação de dados?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2316
- *Score:* 0.8610281348228455
- *URL:* oculto
- *Início:* 00:03:12
- *Fim:* 00:05:14
- *Transcrição:* coleta e preparo dos dados Então quais são os atributos como é que vou transformar esse atributos como é que eu vou particionar os dados para desenvolver o modelo né treinar validar otimizar e preparamos como é que eu vou lidar com valores faltantes enfim tá E também a parte de engenharia de atributos Então a gente vai estar discutindo um pouquinho essas etapas iniciais esse processo de preparar os dados e a gente também falou da nossa introdução que existe esse level aprendizado de máquina que que diz que se eu tiver garba jeans garba de alto né então tudo que eu pretendo fazer com o meu modelo de aprendizado de máquina de metodologia não vai ser tão boa quanto forem é só te será tão bom quanto foi os meus dados então a gente precisa prestar muita atenção nessa etapa dos dados de preparar os dados para ter dados que sejam mais digamos assim limpos e utilizáveis né o seu valor mais facilmente explorável pelos modelos de aprendizado de máquina tá Então acho que vocês lembram né que a gente discutiu e realmente existe a estimativa que a gente gasta boa parte do nosso tempo nessa etapa de preparar os dados o que que são bons dados e aprendizados de máquina Tá eu vou passar por alguns pontos aqui porque acho que é bom a gente ter uma ideia quando a gente fala que a gente quer tomar os ossos dados melhores para serem mais facilmente explorados pelos modelos é bom a gente pensar em algumas características do que são bons dados tá então primeiros são Dados que são informativos ou seja são Dados que possuem características descritas das atributos dos dados tanto em quantidade quanto o nível de gravidade suficiente para aquele problema que eu tô querendo abordar de pressão então

- *Corpus ID:* 2169
- *Score:* 0.8599150776863098
- *URL:* oculto
- *Início:* 00:10:46
- *Fim:* 00:12:47
- *Transcrição:* decisões na coleta né Que atributos que eu vou utilizar Que tipo de informação de indicadores de características eu tenho que mensurar sobre aquele problema que eu quero abortar como é que eu particiono os meus dados né para fazer o desenvolvimento do modelo como é que eu lido com valores faltantes Será que eu preciso aumentar fazer algum tipo de deitar um processo que a gente aumenta os dados através de ou instâncias artificiais ou algumas alterações para poder ter mais dados Será que eu tenho que reduzir os meus dados enfim tá então tem várias decisões depois eu tenho a parte de engenharia de atributos Será que eu preciso criar ou transformar atributos né Será que eu preciso fazer uma redução de dimensionalidade seja com seleção de atributos seja com técnicas como PCA então tem várias questões que entram aí e depois na parte de treinamento e avaliação de modelos também a gente vai decidir quem algoritmos a gente vai utilizar para treinar que a gente vai explorar como é que a gente vai fazer a divisão de dados de forma independente Ou seja eu tenho que ter dados diferentes para avaliar o desempenho do modelo né em relação aos dados que eu usei para treinar como é que eu vou otimizar e preparaâmetros que métrica desempenho eu vou eu vou avaliar E qual vai ser a minha métrica principal enfim existem diversas decisões né Em vários pontos aqui então essa metodologia tem essa sequência de etapas que como eu falei ela não necessariamente é linear ela começa a linear mas muitas vezes a gente tem que revisar etapas anteriores voltar né E essas diferentes decisões opções de métodos estratégias isso junto né O que a gente prega é a nossa metodologia Então quando vocês forem escrever um desenvolvimento do modelo vocês vão iniciar descrevendo inclusive decisões de pré-processamento de dados porque faz parte da metodologia de trabalho de

- *Corpus ID:* 8926
- *Score:* 0.8580869436264038
- *URL:* oculto
- *Início:* 00:24:55
- *Fim:* 00:27:47
- *Transcrição:* impacto muito grande no resultado das aplicações né em geral assim em bancas Eu sempre gosto de perguntar né quando tô em banca de ciência de dados ou né de a sistema de recomendação é quanto tempo né a pessoa né o aluno levou considerando assim eu tenho a preparação de dados e eu tenho a parte de análise e processamento né e muitas vezes eh O que é dito é que 70 ou 80% do tempo é com a preparação de dados né então é é bastante crucial Eu só acho vai chegar depois agora só desculpem Vamos lá eh então a parte de preparação de dados né e de armazenamento Então ela é bem importante e e às vezes a gente né gasta um tempo nisso e não dá eh tipo a importância e que ela merece né então aqui o que o que Opa o que a gente vai trabalhar então só detalhando um pouco mais né então é como que a gente vai organizar os dados aí depois como a gente vai representar eles no modelo né então antes de entrar pro sistema gerenciador de banco de dados propriamente dito depois como a gente vai armazenar os dados né então como fisicamente cada um desses modelos faz o armazenamento e depois então como a gente acessa esses dados né então esse aqui é o é o o objetivo principal então Eh de cada um dos modelos a gente vai seguir essa ordem tá aqui depois eu vou falar um pouquinho eh mais detal então sobre essas gerações de bancos de dados mas aqui só paraa gente ter um Panorama né da evolução Então os bancos de dados eles surgiram lá na década de 70 né então o que a gente chama de pré-relativística a gente vai ver que de todos os bancos mesmo esses novos o relacional até hoje é o mais utilizado porque é funciona né e é bastante é eficiente aí depois nós tivemos a etapa

- *Corpus ID:* 2170
- *Score:* 0.857373058795929
- *URL:* oculto
- *Início:* 00:12:17
- *Fim:* 00:14:18
- *Transcrição:* que como eu falei ela não necessariamente é linear ela começa a linear mas muitas vezes a gente tem que revisar etapas anteriores voltar né E essas diferentes decisões opções de métodos estratégias isso junto né O que a gente prega é a nossa metodologia Então quando vocês forem escrever um desenvolvimento do modelo vocês vão iniciar descrevendo inclusive decisões de pré-processamento de dados porque faz parte da metodologia de trabalho de vocês Né desde essa etapa de coleta preparação né e um pré-processamento específico então assim essa sequência de decisões que a gente toma né E todos esses pontos em que a gente tem que tomar realmente decisões porque existem diferentes possibilidades isso faz a nossa metodologia claro que a gente normalmente tem um conjunto de aspectos que a gente precisa prestar atenção e dependendo de um problema Alguns alguns aspectos eu vou levar em conta outros eu vou ver que não preciso certo por exemplo você não tem o valores faltantes eu não preciso pensar numa estratégia de como que eu vou lidar com essa situação mas é de uma forma geral né os pontos que a gente é tem que prestar atenção né são é um grande conjunto de opções e a gente conforme né caso a caso a gente vai discutindo e decidindo como abordar cada uma delas dependendo das demandas do projeto dos dados tá então isso é a metodologia então aqui fica claro a ideia da disciplina a ideia da disciplina é a gente passar por vários dessas desses problemas né ou decisões que a gente tem que tomar ao desenvolver o modelo é que na disciplina anterior a gente não fez porque o foco era a gente entender um pouquinho mais como é que ocorre o aprendizado supervisionado ok então na disciplina anterior né a gente focou como eu tinha comentado entendeu Nacional por trás de algoritmos pressionado Como é que os algoritmos

- *Corpus ID:* 1237
- *Score:* 0.8570088744163513
- *URL:* oculto
- *Início:* 01:49:46
- *Fim:* 01:52:19
- *Transcrição:* Então quais são os próximos passos né analisar Então até acho que analisar essas regras de associação mais adequadas ao objetivo do negócio que é tentar entender bem mesmo nesses casos aí de envio ou não de viatura e até acho que faltou colocar aqui mas poderia dizer assim que deve fazer os dados estatísticos né que vai constar no trabalho final colocar todos as correlações ali das análises estatísticas das variáveis e o que que a gente viu como dificuldade de aprendizado até o momento né de fato disparar e ganha longe ali a preparação dos dados né você ver ali né ter todos aqueles dados brutos e você é observar aquilo e formatar aquilo para ficar adequado para regras de associação é sempre é Um Desafio né então ali unificar informações elegível para o algoritmo né isso eu falei também é a partir dos dados digamos contínuos né estipular valores categóricos adequados né Por exemplo que que eu vou considerar que é madrugada Será que eu considero da uma da manhã e cinco enfim coisas do tipo né e esse passo aí da própria análise de regras né assim quando que eu paro de ficar tentando filtrar e analisar as regras parece que a gente sempre quer é um processo ali que você não quer parar será que tem mais alguma coisa escondida e a gente fica assim nesse Loop e com esse processo né ficou como ficou bem evidenciado esse processo né de vai e volta pode ser que a gente ainda tem talvez eliminar né alguma atributo lá do data 7 ou inclui outros né para poder tirar regras mais relevantes mas no momento é isso que a gente tá trabalhando acho que aqui no finalzinho acabou perfeito tá muito bem enquadrado trabalha a única observação que que eu faria mas isso às vezes na tua Fala tu vai e volta mas eu acho importante

- *Corpus ID:* 815
- *Score:* 0.8568754196166992
- *URL:* oculto
- *Início:* 01:26:42
- *Fim:* 01:29:18
- *Transcrição:* precisar de uma composição de um time assim E aí vem aparecendo novos nomes né eu gosto desse slides aqui até porque meu passado em banco de dados os engenheiros de bandidados que ele né para um trem todo com os dados que estão brutos que não foram instalados né e ele protege com seus dados bem comportados esse pequeno garoto né que é o cientista de dados que vai derivar conhecimento é de certa forma esse desenho ele tá certo né porque a seleção e a preparação de dados aquelas fases iniciais até a gente chegar na mineração ela é a que mais consolidados a preparação de dados estima-se em 80% dos trabalhos de cientista de dados tá então sim ela é verdade pelo pelo valor né que a gente dispensa então assim não é só sobre a é muito sobre dado mas também não é só saber fazer consultas ou transformações é sobre relacionar um conhecimento que se quer com que a técnica pode revelar por isso que eu não enxergo engenheiro de dados aqui como esse pequeno ser né protegido brutos ele tenta muito conectado com aquilo que o negócio é precisa descobrir do ponto de vista de valor de Preparações necessárias e mesmo da validade dos padrões e esse desenho é quando eu vi né eu botei esse artigo todo aqui né é uma realidade Estados Unidos né Porque tantos cientistas de dados estão Deixando os seus seus profissões né os seus trabalhos a razão número um é porque ele tá errado nas suas expectativas né ele virou E aí desculpa né Eu não quero usar linguagem neutro ele virou a pessoa dos dados no artigo tá falado do Deita gaita girl é a pessoa dos dados né é o cara que faz tudo com os dados então isso não é verdade e a segunda é não compreender as questões corretas não compreenderam o negócio portanto não conseguir agregar valor ao

- *Corpus ID:* 6634
- *Score:* 0.8564904928207397
- *URL:* oculto
- *Início:* 00:30:19
- *Fim:* 00:32:26
- *Transcrição:* tá mas tem todos esses outros aqui tem até do read clipboard que é tu faz contrl c e depois tu faz read clipboard né eh Mas aí tem por exemplo capítulo sete data cleaning preparation que maior parte do tempo quando a gente vai analisar dados a gente gasta com limpeza desses dados porque normalmente os dados que a gente pega eles são meio tem coisas estranhas dentro a gente precisa entender bem as todas as coisas estranhas para não gerar conclusões erradas né então é na na disciplina de aprendizado de máquina a gente viu isso na prática que a gente gastava perto de 90% para limpar a nossa base isso para poder depois e processá-la isso E aí Aqui tem os métodos de como é que a gente limpa como é que a gente entende Quais são os nas que que a gente faz com esses nas que é são informações faltantes né como é que remove como é que verifica se existe enfim tem uma série de coisas que dá pra gente fazer aqui tem vários exemplos ó feeling in Miss data isso aqui extremamente perigoso fazer isso dependendo do cenário né tipo assim tu tem lá uma dado faltante tu quer colocar um dado lugar ali será que isso faz sentido entendeu ver como é que isso funciona né para olhar eh e e tomar decisões importantes ali na na hora da preparação né antes de fazer qualquer coisa então todas essas operações fio na aqui Peg pegi um exemplo aqui aqui né tipo sei lá qualquer outra operação aqui drop duplicates que a gente já viu que a gente inclusive já usou essas operações todas elas já estão paralelizador a gente acaba é muito mais importante como como acho que foi o Denis que resumiu né ficar bem prolífico nessa api aí porque depois existe uma correspondência com com rapids aqui no caso por que que eu eu lancei essa discussão porque eu queria entender se eu preciso continuar explicando cada um dos elementos aqui entendeu para vocês eu sei que a gente tá meio que talvez na

- *Corpus ID:* 2195
- *Score:* 0.8563852310180664
- *URL:* oculto
- *Início:* 00:51:40
- *Fim:* 00:53:53
- *Transcrição:* coisas como normalização de dados transformação de atributos Auto laires valores faltantes isso sim isso vai ser aplicado agora outras coisas como por exemplo vazamento de dados não porque a gente supervisionado a gente tem aquela ideia de ter um alvo e aprender a estimar aquele alvo a partir dos dados né então o vazamento de informações tá quando eu uso dados de teste que a gente tá sumindo que não conhece essa saída para o treinamento do modelo então aí essa relação né de vazamento de dados aí já é diferente mas a questões por exemplo de avaliação de modelos muda Tu aprendeu as coisas do nada também existem estratégias para avaliar modelos novos profissionado é mas elas são diferentes supervisionado tá então eu diria que eu diria que a parte pré-processamento de dados ela se aplica tá ela se aplica bem porque muitos desses algoritmos nós pressionados vão demandar preparação de dados para processamento Ok Ok mais alguma pergunta acho que a gente vai começar então a discussão sobre estratégia de divisão de dados então como eu comentei né primeiro assunto da disciplina avaliação de modelos preditivos tá então a gente vai aprofundar o que a gente vem discutindo e essa avaliação de modelos preditivos a primeira coisa que a gente vai discutir estratégias de divisão de dados porque a gente viu que a gente não pode avaliar modelos em dados que não sejam Independentes ou seja dados que não foram vistos em tempos de treinamento e existem diversas estratégias para a gente fazer isso né então a gente vai discutir essas mais comuns e uma que é considerada tipo assim talvez né o chamaria o estado da arte tinha mais recomendada que a validação cruzada tá e tem diversas variações Ainda assim eu trouxe outras que vocês inclusive já ouviram falar que

- *Corpus ID:* 2085
- *Score:* 0.8555184602737427
- *URL:* oculto
- *Início:* 00:57:30
- *Fim:* 00:59:48
- *Transcrição:* da gente usar um samba então aqui a gente tá usando árvores decisão a gente tá usando ela se faz alguma dúvida que pessoal Pode ficar à vontade se quiserem fazer uma pergunta só porque geralmente ele podia ser melhor mas exatamente nesse horário é isso aqui por exemplo né se eu fosse por exemplo escolher mesmo que a curasse ou algum outra métrica lá como a precisão ou Recall do Random Force de repente fosse melhor ou um pouco melhor talvez o melhor modelo a escolher mesmo com isso ou sei lá nos dados de treino de teste acaba acabaria sendo essa o bag com árvore de decisão é exatamente assim se a gente tiver olhando para essas para essas análises né a gente olharia e pensaria bom ou pega e realmente ele se eu tivesse uma análise desempenho e olhando isso aqui eu realmente acho que deve seria uma melhor escolha Mas uma coisa importante da gente tem mente a gente raramente fazer esse tipo de visualização nos modelos né porque a gente tá trabalhando com dados multi mencionais mesmo que eu trabalho é mesmo que eu trabalhe com técnicas de projeção eu posso fazer uma técnica de projeção para editar meus dados lá que tem 50 atributos em duas dimensões mas essas técnicas de projeções elas podem teria que aplicar isso né tanto para os dados quanto para as predições então torna um Pouquinho complicado não necessariamente vai refletir né aqui a gente está conseguindo ver porque é um exemplo com duas dois atributos enfim e computacionalmente deve ser bastante quando você tem diversos atributos Isso computacionalmente deve ser bastante complexo e demorado que se de você conseguir tirar essa conclusão ou seja seria pouco ganho muito esforço para pouco ganho exatamente então na prática

- *Corpus ID:* 2538
- *Score:* 0.8543694615364075
- *URL:* oculto
- *Início:* 01:30:34
- *Fim:* 01:32:34
- *Transcrição:* variáveis que tem uma relação forte né e positiva são consideradas antes mantém o só uma tiro a outra então aí a gente já tá reduzindo a dimensionalidade né aí a gente parte para etapa de pré-processamento dos dados de fazer aquela questão de ajustar valores faltantes lidar com os Outlander transformar categóricos e numéricos de acordo com as nossas necessidades E aí vem a parte da seleção de atributos que é bom de todo data 7 que eu tenho que ele já foi Limpo de certa forma será que ainda consigo melhorar essa informação contida nos dados deixando tipo elas mais claras né a informação mais clara possível para o modelo para o algoritmo aprender essa modelagem aí vem a etapa de seleção de atributos isso é bom até a gente fazer antes porque a gente diminui o curso computacional né a gente vai reduzindo ali antes um pouco dentro do possível exato porque às vezes acontece mesmo de ter por exemplo tanto essa questão de variáveis que não tem uma variância digamos assim uma variância muito baixa ou variáveis correlacionadas da correlação às vezes depende um pouquinho do conhecimento do problema para ver o que que a gente vai manter mas sim a gente vai tudo a gente vai reduzindo os dados e vai reduzir o custo para todas as tarefas seguintes então e aí percebam que todas as questões para a gente correlação de variância de valores que tem parece que tem muitos valores faltantes são tipo variáveis né super mal preenchidas assim ponto de vista acabam sendo meio inúteis assim no processo de modelagem isso tudo a gente faz antes completo porque nada tem vazamento de dados quando eu faço análise entre duas duas variáveis não variável e saída mas duas variáveis não tem vazamento de dados eu poderia é digamos assim porque eu tô removendo né eu tô eu não tô mudando eu não tô


**Resposta gerada pelo LLM**: As decisões envolvem a coleta de dados (atributos a serem utilizados, tipo de informação a ser mensurada, e como particionar os dados), lidar com valores faltantes (aumentar dados com instâncias artificiais ou reduzir dados), engenharia de atributos (criar ou transformar atributos, redução de dimensionalidade), e o pré-processamento (ajustar valores faltantes, lidar com outliers, transformar variáveis categóricas em numéricas).

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should address the decisions that need to be made during data preparation comprehensively. Supporting Evidence: The RESPONSE directly addresses the PROMPT by outlining various decisions that need to be made during data preparation, such as data collection, handling missing values, feature engineering, and preprocessing. Each of these points is relevant to the overall topic of data preparation, demonstrating a clear understanding of the necessary decisions involved. Therefore, the RESPONSE is relevant to all parts of the PROMPT.  Score: 3


---
