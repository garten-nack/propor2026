**Pergunta 29**: Como o sistema Compass, usado nos Estados Unidos, perpetuava o viés racial em relação a afro-americanos? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2180
- *Score:* 0.8677826523780823
- *URL:* oculto
- *Início:* 00:27:42
- *Fim:* 00:30:01
- *Transcrição:* perpetue esses comportamentos que são considerados preconceituosos negativos enfim certo e aí eu coloquei aqui alguns exemplos que não são muito antigos digamos assim né e coloquei aqui umas referências se vocês quiserem ver depois mais um baias numa ferramenta de recrutamento de pessoas online né então da Amazon mesmo que basicamente o que acontecia é que o algoritmo eles dava digamos assim menos preferência a mulheres né do que é homens tá então tinha essa questão né de discriminação de gênero e essa questão de desses algoritmos usados na Justiça Criminal então tem esse sistema Compass usado nos Estados Unidos e ele atribuía um risco maior de ser digamos assim culpado né Para afro-americanos né Por conta dos velhos nos dados ele estava digamos assim perpetuando né esse viés então Isso tá errado isso é algoritmo não pode fazer isso então ele não pode discriminar não pode criar novos novas formas de discriminar as pessoas nem perpetuar algo que tá errado né que que acontece no nosso mundo mas está errado então eu chamo atenção porque quando a gente está desenvolvendo o modelo também é a nossa digamos assim a nossa responsabilidade explorar os dados para ver se isso não tá acontecendo nos dados e explorar o nosso modelo para ver se o nosso modelo não tá sistematicamente errando mais para um grupo de pessoas por exemplo tem homens e mulheres o modelo a certa mais para homens Ou acerta mais para mulheres dependendo do caso isso pode ser é digamos assim desejável fazer essa diferença né muitos casos certo então só trazendo algumas coisas isso que eu achei interessante essa linha do tempo eu deixei o link para

- *Corpus ID:* 2639
- *Score:* 0.8453865647315979
- *URL:* oculto
- *Início:* 00:38:05
- *Fim:* 00:40:06
- *Transcrição:* tem um caso bem conhecido que foi publicado na Saens que foi em relação ao algoritmo que era usado nos Estados Unidos para fazer Triagem de pacientes e basicamente esse algoritmo ele sempre atribuiu um risco menor a pessoas negras e aí foi ver olhando né os pesquisadores viram É que olhando o modelo existia uma informação que era custos em saúde e esse custos de saúde era usado como um proxy para necessidade de saúde Então se os custos eram menores é porque a pessoa tinha menos necessidade de saúde ou seja de atendimento se o custo maior porque tinha mais necessidade o ponto todo é que esses custos eles eram sempre menores para a população negra por questões de justamente questões desigualdade social menos condições de gastar menos momentos Enfim então isso gerava um viés em favor da população branca e é isso foi um problema assim bastante gráfico que era um algoritmo usado né no sistema de saúde americano Então entendo é percebeu o impacto que isso tem né para essas pessoas tem uma pergunta da Caroline pode falar oi professor então assim né quando a gente detecta que tem alguma coisa em vias dando o algoritmo O correto é retirar aquele fator no caso desse templo aí do da questão racial seria não levar em consideração os custos com saúde e no caso lá da asma retirar essa característica de que o paciente tem asma não tem várias formas uma das formas é tirar essa essa informação mas por exemplo Vamos pensar nesse algoritmo aqui desse que tem esse Baía social tá vamos supor que alguém desenvolvendo os algoritmo tirou a questão a informação raça para tentar evitar esse E aí percebam que esse viés ele tava incorporado numa outra variável que a gente que do ponto de vista né

- *Corpus ID:* 4660
- *Score:* 0.8424151539802551
- *URL:* oculto
- *Início:* 00:51:55
- *Fim:* 00:54:23
- *Transcrição:* tá usando nove cores ele tá usando uma variação de tons eh mais claros para Mais Escuros identificando eh os Mais Escuros eh pessoas que votaram no Donald trump e na outra dimensão dos Claros pros os escuros eh pessoas que têm o medicade que é o sistema de saúde eh gratuito ou suporte para pessoas suportes gratuitos e e aqui eh e e tenta identificar se existe uma correlação entre pessoas que que precisam desse sistema de saúde e e e não não votar no no no Donald trump né então normalmente pessoas que que que tem você vê no nos Estados Unidos um comportamento isso aparece muito frequentemente eh mais claramente nos Estados Unidos do que no Brasil é uma divisão entre Democratas e republicanos os Democratas normalmente estão nas costas eh Oeste e enquanto que no meio dos Estados Unidos tem mais eh republicanos conservadores polarização eh deles né e aqui diga nesse caso volta um pouquinho slide Então nesse caso Azul seria o que quem a porcentagem de pessoas que mais botou no trump e que menos precisa de serviço Saúde Público Azul esse canto aqui superior direito isso isso são pessoas que T sistema de saúde e votaram no trump né então tem algumas regiões aqui especificamente que tem esse comportamento né e eu tenho aqui algumas eh pessoas que TM sistema de saúde não votaram no trump né e tem esses caras aqui que não tem sistema de saúde e votaram no trump são os Verde claros e tem uma boa parte aqui né que não tem que não são cobertos pel medic bem no interior dos Estados Unidos obrigado mas assim o o não importa os dados que estão colocados aqui basicamente o o que importa é que no caso aqui ele conseguiu mapear duas dimensões para cores né então dimensão a que tem uma escala de valores dimensão B que tem uma outra escala de valores e na hora que você faz um mapeamento para qual cor usar você você vê em qual dessas entradas da

- *Corpus ID:* 2183
- *Score:* 0.8416348099708557
- *URL:* oculto
- *Início:* 00:32:46
- *Fim:* 00:34:55
- *Transcrição:* maior então no processo de triagem ela tinha preferência E aí foram observar que o problema é porque tinha outras informações ali dentro que eram enviesadas Entre esses grupos que o algoritmo tava entendendo né que havia essa diferença e essa informação era custos com saúde Então como as pessoas a população Branca tinha mais custos com saúde né e ele esse custo com saúde era usado como proxy uma relação com a gravidade mas as pessoas brancas têm mais com saúde porque nos Estados Unidos também existe esse problema de desigualdade social e elas têm mais condições então o algoritmo entendia como existia mais custo com saúde no geral as doenças são mais graves Então as pessoas brancas têm mais risco e os custos menores para pessoas negras então ah elas gasta menos porque adoecem menos doenças Veras e na verdade não é isso então não foi exatamente uma variável super explícita por exemplo raça não foi uma outra variável que haviam né possuíam desequilíbrio de valores para uma informação sensível que é a raça e aí tava impactando no algoritmo então às vezes né pode ser vamos supor que tivesse mesmo número de pessoas brancas e negras que acho que não era o caso ali mas vamos supor Ainda assim eu poderia ter esse tipo de viés então e aí nesse exemplo aí até outra algoritmo erraria também por conta dessa variável aí de custo com saúde que é meu ponto de vista era um problema isso exatamente a dificuldade tá identificar essas variáveis que carregam essas diferenças sociais assim às vezes não de uma forma mais explícita né não de uma forma que pelo nome da variável pela descrição a gente entenda né explorando os dados ou explorando às vezes até explorando a saída dos modelos a gente consegue tentar mapear o que que tá acontecendo de erro aqui tipo o erro sistemáticos né tentar ver olha essas instâncias ele tá

- *Corpus ID:* 2182
- *Score:* 0.8365572690963745
- *URL:* oculto
- *Início:* 00:31:09
- *Fim:* 00:33:19
- *Transcrição:* interessante Ah tem uma pergunta deixa eu passar a Caroline Pode falar É o seguinte é sobre essa questão de vieses né como você tinha comentado pode vir da coleta dos dados e pode ser amplificado pelo algoritmo pode acontecer o caso de a gente ter o dado balanceado ou seja coleta foi bem feita e ainda assim o algoritmo ter esse viés acertar mais para um tipo de classe do que para outro pode acontecer pode acontecer porque às vezes a gente olha a distribuição de um atributo né Por exemplo a classe ou por exemplo quero ver seu atributo ali borracha ou idade o gênero tá mais ou menos equilibrado né às vezes mesmo que ele esteja equilibrado o que o algoritmo vê né na maioria dos algoritmos de uma forma muito clara é um padrão entre atributos E aí isso isso pode estar causando nessas diferenças por exemplo eu tenho alguns padrões que eu tenho daqueles daquelas combinações de atributos eu tenho muito mais exemplos torna mais fácil aprender aquele Exemplo né E às vezes tem o que a gente chama essas variáveis que são relacionadas assim por exemplo tem um caso bem famoso de um algoritmo que era usado para atribuir risco de saúde no sistema usado nos Estados Unidos e aí teve uns pesquisadores que acharam que o algoritmo ele prejudicava pessoas negras então se eu tinha uma pessoa branca uma pessoa negra com os mesmos sintomas a pessoa branca era atribuído um risco maior então no processo de triagem ela tinha preferência E aí foram observar que o problema é porque tinha outras informações ali dentro que eram enviesadas Entre esses grupos que o algoritmo tava entendendo né que havia essa diferença e essa informação era custos com saúde Então como as pessoas a população Branca tinha mais custos com saúde né e ele esse custo com saúde era usado como proxy uma

- *Corpus ID:* 4661
- *Score:* 0.8360002636909485
- *URL:* oculto
- *Início:* 00:53:48
- *Fim:* 00:56:03
- *Transcrição:* tem uma boa parte aqui né que não tem que não são cobertos pel medic bem no interior dos Estados Unidos obrigado mas assim o o não importa os dados que estão colocados aqui basicamente o o que importa é que no caso aqui ele conseguiu mapear duas dimensões para cores né então dimensão a que tem uma escala de valores dimensão B que tem uma outra escala de valores e na hora que você faz um mapeamento para qual cor usar você você vê em qual dessas entradas da Matriz ela cai e aí você usa ela para para Cor e para deseno é basicamente isso né né O que torna o gráfico mais rico ele tá usando Não somente uma escala de cores tá usando duas para mostrar informação interessante que quase não tem cinza né Professor ou seja quase não existe pessoas que não tem sistema de saúde e não voltaram né não voltaram pro trânsito estranho né porque eu acho que eh Pois é e talvez talvez seja um pouco difícil de ver mas tem uma eh esse esse canto aqui acho que é porque mas mas era para ter mais né não você localize eu acho que mais do lado esquerdo Aqui tá quase branco né Talvez seja né né É mas assim era para ter mais né Eu não eu não sei não sei o suficiente dos Estados Unidos para para dizer como Qual é o padrão da pessoa tem ou não tem esse esse plano de saúde Porque aqui no Brasil ele ele é para todos né então eu não sei como que que lá eles se eles têm o que que que faz a pessoa ter ou não ter mas mas mas isso aqui é só para ilustrar a que que você tem essa possibilidade de mapeamento mas mas obviamente que o que você quer Eh como consequência é fazer o que você tá dizendo você quer interpretar né você quer olhar e tentar entender tendências né a gente eh quando eh olhou para dados de de covid no Brasil a gente tentou olhar para algumas dessas tendências Deixa eu aproveitar e pegar algum exemplo de uma das visualizações que a

- *Corpus ID:* 4619
- *Score:* 0.8345021605491638
- *URL:* oculto
- *Início:* 00:58:18
- *Fim:* 01:00:57
- *Transcrição:* Estados Unidos e você quer mapear alguma informação associada à cor onde cor mais alta representar um por exemplo no nível alto de eh eh digamos assim de população por exemplo e branco é a população mais baixa né eh não sei se a população colocada aqui mas só para dar um exemplo esses mapas corop ou corop que a gente não tem uma tradição boa para isso eles eles surgiram também ao longo desse tempo né Hit maps ou mapas de calor que são usados em várias informações nada mais são do que você eh mapear alguma informação que você tem uma informação numérica que você atrela uma uma tabela de cores e você mateia para uma para uma representação geométrica nesse caso aqui você tem um uma matriz que ela tem S por 24 eh colunas sete linhas por 24 colunas respondendo às 24 horas do dia nos sete dias da semana e a em cada dessas eh eh células né Você tem uma hora do do dia né E e aí você mapeia paraa cor informações como a quantidade de tráfego que você tem na cidade onde você tem um no caso aqui ele usa uma escala de cores que vai do roxo ao vermelho Onde você passa por uma zona neutra que é usualmente aplicado quando você tem extremos E você tem uma digamos assim uma média uma normalidade seriam as cores neutras seriam os amarelos o laranja claro aqui né Qualquer coisa que tá mais pro roxo pro Azul pro vermelho mais forte são extremos tá então o que ele tá mapeando aqui é trá trânsito né então você vê que eh usualmente você tem um trânsito maior entre nove da manhã e 9 da noite em algum lugar por exemplo e de madrugada você tem menos trânsito O que é usualmente o que tá acontecendo né Aí você tem a alguns comportamentos por exemplo sábado você tem menos trânsito domingo você tem um pouco mais trânsito então pessoas eh aqui também sexta-feira tem um pouco menos de trânsito Então você consegue identificar Professor Ah Diga tá entre os mapas coropléticos e

- *Corpus ID:* 2638
- *Score:* 0.8330532312393188
- *URL:* oculto
- *Início:* 00:36:29
- *Fim:* 00:38:36
- *Transcrição:* ponto de vista prático é totalmente contrário porque a gente sabe que Paciência asmáticos teriam mais mais sensibilidade a esse tipo de complicação Então na verdade ele deveriam receber atenção né uma atenção nesse processo nessa triagem eles deveriam ter uma prioridade então o modelo se tornaria inválido do ponto de vista da aplicação interpretando esse modelo a gente poderia extrair essa informação Olha isso não se aplica real essa relação ela não existe ela vai estar prejudicando pacientes que deveriam receber uma prioridade de atendimento tá E aí tem tudo a questão de tomada de decisão Justa e ética né que obviamente tem uma relação né direto no que a gente discutiu mas é todo modelo de aprendizado de máquina a gente tem garantia que ele não é enviesado ele não prejudica um grupo um indivíduo por conta das suas características né e de uma forma que a gente considere antiética ou injusta né então diferenças que não deveriam estar sendo aplicadas né que não são aceitáveis do ponto de vista social legal certo e aí eu trouxe um conjunto de digamos assim né de Notícias que mostram são notícias algumas né bastante recentes de um dois anos para cá enfim que mostram que isso essa questão viés é desse envezamento que é considerado antiético continua acontecendo de forma recorrente tá então tem um caso bem conhecido que foi publicado na Saens que foi em relação ao algoritmo que era usado nos Estados Unidos para fazer Triagem de pacientes e basicamente esse algoritmo ele sempre atribuiu um risco menor a pessoas negras e aí foi ver olhando né os pesquisadores viram É que olhando o modelo existia uma informação que era custos em saúde e esse custos de saúde era usado como um proxy para necessidade de saúde Então se os custos

- *Corpus ID:* 2181
- *Score:* 0.8322915434837341
- *URL:* oculto
- *Início:* 00:29:31
- *Fim:* 00:31:44
- *Transcrição:* não tá sistematicamente errando mais para um grupo de pessoas por exemplo tem homens e mulheres o modelo a certa mais para homens Ou acerta mais para mulheres dependendo do caso isso pode ser é digamos assim desejável fazer essa diferença né muitos casos certo então só trazendo algumas coisas isso que eu achei interessante essa linha do tempo eu deixei o link para vocês tá pelo PDF vocês vão conseguir acessar então é um site que fez a linha do tempo do racismo algoritmo que os racismo do que ocorrem dos dados né Então aqui tem um por exemplo de 2010 que essas câmeras da Nikon né as câmeras digitais que faziam tag no rosto da pessoa tirar foto não encontrava rostos asiáticos então que você tinha pessoas asiáticas né e não asiáticas na foto conseguia dar um tag para digamos assim conseguir focar melhor no rosto das pessoas né mas não identificar asiáticos E aí tem a questão parecida aqui com essa estudante do Myth né que denunciou Esse reconhecimento facial em relação a pessoas negras certo então bom vocês podem ver né aqui tá aqui esse é de 2017 Mas podem ver é quantidade de coisas que tem aqui depois quiserem dar uma olhadinha Então são inúmeros casos inúmeros casos em que isso acontece tem esse artigo ele é um artigo bastante interessante Ah tem uma pergunta deixa eu passar a Caroline Pode falar É o seguinte é sobre essa questão de vieses né como você tinha comentado pode vir da coleta dos dados e pode ser amplificado pelo algoritmo pode acontecer o caso de a gente ter o dado balanceado ou seja coleta foi bem feita e ainda assim o algoritmo ter esse viés acertar mais para um tipo de classe do que para outro pode acontecer pode acontecer porque

- *Corpus ID:* 4700
- *Score:* 0.8318518996238708
- *URL:* oculto
- *Início:* 00:32:18
- *Fim:* 00:34:44
- *Transcrição:* do Sul tem muitas muitas vinículas aqui para saber se eles teria o interesse em algo parecido como com isso aqui bom eh agradeço vocês estão por falar sobre o projeto e mostrar Bom enfim Ahã Então a nossa interface ela ela era interativa né permitia você poder trocar os anos e essa parte de combinar a interação com a visualização é bastante importante Principalmente quando você faz também eh configurações do algoritmo você você faz filtragens você quer entender bom um uma das coisas que a gente acabou eh eh tendo que que alterar nesse processo também foi foi essa tabela de cores que a gente trabalhou ela os os biólogos não gostaram dela inicialmente porque ela não era natural para eles eles achavam que e e é verdade né a gente basicamente introduziu cores artificiais que não tinham nada a ver com o que tava sendo representado nesse caso aqui é um percentual de verde a gente estava usando cores que não tem nada a ver com o verde né tá a gente pensou diferentes alternativas a gente implementou na nossa interface diferentes tabelas de cores você podia entrar na interface e configurar qual era a tabela de cor que você cores que você você poderia usar E aí a a gente implementou uma que basicamente a gente convertia o o a gente basicamente usava um espectro hsv que é aquele espectro que que que pega quase todas as cores do arcoíris e e para para fazer esse mapeamento a gente convertia o valor da da coordenada para hsv e o que era interessante é que os valores de verde apareciam em verdes embora os outros não apareciam e aí a gente calculou essa imagem para diferentes anos aqui a gente tem de 2004 lá 2012 e e e o que é interessante e a gente já tinha visto naquele problema da dos corredores é que isso é uma imagem do ano É como se eu tivesse uma fotografia que simbolizasse o ano e e a gente vê que tem anos que são muito parecidos com outros mas tem anos que


**Resposta gerada pelo LLM**: O sistema Compass atribuía um risco maior de culpa para afro-americanos devido a vieses nos dados, perpetuando assim o viés racial.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: A resposta deve ser relevante para todo o PROMPT, abordando como o sistema Compass perpetuava o viés racial em relação a afro-americanos. Supporting Evidence: A resposta menciona que o sistema Compass atribuía um risco maior de culpa para afro-americanos, o que é diretamente relevante para a questão de como o sistema perpetuava o viés racial. Ela também aponta que isso se deve a vieses nos dados, o que fornece um contexto adicional sobre a origem do viés. A resposta é clara e aborda a questão de forma completa.  Score: 3


---
