**Pergunta 97**: Como a variabilidade dos dados é medida e interpretada? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 3875
- *Score:* 0.8616801500320435
- *URL:* oculto
- *Início:* 01:02:19
- *Fim:* 01:04:44
- *Transcrição:* se nós trocarmos os números por categorias né é bom médio e ruim bom excelente ruim não tem como eu garantir que o que o que o que o excelente é duas vezes melhor do que o ruim ele só é melhor do que como é dado pode fazer uma uma distribuição quando a distribuição desses dados assim eu tô só pensando tá mesmo que você falou vou fazer uma distribuição desses dados e de repente pode pegar a probabilidade por exemplo ser baseado na probabilidade daquele valor baseado só naquela na coluna né que esses dados categóricos estão sei lá aí você sei lá você tem Ah o ele aparece mais por exemplo ah ele tá ali no meio tá próximo do 50% de ser um M ah sei lá ser um ar tá perto ali do começo até 10% dessa distribuição ele representa Então esse é o ponto que eu queria chegar então algumas pessoas aceitam em algumas áreas alguns instrumentos digamos assim por exemplo a escala ali que ele tinha um instrumento que permite a uma flexibilidade as pessoas transformam em números e fazem operações sobre esses números aí vocês tem que avaliar se naquele domínio aquele instrumento aquela questão que vocês que fizeram permite esse tipo de interpretação Pode ser que alguma situação sim e outra situação não mas vocês pegam um banco de dados qualquer começa a ficar difícil né tem que ter um analista de domínio para dizer a essa dimensão eu posso considerar esses números né E aí fazer uma usar uma equação tradicional não aqui não dá e aí tem outras opções você mencionou uma delas né eu faço uma análise de distribuição dos valores e calculo percentual vejo posso calcular ocorrências também usar essas ocorrências como como possibilidades eu trouxe para vocês aqui uma é um artigo também aonde ele ele discute essas possibilidades né então mais importante é saber que isso que nem

- *Corpus ID:* 68
- *Score:* 0.8611226677894592
- *URL:* oculto
- *Início:* 00:34:16
- *Fim:* 00:36:36
- *Transcrição:* muito baixinha quanto gente muito alta e a distribuição de altura é simétrica tá outra medida importante é medir variabilidade tá gente variância dizia o padrão a variância na verdade a gente calcula ela primeiro para depois obter o padrão é raiz quadrada da variância mas a gente interpreta o desvio padrão tá porque a variância tem um problema ali de estar ao quadrado já vamos entender o porquê tá pensem o seguinte aqui os nossos valores lá são 11.6 esse aqui se eu não me engano é um exemplo ali dos acho que é tá gente dos 50 valores ali do número de caracteres do do número de caracteres dos e-mails tá então 50 e-mails ali eu calculei a média a média deu 11,6 E aí por exemplo lá peguei uma observação então tem 21.7 então deve ser 217 tá gente caracteres no e-mail e seu diminuir isso da Média eu tenho 10.1 ou seja esta observação aqui e está 10.1 Acima da Média se eu pegar um outro lá que deu o resultado 7 e diminuir da Média eu tenho menos 4,6 então é essa observação está 4.6 pontos abaixo da média e assim por diante né então eu vou ter observações que vão estar para o lado esquerdo e observações direito lado esquerdo e lado direito da média são maiores que a média né menores ou maiores que a média tá quanto mais elas tiverem para um lado para o outro e mais distante isso vai estar me dizendo o que que os meus dados têm variabilidade então variando no momento que eu comecei a calcular essas diferenças aqui eu estou querendo medir o quanto elas variam o quanto elas estão distante da Média tá essa é a ideia bom se eu somar estes valores aqui gente dá zero se eu sou mal 50 valores que eu tenho aqui dos distanciamentos de cada né esse desvio essa variabilidade desvio de cada valor em relação a média se eu

- *Corpus ID:* 3702
- *Score:* 0.8604817390441895
- *URL:* oculto
- *Início:* 00:08:54
- *Fim:* 00:11:05
- *Transcrição:* elas são são mais descritivas ou funcionam melhor do que do que eventualmente para para outras não é a gente tem aqui uns exemplos de variação a gente já viu lá um exemplo de variação bem bem alta aqui até que elas estão razoavelmente também distribuídas dentro dos diferentes classes Mas enfim a ideia é que eu possa eventualmente avaliar se alguma dessas dessas variáveis não tendo uma distribuição normal E aí ao mesmo tendo E aí ela tendo uma média de um padrão muito altos não fica que ela não não consegue distinguir bem os elementos não é o ser eventualmente utilizado poderia ser excluída e também outra coisa que a gente pode pensar né uma análise de correlação que nós temos lá em cima antes fazendo uma avaliação se realmente a curva é normal algo que nós vamos retomar mas adiante você já devem ter estudado vai acontecer no caso real deu deu ter eventualmente 10 15 ou mais atributos né Tem situações aí de colegas que já estão na turma mais mais que começou antes né que já estão fazendo monografias que estão usando o conjunto de dados que tem 30 40 atributos né Aí fica fica uma coisa quase que não diga impossível mas talvez não faça sentido pelo tempo de processamento e também pela pela pela complexidade disso na ideia reduzir então a gente pode usar análise fatorial pode usar PCI ou pode simplesmente fazer uma análise correlação Eu tenho dois dois atributos altamente relacionados né eles crescem juntos funcionam muito bem juntos talvez eu possa usar um deles só não preciso usar os dois não é então também é um tipo de análise simples que me ajuda de alguma maneira identificar atributos né que que possam ser usados então aqui foi só uma análise simples para tentar entender o conjunto de dados e as

- *Corpus ID:* 60
- *Score:* 0.8601138591766357
- *URL:* oculto
- *Início:* 00:20:34
- *Fim:* 00:22:56
- *Transcrição:* daí se todos os valores são iguais não é mais uma variável constante tá então mudando o exemplo se um valor for muito alto em 99 99 valores foram iguais a mediana ela ficaria valor quase uma constante esse valor esse sei lá pouquíssimos valores diferentes a gente bota a dúvida se eles Primeiro eles vão ser né eles vão ser Atlas tá e a questão é a gente vale a pena examinar esses outlines esses outlines fazem parte realmente do que eu tô medindo se eu decidir que não não vamos olhar para os outlares eu vou excluir eles na minha análise que é uma das abordagens e eu voltei até a minha constante de fato o que que eu tô dizendo a base da estatística é a variabilidade se as coisas não variassem a gente não ia querer identificar padrões não ia querer medir essas diferenças entre os valores né então quanto mais variabilidade tiver mais é a necessidade de usar métodos estatísticos então se eu tiver casos em que eu tenha pouca variabilidade não faz sentido né ou pouco quase nenhuma não fazem muito sentido [Música] analisar esse dado queria fazer uma pergunta em termos matemático se tivesse numa prova lá Acha mediana de tal e ela é uma aquela constante lá eu não teria resposta é isso ou não faz sentido exatamente o valor tudo seria a Diana seria ela e a média seria ela isso mesmo valor bom Gente uma outra forma então de avaliar distribuição dos dados né além do histograma veja né o histograma dá essa ideia de como os dados estão distribuídos outro gráfico também é bastante utilizado é o box plot não vamos se atentar tanto assim de como construir ele porque isso aqui é o r faz para nós tá mas só para a gente entender primeiro Box tá Da onde que tem essa caixa aqui tá esta caixa aqui os limitantes dessa caixa aqui e aqui são

- *Corpus ID:* 294
- *Score:* 0.8600140810012817
- *URL:* oculto
- *Início:* 00:11:16
- *Fim:* 00:13:16
- *Transcrição:* quanto varia os meus dados ou sei lá é um estudo que eu repito muito tempo então eu tenho dados históricos ou alguém já estudou sobre isso que eu estou agora querendo fazer referência e eu tenho uma ideia do quanto esses dados variam é o que eu vou substituir aqui né então isso aqui é s sobre raiz de n Então esse s é essa estimativa que a gente em algum lugar ou de algum estudo anterior ou eu vou lá e busca uma amostra aqui com buchinha tá nós observações para ter ideia dessa variabilidade bem ideia tá E aí eu vou falar e digo eu quero errar tanto eu vou ter Sua Mãe coloca aqui que é o n e aí eu consigo descobrir o tamanho da minha amostra para poder ter no máximo aquela margem de erro Então esse é o caminho inverso que acontece lá na eleição Nós já vamos no nosso exemplo fazer isso na mão tá gente só para a gente entender a ideia já vai aparecer esse cálculo a gente vai lá para que serve vocês vão chegar nesse n para o nosso exemplo tá O que que significa esses 95% de confiança gente tá quer dizer o seguinte a minha amostra eu vou lá pego o meu x barco aqui é minha amostra tá aqui o meu intervalo que eu vou conseguir com a minha amostra peguei minha amostra e obtive tá aqui a minha média Esse é o meu x/ eu pego o meu x-barr e estico dois desvios para um lado dois dias para o outro né 1,96 é o padrão um 96 é o padrão Aí vai lá Carol pega a mostra dela tá aqui a média dela ela anda um 96 eu vou padrão um 96 é o padrão e assim por diante cada um de vocês já pegou uma amostra dele um 96 é um padrão um 96 é o padrão nós vamos gerar muitos intervalos de confiança Qual é a ideia gente desse 95 5% de nós seremos as arados pegaremos a amostra azarada lembra das amostras aquelas das causas ou que vão super estimar ou que vão subestimar 5% de nós seremos azarados ou seja em 5% das vezes eu vou errar que é o contrário da minha confiança

- *Corpus ID:* 2321
- *Score:* 0.8574227690696716
- *URL:* oculto
- *Início:* 00:10:44
- *Fim:* 00:12:54
- *Transcrição:* difícil eles conseguiria aprender isso se eu tenho uma mesma estrada que gera duas saídas distintas então isso às vezes pode acontecer por exemplo de variabilidade intrínseca aos avaliadores adotadores daqueles dados a própria evolução da definição das classes ao longo do tempo né então esses dados foram coletados e múltiplos períodos né ou estantes de tempo pode acontecer que o padrão tá mudando Então antes aquela característica tinha uma saída x passa o tempo aquela mesa características pode acontecer claro que do ponto de vista dos modelos é um problema tá mas pode acontecer e o último ponto seria ter dados com o volume né grande o suficiente para permitir a generalização daquele modelo né para novos dados E aí Claro a questão de saber qual é o volume ideal é muito difícil a gente não tem acho que a gente já tocou um pouco desse assunto a gente não tem como estimar isso através de por exemplo cálculo de tamanho amostral que daí tem da estatística né a gente não tem nada consolidado com aprendizado de máquina então isso vai ficando o volume de dados vai ficando mais claro a medida que a gente desenvolve o modelo porque a gente vai vendo né digamos assim padrões por exemplo para o modelo não consegue reconhecer adequadamente a gente consegue identificar através da análise dos erros dos modelos de aspectos como interpretação do próprio modelo e consistências né entre o que ele tá gerando de sair do que a gente espera da saída para poder entender aonde que a gente tá o que que está nos faltando de dados digamos assim bom comentando um pouquinho de alguns problemas tá E são problemas que a gente vai abordar ao longo da aula de como é que a gente poderia lidar com eles então

- *Corpus ID:* 74
- *Score:* 0.8564593195915222
- *URL:* oculto
- *Início:* 00:44:00
- *Fim:* 00:46:12
- *Transcrição:* pontas porque se eu pegar os valores das pontas eu vou conseguir enxergar na minha amostra tudo da variabilidade que eu tenho na população mas é muito mais provável que as pontas não apareçam e por isso na hora evacuar a variedade eu vou dizer que ela é menor do de fato é entenderam E aí por isso que a gente faz essa essa correção na variância amostral só na amostral tá gente se eu tivesse acesso à população dividiria pelo n da população na amostra eu faço essa correção para dar aquela subidinha na né No momento que Eu dividi por 49 o valor ficou um pouquinho maior que o seu dividido por 50 é isso para tentar então equilibrar corrigir esse esse subestimativa que de fato vai acontecer no momento que eu pego uma amostra de uma população beleza E esse ajuste ele é muito significativo né quando o n é pequeno mas quando o n é aumenta é isso tende a zero digamos né Por 49 agora ou 50 na minha dúvida é 50 ou 49 se isso aqui fosse 5 mil e quatro mil noventa e nove tem coisa percebeu dividir por 5.000 dividido por 4 99 então quanto maior o tamanho da minha amostra menos é né importante né a divisão para ele menos um mas a fórmula traz né que tem que dividir por ele menos um tem prova matemática para isso que é uma forma então da gente não subestimar a variabilidade populacional seguindo gente eu não fui perde ainda Bora lá tá aqui eu só tô mostrando o quanto modificar valores extremos mexe com a média e o desvio padrão que vocês tinham dito ali lembram a pontinha lá vocês disseram quanto mais aquele 60 fosse mais para esquerda ainda maior seria a média tipo a ideia de pensar em distribuição de renda quanto mais algumas pouquíssimas pessoas tem renda

- *Corpus ID:* 4040
- *Score:* 0.8562150001525879
- *URL:* oculto
- *Início:* 00:23:58
- *Fim:* 00:26:14
- *Transcrição:* remover aquilo que não é essencial então eles fazem isso usando estatística né matemática e ver tenta de uma maneira analisar a distribuição dos elementos e ver então aquilo que tem mais variabilidade e tenta representar essa variabilidade e as dimensões os elementos que tem pouca variabilidade eles são são ignorados são excluídos porque eles não agregariam muita muita informação né então informação na própria teoria da informação tá nessa diferença entre as coisas na variabilidade entre as coisas não interessa saber coisas que são iguais assim aquilo que é diferente que que elas são diferentes então representar o que é diferente é que dá informação aquilo que é igual eu sabendo que é igual não tem mais informação a ser acrescentada então dá para reduzir resumir assim a teoria por trás dessa dessa maneira né E aí o que acontece é que o PCA faz essa transformação ortogonal então eu vejo crianças essas linhas ortogonais quer dizer elas são ortogonais de conceito automonalidade ele tem a ver com isso são coisas que não se misturam né Vocês pensarem em linhas que tem 90 graus Entre uma e outra significa que se eu estou caminhando por uma delas eu estou fixo na outra na outra dimensão né Então esse é o conceito ortogonalidade é uma não influencia na outra então se eu mexo no valor de uma como a outra tá fixa eu não mudo o valor dessa outra então coisas que não se misturam é um segredo tá em achar esses componentes principais que são ortogonais entre si e que representam então a variação naquela naquela naquele sentido não é independente dos outros digamos assim então o componente principal são essas dimensões ou elementos que capturam a variância nos dados E aí eles possuem uma uma direção e uma magnitude ou um tamanho né então o segredo tá em

- *Corpus ID:* 2343
- *Score:* 0.8561623692512512
- *URL:* oculto
- *Início:* 00:46:30
- *Fim:* 00:48:49
- *Transcrição:* faltante tipo substituí-lo pelo valor faltante tratar esse dado junto com os valores faltantes através de métodos de imputação então por exemplo aqui essa temperatura 97.6 eu poderia tentar estimar esse valor através de um método de imputação para fazer isso eu pego esse outro lá e substitua por um valor vazio e aí trata os meus valores vazios nulos da base de dados e automaticamente está tratando esse erro aqui tá esse erro ou o Outlander outro caso e depois tem duas questões que seriam alterar esses valores um é fazer um Billy tá que basicamente o Billy é uma descritização de dados tá eu pego os dados do médicos por exemplo temperatura né e separo em faixas intervalos de tamanho uniforme então sei lá de 36 a 37,5 de 37639 enfim e aí esses esses intervalos esses todos os valores que caem no mesmo intervalo eu posso optar por pegar esse esses valores que são autores vão estar nos extremos né e substituir pela média mediana da faixa correspondente ou até categorizar os dados o momento que o categoriza os dados numérico para categórico eu acabo elimido essa atuais porque ele vai estar numa faixa que vai se transformar numa categoria então todo mundo que tem Claro que uma faixa mais alta né então todo mundo que tem esse o valor 97.6 ou sei lá 41.5 vamos pensar 40 enfim temperatura super altas vou ter lá a mesma categoria que indicando que a temperatura tá muito acima do normal tá então a gente elimina esse esse alto lá e outra questão é fazer o que a gente chama de capim que é substituir os valores baseado percentil fixo então todos esses que são muito discrepantes que são ruídos ou outlines a gente ele bebeu pelo percentil 3 pelo valor que representa o percentil 3 na minha distribuição ou percentil 97 né então os mesmos extremos digamos assim

- *Corpus ID:* 4041
- *Score:* 0.8553566932678223
- *URL:* oculto
- *Início:* 00:25:37
- *Fim:* 00:27:55
- *Transcrição:* coisas que não se misturam é um segredo tá em achar esses componentes principais que são ortogonais entre si e que representam então a variação naquela naquela naquele sentido não é independente dos outros digamos assim então o componente principal são essas dimensões ou elementos que capturam a variância nos dados E aí eles possuem uma uma direção e uma magnitude ou um tamanho né então o segredo tá em matematicamente descobrir né Qual é a direção de cada um desses componentes e a magnitude deles e aí fazer esse ajuste no espaço de dimensões para que eles têm uma direção uma magnitude que represente a maior quantidade de variância possível do conjunto de dados E aí a ideia é que esse esses elementos eles vão ter uma uma correlação né com quantidades originais bom aqui tem tem uma explicação mais detalhada para quem gosta de matemática né mais importante é que eu consigo Então criar um conjunto mínimo aí de componentes vou chamar de dimensões os PCs como os principais só podem ser como dimensões que substituem substituir as dimensões originais a ideia então é eu pego o conjunto de dados que eu tenho eu aplico a técnica de PCA para criar um novo conjunto de dimensões que são os componentes principais E aí essa esse algoritmo ele vai então fazer esse ajuste dessas componentes que são linhas que separam o espaço colocando essas linhas nas melhores posições possível para manter a representação dos dados originais e aí eu uso esses componentes no restante do processamento seja a mineração de dados ou visualização e aí como eles são uma quantidade menor eu


**Resposta gerada pelo LLM**: A variabilidade dos dados é medida calculando-se a variância (e seu desvio padrão, que é a raiz quadrada da variância). A variância calcula as diferenças entre cada observação e a média, e o desvio padrão é interpretado como a dispersão dos dados em relação à média.  Quanto maior o desvio padrão, maior a variabilidade.  Métodos como o histograma e o box plot também auxiliam na visualização da distribuição dos dados e, consequentemente, na compreensão de sua variabilidade.  A variabilidade é crucial na estatística, pois sem ela não há padrões a serem identificados.  Em casos de pouca variabilidade, métodos estatísticos podem ser desnecessários.  A análise da variabilidade também é importante para identificar outliers, que podem ser excluídos da análise ou tratados por métodos de imputação, como o binning ou capping.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: A resposta deve ser relevante para a pergunta sobre como a variabilidade dos dados é medida e interpretada, abordando os métodos de medição e a interpretação dos resultados. Supporting Evidence: A resposta aborda diretamente a medição da variabilidade dos dados, mencionando a variância e o desvio padrão, que são métodos fundamentais para essa medição. Além disso, explica como a variância é calculada e como o desvio padrão é interpretado em relação à dispersão dos dados. A resposta também menciona métodos visuais, como histogramas e box plots, que ajudam na compreensão da variabilidade, e discute a importância da variabilidade na estatística, incluindo a identificação de outliers. Isso demonstra uma compreensão abrangente do tema, cobrindo tanto a medição quanto a interpretação da variabilidade.  Score: 3


---
