**Pergunta 49**: Quais são as fontes de variação que podem afetar a avaliação de modelos preditivos?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2706
- *Score:* 0.8625576496124268
- *URL:* oculto
- *Início:* 00:26:06
- *Fim:* 00:28:10
- *Transcrição:* permutação desses valores eu tô perdendo um pouco a capacidade preditiva assim no sentido de estar perdendo um pouco em desempenho e aí essa esses aqui contrato né a cada ano contrato a cada dois anos eles tiveram um valor significativo na hora da permutação Tá mas no sentido contrário aqui de negativo ou seja como se eu tivesse um desempenho maior tá essa questão de aumentar o desempenho na verdade na maioria das vezes é difícil a gente interpretar Porque isso pode ser um efeito de permutação ou seja não necessariamente não posso é difícil a gente dizer assim no sentido porque se eu permutar e melhorar a gente sabe que tem impactando mas eu não posso dizer bom eu não tenho usar esse valor perguntado o valor dele real né É aquele ali é fácil a gente interpretar pelo menos trazer uma uma conclusão quando a gente vê que ao permutar esse valor Caio desempenho agora se aumenta o desempenho a gente tem que é muito mais complexo a gente analisar porque a gente tem que ver se tem ruído tem problema nesse nesse atributo enfim né que poderia estar impactando no desempenho real né então talvez seja um atributo para a gente tirar do modelo porque ele não tá ajudando o modelo do desempenho real não sei se Ficou claro né mas é quando a gente vê essa questão essa variação negativa a interpretação que tá acontecendo aqui é um pouco mais difícil porque pode indicar algum problema no modelo né E aí percebam diferenças né aqui o contrato mesa mês ele teve um aqui não tão importante negativo mas não tão importante magnitude ao contrário do que apareceu lá no outro que foi um outro critério né de florestas aleatórias baseado na medida da análise de impureza certo então qualquer algoritmo pode ser usado aqui tá qualquer algoritmo pode ser usado basta que a gente defina aqui o meu modelo aqui eu tô usando o próprio

- *Corpus ID:* 2177
- *Score:* 0.8607672452926636
- *URL:* oculto
- *Início:* 00:23:02
- *Fim:* 00:25:13
- *Transcrição:* treinamento do modelo né mas da parte já é essa divisão dos dados dados Independentes treinamento a validação enfim se eu fizer isso de forma Impecável se os meus dados forem muito ruins essa modelagem Impecável do ponto de vista de metodologia ela não salva esses dados ela dificilmente salva esses dados certo então a gente tem que cuidar a gente tem que entender o que que a gente precisa trabalhar em nível de dado né para que a gente possa tentar melhorar esse esse dado aqui que não é não é bom mas a gente também tem que cuidar para que a nossa metodologia do ponto de Treinamento não insira viésis não faço uma avaliação ruim né do ponto de vista de não perceber que tem um overfite tem coisas desse tipo ok e tem a questão da metodologia né do ponto de vista de risco de vieses e esse ponto apareceu um pouquinho naquele notebook que a gente fez né do nave base com dados de risco de doenças cardíacas em que tinha pessoas de diferentes Ali era variável raça né quantificada então e aí chamava atenção o caso ali que por exemplo nós tínhamos não tínhamos asiáticos com registro para doença né cardíaca e a gente discutiu um pouco isso pode ser uma questão de amostra de dados né um viés de amostra enfim a questão é que quando a gente não não Cuida dessa questão metodológica do desenvolvimento dos modelos e nesse ponto tá incluso né A questão da gente avaliar os dados perceber se a gente tem potenciais vieses enfim a gente acaba criando modelos com viés Então o que a gente tem que ver é que o nosso modelo ele não pode se cumprir com o nosso objetivo da tarefa de aprendizado de máquina né claro que a gente quer fazer isso aí mas isso não é a única questão a gente tem que garantir que ele não tenha implicações sociais

- *Corpus ID:* 2224
- *Score:* 0.860723614692688
- *URL:* oculto
- *Início:* 00:11:32
- *Fim:* 00:13:34
- *Transcrição:* algum tipo de uma entrada sub padrão digamos assim de entrada específico Então essas características dessas instâncias né juntas formam os seus padrões sobre padrões e esse padrões podem mudar de distância parece tanto as mesmas instâncias associadas na mesma classe posso ter né grupos diferentes Então essa é uma limitação do roldalto mesmo ele repetido ele não garante que todo instante você vai ser usada pelo menos uma vez para teste tem uma pergunta o Jean pode falar Jean e a diferença do da avaliação dos 10 treinamentos for muito grande a discrepância for absurda quer dizer que o meu dado está envenenado vamos dizer assim tipo assim aquele meu dado não é bom Se for muito grande a variação E aí bom aí Claro depende do tamanho do teu data 7 né mas pode sugerir pode sugerir tanto que o teu modelo tá muito sensível aos dados que ele foi treinado E que esses dados que que foram usados treinamentos não comntemplam todos os padrões de entrada espera receber no teu dado né então pode ser essa questão essas instâncias que foram deixadas para teste não tinha nada parecido no treino por exemplo então eu não conseguiu aprender aquilo E aí é corre essas variações né mudando muito então quer dizer que ele tá sendo muito sensível a esses dados de entrada Talvez os dados possam ser um conjunto limitado então mesmo deixando o máximo possível para treino Você ainda tem poucos né então eles se torna mais sensível essas variações enfim tem acho que até diversos motivos pode ser o algoritmo que tá utilizando as informações uma árvore de decisão né a gente sabe que ela é bem sensível aos dados tem uma série justificativas Mas é uma coisa que chama atenção é preferível eu ter um desvio como a gente falou um desvio padrão uma variância mais baixa

- *Corpus ID:* 2641
- *Score:* 0.8602325916290283
- *URL:* oculto
- *Início:* 00:41:07
- *Fim:* 00:43:06
- *Transcrição:* que é mais explícita assim para esse caso né então não nem sempre é fácil identificar as variáveis que estão carregando esse viés mas mas seria o ideal a gente tentar identificar seja por um conhecimento prévio seja por uma análise protória dos dados seja usando algumas métricas que a gente tem métricas que tentam analisar o potencial de baias antes do treinamento né o potencial de várias carregado por cada variável mas tudo isso do ponto de vista de métricas de métodos para tratar esse tipo de problema ou evitar ou tratar ainda são coisas em desenvolvimento tem bastante coisas já que foi desenvolvida que está sendo usada mas a gente pode dizer sim o problema ainda não tá 100% resolvido sabe ainda tem muito do ponto de vista de pesquisa e desenvolvimento para ser feito para realmente tornar algoritmos mais seguros em relação a isso mas de fato uma possibilidade seria isso identificar esses riscos de viés nos dados e tirar essas variáveis que carregam esse viés seja de uma forma bem explícita né ou seja dessa forma mais mascarada assim então não é uma tarefa muito fácil e a gente só consegue Observar isso imagino eu nos modelos interpretáveis e isso ou usando técnicas para interpretar os modelos né a prioridade seria usar o modelo interpretável tá E seria prioridade porque a gente consegue ver exatamente que o modelo tá aprendendo as outras técnicas a gente usa recursos para tentar estimar né Essas associações ou essas essas associações do ponto de vista né do que que tá explicando uma saída então quando a gente Depende de um outro método para compreender o nosso modelo a gente sempre fica refém do que esse método faz né E até que ponto ele consegue de fato encontrar isso né Então realmente os modelos interpretáveis eles

- *Corpus ID:* 2539
- *Score:* 0.859893798828125
- *URL:* oculto
- *Início:* 01:32:02
- *Fim:* 01:34:16
- *Transcrição:* variância de valores que tem parece que tem muitos valores faltantes são tipo variáveis né super mal preenchidas assim ponto de vista acabam sendo meio inúteis assim no processo de modelagem isso tudo a gente faz antes completo porque nada tem vazamento de dados quando eu faço análise entre duas duas variáveis não variável e saída mas duas variáveis não tem vazamento de dados eu poderia é digamos assim porque eu tô removendo né eu tô eu não tô mudando eu não tô adiciodo informação no meu j7 eu não tô mudando as minhas variáveis eu tô removendo coisa então quando eu removo coisa não vai ter vazamento de dados agora quando eu tô usando essa remoção a fim de otimizar meu desempenho melhorar meu desempenho Aí sim aí começa a ter porque a minha remoção tá usando o desempenho como um critério aí pode ter vazamento de dados Mas alguma pergunta pessoal Então essa questão do documento ela ajuda bastante tá e o que eu vou fazer o que eu vou fazer o seguinte pessoal eu vou só atualizar os slides ali para organizar algumas coisinhas aqui que eu vi que tinha pequenos inconsistências assim mas é mais da ordem das coisas enfim eu vou atualizar os slides da aula 5 e 6 tá quem já baixou o conteúdo vai ser o mesmo só vou aprimorar alguns aspectos que eu vi que poderiam ser melhorados e a gente fica então os notebooks eles já estão disponíveis quem quiser dar uma olhadinha Pode ficar à vontade Tá mas a gente vai passar pelo notebook especialmente um que compara PCA e seleção de atributos a gente vai passar na sexta-feira por ele tá porque eu queria que vocês vissem que pontista prático O que que a gente tem quando usa um PCA que que a gente recebe como é que a gente usa isso e como é que funciona né quando a gente usa uma celebração de atributos certo uma questão final antes da gente

- *Corpus ID:* 2707
- *Score:* 0.8595264554023743
- *URL:* oculto
- *Início:* 00:27:34
- *Fim:* 00:29:55
- *Transcrição:* indicar algum problema no modelo né E aí percebam diferenças né aqui o contrato mesa mês ele teve um aqui não tão importante negativo mas não tão importante magnitude ao contrário do que apareceu lá no outro que foi um outro critério né de florestas aleatórias baseado na medida da análise de impureza certo então qualquer algoritmo pode ser usado aqui tá qualquer algoritmo pode ser usado basta que a gente defina aqui o meu modelo aqui eu tô usando o próprio modelo já treinado mas poderia ser outro modelo E aí Os dados aqui de treinamento e basicamente a gente tá fazendo né Essas repetições com base nos dados de Treinamento percebam que eu não tô usando todos os dados aqui certo só de Treinamento que foi o que o modelo aprendeu então e aí claro essa questão do valor Isso aqui é uma Estimativa de medida de avaliação de variação de desempenho tá é até mais comum a gente trabalhar isso de forma relativa ou seja Olha a gente percebe que tem é o que mais impacta nessa nessa redução de desempenho né Muito enfim quase o dobro do que esse terceiro que quer Internet service dsl então claro que a gente pode citar esse valor como um valor atingido Mas isso é sempre uma estimativa Então ela serve muito mais para a gente ter uma relação entre esses atributos do que de fato dizer que esse vai ser o impacto que a gente vai ver realmente algo não utilizar a utilizar ou não esse atributo Salvador tem uma pergunta pode falar minha professora é eu não entendi como é que interpreta o que aconteceu com esse atributo contra porque o temer o totochat e o contrato eram os três melhores antes de permutar em termos de importância tem que ver no outro gráfico e agora fez a permutação mostraram que o desempenho piorou então tô entendendo

- *Corpus ID:* 1724
- *Score:* 0.8592382669448853
- *URL:* oculto
- *Início:* 00:48:38
- *Fim:* 00:50:53
- *Transcrição:* assim quando vocês decidem por usar o Anderson que a gente chama o ovo ele já vai gerar para vocês uma decisão uma valor por classe e qual é a classe e predita assim como o ano versus All Mas é interessante a gente começar essas discussões quando a gente falar da combinação de múltiplos modelos por exemplo usar um naipes dele para tratar o mesmo problema eu vou cair nesse ponto de pensar bom como é que eu agrego essa saídas né Então essa discussão ela já é útil para quando a gente começar a pensar nisso porque a gente vai discutir sobre isso também tá mas do ponto de vista do svm vocês podem pensar o seguinte isso tá resolvido internamente o que vocês vão definir se vocês vão usar o universo do ano ou o ano versus professora Oi pode fazer uma pergunta Talvez assim um pouco mais geral por exemplo né a gente gasta um trabalho grande aí né nessa fase de análise exploratória dos dados experimentação dos hiper parâmetros até encontrar um modelo que seja melhor para o nosso problema e aí beleza colocamos ele em produção modelo tá ali funciodo bem durante o primeiro ano e aí com o passar do tempo sei lá as características da população vai mudando e esse modelo começa a errar é importante que que esse modelo ele seja avaliado então e seja acompanhado quando ele foi posto em produção de forma que quando a gente observa que ele começa a errar muito ele tem que ser refeito Escolhido um outro modelo para tratar aquele problema sim e sim isso é bem importante porque muitos dos problemas a vida real eles de fato mudam com o tempo né e é isso alguém comentou da siglas ali desculpa e é isso muda com o tempo e isso é natural assim acontecer só que o problema é que quando a gente tem isso a gente chama de data certifiques tá de um shift deslocamento dos dados e é esse deslocamento pode ser em função as classes estão mudando por exemplo antes

- *Corpus ID:* 628
- *Score:* 0.8588484525680542
- *URL:* oculto
- *Início:* 00:19:42
- *Fim:* 00:21:55
- *Transcrição:* ser avaliada traduzido fator de inflação da variância mas do inglês é VIP ele tem que ser menor que 10 tá então se o vice der menor que 10 eu tô dizendo que sim existe correlação entre os chineses Mas é uma correlação tolerável Tá aceita tá que não caracterizaria excesso de múltipleneidade estaremos então de forma aproximada atendendo a ausência da multicoloridariedade tá tendo multicoliaridade qual é o problema tá que que é multicolinaridade são variáveis explicativas altamente correlacionadas entre si ou seja elas dão informações similares para prevenir não é a idade do cliente e a renda dele que as duas Breves estão altamente correlacionadas Como estão a maioridade maior deve ser a renda dessa pessoa e aí fica difícil separar o efeito de cada uma que cada uma tá exercendo ao prever o y e isso faz o que prejudica a habilidade preditiva do modelo Tá o que que isso tem como consequência indecisão nos parâmetros estimados que que a gente que pode acontecer tá gente e prejuízo na interpretação né desses estimadores o que que pode desses parâmetros estimados que que pode acontecer se eu botar a idade e a renda que que Eu esperaria Aumentou a idade aumenta a chance de pagar se aumentar a renda aumenta a chance de pagar o cliente ser bom pagador tá é isso que eu tô medindo lá é uma regressão logística que eu faço nesse modelo de crédito tá um dos modelos Eu esperaria coeficiente Positivo na frente desses dois dias tanto da idade quanto da renda só que como eles estão altamente relacionados e eu estaria duas duas dois coeficientes positivos e com uma significância importante né pequenininha lá mostrando que aquilo ali é significativo são duas variáveis fortes e são de fato mas na hora que as duas entram juntas no modelo uma acaba matando a outra E aí pode Às vezes as

- *Corpus ID:* 2554
- *Score:* 0.8579596877098083
- *URL:* oculto
- *Início:* 00:16:10
- *Fim:* 00:18:10
- *Transcrição:* diferente Isso é um problema para os modelos de aprendizado de máquina e isso pode derivar por vários motivos por exemplo dois anotadores diferentes dois avaliadores que determinar a classe com viés diferentes uma experiência diferente então por exemplo um diz que a classe A outra B essa variabilidade entra notadores pode acontecer alguns domínios pode ser por uma mudança temporal de padrão aquela combinação de características agora era a mas depois sei lá um tempo passa a serber E se a gente não tem Esse aspecto temporal no dado a gente não consegue entender porque que mudou saber então esses são só dois motivos assim que eu tô citando como exemplo mais de uma forma geral você tem duas instâncias que são as minhas entradas com saídas diferentes isso é difícil para os modelos de aprendizado de máquina né porque eu tenho dois pontos que se sobrepõe no meu espaço de entrada e uma de uma classe outra de outra então não tem como eu dividir corretamente né a fronteira de decisão não tem como não então assim são instâncias que vão ser difíceis de resolver então na maioria desses casos a gente tenta olhar para essas instâncias e ver porque que elas estão com anotações conflitantes se a gente consegue entender né a gente pode pensar uma forma de tratar isso mas é do ponto de vista do aprendizado seria bastante difícil o modelo aprender a distinguir tá vamos adiante tem mais duas perguntas deles a minha é mais uma curiosidade ali a gente nós excluímos né O atributo porém ele foi guardado numa variável tem alguma razão específica para ele ser guardado na realidade poderia ser útil não nesse caso assim né mas alguns casos poderia ser útil de eu tentar rastrear isso aí por exemplo né Eu não quero usar para predição mas eu quero usar depois para determinar Olha esse ID aqui o custo de

- *Corpus ID:* 3702
- *Score:* 0.8567328453063965
- *URL:* oculto
- *Início:* 00:08:54
- *Fim:* 00:11:05
- *Transcrição:* elas são são mais descritivas ou funcionam melhor do que do que eventualmente para para outras não é a gente tem aqui uns exemplos de variação a gente já viu lá um exemplo de variação bem bem alta aqui até que elas estão razoavelmente também distribuídas dentro dos diferentes classes Mas enfim a ideia é que eu possa eventualmente avaliar se alguma dessas dessas variáveis não tendo uma distribuição normal E aí ao mesmo tendo E aí ela tendo uma média de um padrão muito altos não fica que ela não não consegue distinguir bem os elementos não é o ser eventualmente utilizado poderia ser excluída e também outra coisa que a gente pode pensar né uma análise de correlação que nós temos lá em cima antes fazendo uma avaliação se realmente a curva é normal algo que nós vamos retomar mas adiante você já devem ter estudado vai acontecer no caso real deu deu ter eventualmente 10 15 ou mais atributos né Tem situações aí de colegas que já estão na turma mais mais que começou antes né que já estão fazendo monografias que estão usando o conjunto de dados que tem 30 40 atributos né Aí fica fica uma coisa quase que não diga impossível mas talvez não faça sentido pelo tempo de processamento e também pela pela pela complexidade disso na ideia reduzir então a gente pode usar análise fatorial pode usar PCI ou pode simplesmente fazer uma análise correlação Eu tenho dois dois atributos altamente relacionados né eles crescem juntos funcionam muito bem juntos talvez eu possa usar um deles só não preciso usar os dois não é então também é um tipo de análise simples que me ajuda de alguma maneira identificar atributos né que que possam ser usados então aqui foi só uma análise simples para tentar entender o conjunto de dados e as


**Resposta gerada pelo LLM**: Variações no desempenho podem ser causadas por efeitos de permutação, ruído ou problemas em atributos que impactam o desempenho real. Dados ruins, metodologias de treinamento com viés, overfitting e vieses de amostragem também afetam a avaliação.  A discrepância entre avaliações de diferentes treinamentos pode indicar dados envenenados, modelos sensíveis aos dados de treinamento, ou poucos dados para treinamento. Mudanças temporais nos padrões e anotações conflitantes em instâncias também são fontes de variação.  A multicolinearidade, onde variáveis explicativas são altamente correlacionadas, prejudica a habilidade preditiva do modelo.  Variáveis com muitos valores faltantes podem ser inúteis no processo de modelagem.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address the sources of variation that can affect the evaluation of predictive models comprehensively. Supporting Evidence: The RESPONSE effectively identifies multiple sources of variation that can impact the evaluation of predictive models, such as noise, biased training methodologies, overfitting, sampling biases, and temporal changes. It also discusses the implications of poor data quality and multicollinearity, which are all relevant to the PROMPT's inquiry about sources of variation. The RESPONSE is comprehensive and directly addresses the question posed in the PROMPT.  Score: 3


---
