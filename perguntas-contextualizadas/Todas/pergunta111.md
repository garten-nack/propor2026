**Pergunta 111**: Quais são alguns exemplos de aplicações de clusterização de textos?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 3933
- *Score:* 0.8743017315864563
- *URL:* oculto
- *Início:* 00:12:36
- *Fim:* 00:14:50
- *Transcrição:* documentos por palavras que são comuns por tópicos que são comuns por entidades que são comuns por polaridade também né positiva negativa enfim sentimento tem várias várias aplicações para isso né para depois isso estando segmentado nós podemos ter uma compreensão boa uma visão Global sobre aquele produto serviço e assim por diante mas também pode ser para para agrupar os documentos da empresa para agrupar as páginas resultantes de uma busca os documentos de uma busca tem várias aplicações mas a ideia é essa eu não eu não sei nada sobre o conjunto de dados e eu vou agrupá-los e eh criando aí categorias que representam elementos que tem alguma relação E aí eh a base continua sendo a mesma mas para para para texos houve aí um um grupo de pesquisadores que que pesquisou durante muito tempo sobre isso e eles formularam essa hipótese aqui que serviria também para outros tipos de dados mas para textos ela ela está mais mais clara e bem definida né de que ao clusterizar documentos aqueles documentos que são similares que TM o mesmo assunto né eles vão permanecer no mesmo no mesmo cluster então essa é a hipótese ess esse é o nosso objetivo e aí indo paraa parte mais específica não é que tem a ver com o que Eu mencionei antes eh temos globais as etapas gerais do processo seguem seguem as mesmas não é eh eventualmente nós temos que ajustá-las né para para esse tipo de dados específico né então eu tenho uma etapa lá de limpeza eu tenho que fazer limpeza específica é que tipos de pergunta que tipos de operações de limpeza eu faria em textos não é que tipo de operações de seleção de de características né de atributos eu faria para textos como é que eu calculo a semelhança similaridade a proximidade entre textos e como é que eu identifico os grupos então com base nisso então

- *Corpus ID:* 8557
- *Score:* 0.8737077116966248
- *URL:* oculto
- *Início:* 01:01:27
- *Fim:* 01:04:00
- *Transcrição:* três tá junto com a dois Ah então elas são do mesmo assunto então eu nessa etapa agora eu vou tentar descobrir o que que são esses clusters então vamos entender o que cada grupo significa então para isso a gente Verifica o texto dos documentos fazendo o link né a gente volta lá para aquele texto original lembra a gente fez a limpeza bem pouquinha Tirou só links para gerar edens e agora a gente puxa esse texto de volta e o que que nós vamos fazer a gente vai buscar as palavras mais representativas desses textos que estão dentro de cada cluster E aí sim a gente vai aplicar pré-processamento mais agressivo para remover Stop Words eu já vou falar mais disso e nós vamos usar uma técnica que a gente já viu que é o TF IDF só nesse caso aqui é o c tfidf FCC é de cluster ela foi desenvolvida por esse Martin grutten Store que é o autor dessa desse Framework be Topic e a gente usa ela para eleger as palavras mais representativas de cada tópico Então o que o que que ele faz inicialmente esses documentos que tão num cluster eh eles são concatenados ele pega ele pega e forma um testão desse CL tá E aí com isso ele calcula o tfidf modificado que a gente já vai ver de cada termo ali dentro do CL por isso que a gente vai eliminar tows dali vocês vão ver que nós vamos tirar essas coisas que a gente não quer que ele calcule E aí eh aqueles eh eh aquelas aqueles termos eh que participam do os os documentos com maior CF aidf representam o CL Na verdade são os termos aqui tá que a gente vai chegar nos valores maiores eles representam o cluster tá muito assim falando vocês no Exercício vocês vão enxergar isso melhor professora Oi nessa concatenação aí tem algum limite de tamanho não não tem não tem olha eu acredito que vai demorar se for um um um cluster muito volumoso né com muitos dados Claro que tem o tempo de PR

- *Corpus ID:* 1120
- *Score:* 0.872541606426239
- *URL:* oculto
- *Início:* 00:10:17
- *Fim:* 00:12:28
- *Transcrição:* eles são contra o STF E aí então eles vão criando grupos de textos que são parecidos e jornais também tá eu tenho lá um texto que representa uma notícia e esses aqui podem ser textos que falam de saúde isso aqui são textos que falam sobre economia esse aqui são temas que falam sobre um artista esse aqui são são coisas que falam sobre uma série então posso pegar documentos e criar grupos de documentos que são parecidos por algum critério por exemplo E aí quando eu olho esses grupos eu disse Bah mas por que que eles botaram todos esses pontinhos aqui parecido quando eu vou analisar eu descubro ah é porque tudo isso aqui tá falando do Justin Bieber né Ah isso aqui tudo tá falando de refugiados Ah isso aqui tudo tá falando de sei lá debate sobre algum tema um outro exemplo que está colocado aqui é entender padrões diferentes de precipitações na Austrália Austrália é um país absolutamente enorme a gente tem regiões aqui no centro que são muito secas e tem regiões com muitas precipitações então um jeito é representado determinadas regiões por quantidades médias de chuvas por exemplo diárias ou mensais com volumes com mínimo com o máximo como não sei o quê E aí vai se encontrar talvez alguns agrupamentos onde perto da barreira dos Corais onde tem uma vegetação Tropical muito forte que chove muito isso aqui eu acho que a região de si né não tem certeza isso aqui é aquela região de Porto então cada uma delas são são cidades ou são pontos que foram medidos e que são parecidos né E aí quando eu vou interpretar e dizer assim olha Olha esses objetos eles estão parecidos e eu plot esses objetos no mapa dá para entender que aqui tem alguns parâmetros bem característicos enquanto o resto é meio espalhado então uma das principais aplicações que a gente tem aqui são compreender objetos

- *Corpus ID:* 4129
- *Score:* 0.8694181442260742
- *URL:* oculto
- *Início:* 00:10:40
- *Fim:* 00:13:00
- *Transcrição:* história e aí tem um cluster maior aqui que tem um pouco mais de divergência mas ainda assim dá para ver aqui o Malvado Favorito né ele coloca no sub cluster dentro de um cluster maior ele coloca que Pocahontas Mulan que são filmes de heróis não é os minions talvez sejam estejam mais estão mais deslocados talvez ficassem melhor junto com Malvado Favorito Mas enfim estão no mesmo cluster geral Global isso foi usando as palavras né E aí mais embaixo eu uso Lda que é uma técnica assim como PCI as outras que nós vimos que fazem essa transformação né então da mesma forma que lá que nós dizemos quantos componentes nós queremos usar né E aí ele gera esses componentes aqui embaixo eu só mostrei que esses componentes são são ditos então conceitos né então aqui tá o primeiro conceito e as palavras que são mais importantes para esse conceito ver o que que tem personagens de os desenhos o outro aqui também tem como de personagens enfim ele nesse caso aquele agrupou basicamente por personagens mas a ideia é que ele é grupo de dados eventualmente dar remédio etc agruparia que isso já tem testes mostrando eu gostaria por sintomas enfim e aí a ideia é usar esse Então essa Matriz nova né ao invés da Matriz original e aí eu faço aqui esse esse Justamente esse esquema de representação então agora ao invés de eu ter cada palavra presente em cada texto eu quero cada tópico e o quanto ele está presente ou não em cada texto é uma matriz correlacional então menos um e mais um seriam os limites né então próximo de um Tá super relacionado com aquele aquele documento e próximo de zero ele não tem relação nenhuma com documento nenhum ele tá no meio das correlações e menos uma correlação negativa e aí a ideia então aqui embaixo é aplicar a castelização

- *Corpus ID:* 4024
- *Score:* 0.8674805164337158
- *URL:* oculto
- *Início:* 01:32:17
- *Fim:* 01:34:53
- *Transcrição:* pegar aquelas palavras que são mais representativas daqueles cluster E aí você lista essas palavras ou faz um ordering só com essas palavras Esse é o mais é o mais correto é usar esse e eles brigando e aí complementando além do lsi também tem o lixo esse aqui é um outro tipo também de modelo né que consegue que consegue detectar os tópicos vou botar esse link ali na não mudam né então é uma maneira de eu entender o conteúdo né Ele é usado para isso ele foi criado para isso né Então tá aqui ó explicar grupos explicar porque que eles são similares né entender os tópicos de determinado cluster Então esse esse é o esse Essas são as técnicas Eu só não criei porque como eu disse Vocês vão na outra disciplina ver isso mas não me custa também eu vou eu vou preparar alguma coisa e bota um complementar para vocês para mostrar isso conseguindo adiante então aqui eu só só sigo passo passo processamento com uma aglomerativo e peço para calcular alguns score aqui o de silhueta mas só para ter o pipeline e é botei um desafio que não é obrigatório né então isso é uma biblioteca chamado Gutenberg não sei se vocês já viram já usaram não tem Mega biblioteca em Python que logo tem bag é um projeto se vocês entrarem nesse projeto ele ele foi criado para aqui tá a versão em Python dele aqui é a biblioteca que usa o projeto Gutenberg eu botemberg é um projeto de Biblioteca digital ele tem ver não tem 70 mil atualmente né 70 mil livros gratuitos que caíram em que não tem mais domínio né não tem mais passaram 50 anos e vocês podem ler tem vários formatos tem texto tem PDF tem várias opções e aí é um ótimo corpos né na verdade um corpo né

- *Corpus ID:* 8556
- *Score:* 0.8654274940490723
- *URL:* oculto
- *Início:* 00:59:46
- *Fim:* 01:02:02
- *Transcrição:* Bá nós vamos usar alguns algoritmos de cluster que esse tamanho de Vetor ele é grande então eu vou reduzir a dimensionalidade aí a gente viu que tem umap isni formas da gente diminuir isso para usar algoritmos mais complexos ali e aí nós falamos de algoritmos de clustering Então eu peguei diminuí esses vetores para sei lá n dimensões e agora eu vou usar esses vetores menores dentro do algoritmo de cluster e a gente viu aqueles tipos lá de algoritmos Beleza então cheguei num grupo de cluster minhas edens meus vetores estão agrupados Suponha que eu tenha três clusters né gerei três clusters ali eh que importam aí né Com algoritmo x lá agora eu preciso saber do que eles se tratam porque eu eu olho e digo tá eu entendi que tem uma figurinha lá juntando esses vetores Eles parecem até tá visualmente de repente consegui fazer clusters ali que eu até consigo enxergar em três dimensões ou duas que eles estão bem eh definidos mas o que que eles querem dizer então neste momento a gente vai pro que a gente chama de representação dos Tópicos então a gente agrupou em beding isso significa cada eding tá ligada a um documento né Ela é uma representação do documento então eu não eu não posso perder esse link então a imbed número um é o documento número um a dois é o documento número do a1 e a2 estão em cluster diferentes por exemplo Ah mas mas a três do documento três tá junto com a dois Ah então elas são do mesmo assunto então eu nessa etapa agora eu vou tentar descobrir o que que são esses clusters então vamos entender o que cada grupo significa então para isso a gente Verifica o texto dos documentos fazendo o link né a gente volta lá para aquele texto original lembra a gente fez a limpeza bem pouquinha Tirou só links para gerar edens e agora a gente puxa esse texto de volta

- *Corpus ID:* 8555
- *Score:* 0.8654197454452515
- *URL:* oculto
- *Início:* 00:58:03
- *Fim:* 01:00:18
- *Transcrição:* né Qual é a distância euclidiana aí tem várias aqui ó a distância do Cosseno que é o que a gente usa muito né para similaridade de texto mas alguns algoritmos não tem todas Então tem que olhar a biblioteca que vai usar já antecipo que o camins do C kit que é muito fácil de usar ele ele usa de distância Lidiana isso não quer dizer que não vai funcionar mas talvez se ele tivesse outras distâncias disponíveis como a do Cosseno Seria melhor eu já usei e ele dá bons resultados também e assim tem esse esse trabalho a professora Mariana sempre indica na nas disciplinas que ela dá que é um bom trabalho que fala sobre essas métricas descreve elas complexidade vantagens então é um bom trabalhinho de vocês guardarem assim quando precisa consultar uma métrica vocês vão lá que ele fala de muita coisa bom chegamos então terminamos o cluster temos os grupos formados os nossos vetores de embeddings que que a gente fez pegou o nosso texto cada cada linha do nosso texto é uma sentença um parágrafo aí a gente pegou esse texto e fez uma representação numérica dele isso a gente chamou de extração de eding porque a gente colocou ele dentro de um de um modelo sbert e saiu um vetor para cada um dos nossos textos representando os nossos textos a gente deu um um forward pass né nesses nesses vetores aí nesse modelo e e chegou nos vetores aí a gente viu que Bá nós vamos usar alguns algoritmos de cluster que esse tamanho de Vetor ele é grande então eu vou reduzir a dimensionalidade aí a gente viu que tem umap isni formas da gente diminuir isso para usar algoritmos mais complexos ali e aí nós falamos de algoritmos de clustering Então eu peguei diminuí esses vetores para sei lá n dimensões e agora eu vou usar esses vetores menores dentro do algoritmo de cluster e a gente viu aqueles tipos lá de

- *Corpus ID:* 8554
- *Score:* 0.864118218421936
- *URL:* oculto
- *Início:* 00:56:25
- *Fim:* 00:58:37
- *Transcrição:* algoritmo é muito usado para identificar outliers ou ruídos que são aquelas coisas que às vezes não deveriam nem estar ali entendeu que é uma um documento uma sentença que tá fora assim do é um exemplo é esse DB Scan e tem o hierarchical DB Scan que também tem uma questão hierárquica que a gente vai ver no no exemplo do ber top que ele usa como é o top hoje o algoritmo de cluster é o aid de bisca a gente vai ver mais dele depois então só lembrando a gente tá tá fazendo hard clustering Aqui estamos falando disso por quê Porque o hard cluster são as instâncias agrupadas de forma que cada uma pertence apenas um cluster o exercício que a gente vai fazer é assim de mas tem o também o soft clustering e a gente poderia adaptar isso onde os clusters podem se sobrepor que que significa isso são aquelas sentenças que tem um grau de associação com cada cluster lembra a gente falar de um um documento onde eu falo de esportes e também de cultura na mesma frase tá então elas poderiam estar participando de dois cluster mas aqui para fins de facilidade a gente vai pro hard cluster eu vou falar algumas abordagens de possível formas de fazer um soft clustering depois que vocês podem explorar mais e as as diferentes medidas porque no cluster é super importante tem o algoritmo etc mas é super importante a a medida de similaridade que que é a medida de distância que a gente vai usar né Qual é a distância euclidiana aí tem várias aqui ó a distância do Cosseno que é o que a gente usa muito né para similaridade de texto mas alguns algoritmos não tem todas Então tem que olhar a biblioteca que vai usar já antecipo que o camins do C kit que é muito fácil de usar ele ele usa de distância Lidiana isso não quer dizer que não vai funcionar mas talvez se ele tivesse outras distâncias disponíveis como a do Cosseno Seria melhor eu já

- *Corpus ID:* 4188
- *Score:* 0.8636558055877686
- *URL:* oculto
- *Início:* 00:31:44
- *Fim:* 00:33:57
- *Transcrição:* segregar em em supervisionada não supervisionada e sim dar um Panorama geral então imaginar se já tinham visto alguma coisa e já que a disciplina de de cluster clusterização eu eu eu coloquei o link aqui de um artigo para vocês darem uma olhada que ele usa clusterização e regras de assução de maneira combinada tá então ele ele ele mostra como vocês eu já esse aqui é interessante eh porque ele se eu não me engano mostra como usar depois da clusterização então A ideia é clusterizar os os produtos e depois aplicar regras que indicam né O que que esses produtos do mesmo cluster tem comum né então seria a ideia criar cluster de produtos para bancos de dados muito grandes né E aí eh poder ter um subset que fica mais fácil tanto de processar quanto D regras que são mais mais eh ajustadas né para aquele subconjunto Mas vocês podem extrapolar isso né extrapolar isso e fazer tentar usar isso para tentar entender o cluster já que ele mostra o que que tá associado com o que a ideia é que pegar um conjunto de documentos né um clusterização por documentos Então tem um cluster de documentos ele vai poderia usar isso pensando que os produtos seriam as palavras né Então as palavras que mais se associam dentro de um cluster seriam colocadas n mesmas regras e assim eu consigo entender né Eh Quais são as palavras que estão associadas Isso daria uma noção de qual é a temática daquele daquele cluster então dá dá para fazer isso e mas tem outros artigos também que eles fazem do contrário eles primeiro aplicam as regras de associação com o intuito de identificar esses esses produtos que são vendidos né em cestas comuns e depois claster zam isso né para para tentar eh achar conjuntos de regras que TM produtos parecidos e isso isso ajuda nessa nessa na configuração desses algoritmos de recomendação né para para pegar produtos já que estão rizadas as

- *Corpus ID:* 4127
- *Score:* 0.8627505302429199
- *URL:* oculto
- *Início:* 00:07:19
- *Fim:* 00:09:31
- *Transcrição:* que ela é uma técnica de redução de dimensões Assim como nós vimos na aula passada o uso de PCA nos jornais fatorial né como dimensões alternativas as dimensões originais também uma vez que eu detecte tópicos E ela como se fosse uma técnica de imbedem né Para quem para quem já viu e depois seus homens mais adiante com isso eu já mencionei né tem essas técnicas mais atuais né é uma das primeiras técnicas de imbede em que existe eles não chamavam de imbedem na época Mas o que ela faz é isso ela ela Analisa as relações entre as palavras e ver as palavras que estão mais relacionadas E aí cria e cria esses conjuntos esses classes de palavras que possuem alguma relação E aí é isso o pessoal não mente da o nome de tópico e eles fizeram vários vários experimentos e viram que não deixam de representar tópicos num conjunto de documentos e aí então vocês teriam os tópicos que são mais salientes no conjunto de dados e a ideia a opção seria vocês alguns ao invés de usarem as palavras originais do vocabulário vocês usassem esses tópicos esse conjunto de tópicos que foi detectado E aí não só vocês vão ter uma uma representação que semanticamente é mais rica mas também vão ter uma quantidade dimensões menor para processar e a alternativa não dois que eu coloquei aqui também é Vocês poderiam clasterizar os documentos da maneira tradicional e ao final vocês para cada cluster que vocês geraram vocês aplicariam Lda desculpem Essa svd é para vocês terem né identificados Quais são os tópicos salientes dentro daquele subconjunto de documentos Isso daria uma noção então dos assuntos que estão sendo mencionados dentro dele dentro deles no caso e aí eu preparei Então essa aula complementar coloquei lá no mundo onde vocês tem um exemplo aqui que carrega alguns textos que eu que eu disponibilizei no


**Resposta gerada pelo LLM**: Agrupar documentos de uma empresa, agrupar páginas resultantes de uma busca, compreender objetos, entender padrões diferentes de precipitações, entender a temática de um conjunto de documentos e criar grupos de textos parecidos (ex: notícias sobre saúde, economia, artistas, séries).

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should provide relevant examples of text clustering applications. Supporting Evidence: The RESPONSE provides multiple relevant examples of text clustering applications, such as grouping company documents, search result pages, and thematic understanding of document sets. Each example directly relates to the concept of text clustering, addressing the PROMPT comprehensively. Therefore, it is relevant to the entire PROMPT.  Score: 3


---
