**Pergunta 134**: Quais são os níveis de granularidade na análise de sentimento?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8135
- *Score:* 0.8605138063430786
- *URL:* oculto
- *Início:* 01:03:23
- *Fim:* 01:05:45
- *Transcrição:* Então posso fazer em três classes eu posso fazer em cinco classes se eu quiser para mapear pro pro para aquela classificação estrelas que é que é bastante utilizada então muito positivo positivo neutro negativo muito negativo mas além de polaridade também a classificação de emoção eu posso querer dizer qual qual a emoção que esse texto expressa expressa alegria tristeza surpresa raiva e aí quando a gente tem mais de uma classe mais de duas classes aqui a gente chama de multiclasse e aqui na classificação de emoção também entra o problema multi rótulo quer dizer uma um mesmo texto pode ter alegria e surpresa juntos ou pode ter surpresa e e tristeza juntos então pode ter mais de um rótulo associado a cada a cada Instância no caso a gente chama de multirent níveis de granularidade pode ser ao nível da review inteira de um parágrafo da review de uma sentença ou numa granularidade mais fina que é o aspecto então eu posso avaliar ó essa pessoa comprou aqui esse esse equipamento esse celular e e achou que a câmera é muito boa mas o celular é muito grande então ela tá avaliando aspectos diferentes bom ah algumas abordagens para fazer isso uma abordagem assim bem antiga e pouco usada quer dizer até não é tão pouco usada usar um léxico pré-construído o que que é um léxico então é um recurso que eu vou ter uma série de palavras e a polaridade associada ao sentimento associado então por exemplo a palavra Nice é associada a um sentimento positivo a palavra ugly é associado a um sentimento negativo E aí eh pode ser tanto categórico aqui como pode ser numérico então se eu tenho um recurso como esse eu posso simplesmente eh aplicar consulta a tabela é é o meu algoritmo de de classificação de sentimento mas não é isso que a gente vai fazer o que a gente vai fazer essa

- *Corpus ID:* 1086
- *Score:* 0.8495532274246216
- *URL:* oculto
- *Início:* 00:47:09
- *Fim:* 00:49:22
- *Transcrição:* Ah é um problema de análise sentimentos é um problema de detecção de posicionamento tá aqui um monte de dados de treino faça o modelo que vocês quiserem a gente vai testar o melhor modelo separando um conjunto de dados aqui que vocês nunca viram na vida e aí a gente vai dizer quem é que vai ganhar a competição numa empresa pode ser justamente um critério de aceitação ou um benchmarking para você saber em quão bom é um modelo de vocês tá bom uma outra técnica que se usa bastante é chamada técnica de roubouch mas aí ele tem que ficar claro sempre tem um de treino sempre tem um de teste a de Round eu pego os meus dados como um todo e eu reservo uma percentagem deles para testar recomendada razões típicas aí vai depender da quantidade de dados que eu tenho tá mas tipicamente é 7030 ou 80 20 Tá mas não menos do que isso senão a gente mas assim a quantidade de dados sobre o qual a gente testa é muito pequeno e portanto a nossa capacidade garantir que a generalização é boa é limitada bom é de novo importante que a gente preserva as propriedades dos dados nos dois conjuntos porque eu não posso fazer uma divisão aleatória eu entrego dados para treinar onde né tem por acaso toda uma característica temporal por acaso né E vai testar a circunstância completamente diferente se tem para vocês alguns exemplos na nossa disciplina é não adianta eu pegar os dados das eleições da Dilma lá em quando ela ganhou a primeira eleição que tava todo mundo né Muito feliz com o governo e etc e tal e feliz que era a primeira mulher que tava concorrendo a presidência é assim por diante e testar né a capacidade desse modelo na de uma oito anos depois onde ela tava com né todo um problema de implicação os termos que são usados as fichas que foram usadas para dizer esse sentimento

- *Corpus ID:* 8421
- *Score:* 0.8469051122665405
- *URL:* oculto
- *Início:* 01:23:27
- *Fim:* 01:25:58
- *Transcrição:* fica bem livre né não precisa ser um um dataset anotado normalmente qualquer texto que você tiver né um conjunto de e-mails um conjunto de reclamações Você já consegue trabalhar né É mas para pro trabalho a gente precisa avaliar Então como é que como é que tu faria a avaliação por isso que aí precisa do dataset anotado da coleção de teste pod cal da modelagem de tópicos entendeu A modelagem de tópicos é um caso a parte cling é um caso a parte mas as as outras as as tarefas em que que a gente precisa calcular uma métrica que tem que ser baseada num num a gente chama de Ground truth que precisa ser precisa do resultado esperado tipo um gabarito preciso do gabarito para poder avaliar análise sentimento classificação e Recuperação de informação também mas existem coleções de teste que que podem ser usadas então tem a do Glasgow herold do La times em português tem essa da Folha de São Paulo é porque no caso o que vocês querem é que no final a gente faça um tipo de métrica mostrando o que isso o a técnica que a gente aplicou teve tanto por cento de acerto isso isso o teu trabalho é análise de sentimento né Carlos é como eu tá dizendo agora eu acho que talvez eu mude né Eu gostei dear esse tópico de hoje tá se tu quiser uma coleção em português tu me avisa ali me manda uma mensagem no no teams que eu que eu te encaminho a coleção do da Folha de São Paulo pronto tá ótimo obrigado que pode ser usada é que assim a a não ter um Data Set anotado uma coleção de teste daí implica muito trabalho braçal e que a gente não não precisa ter nesse nesse momento Aí sim o trabalho Fica dá muito trabalho bom dia professora aqui Bom dia Antônio alísio falando eh na nossa proposta né Nós eh é aquela proposta de fazer tradução usando um um modelo generativo né llm

- *Corpus ID:* 8491
- *Score:* 0.8459769487380981
- *URL:* oculto
- *Início:* 00:08:42
- *Fim:* 00:11:02
- *Transcrição:* exemplo simples o as instruções é classificar se o texto é neutro negativo e positivo ele não tá dando essa parte do contexto não tá dando que ele poderia explicar olha um texto neutro é aquele que blá blá blá e O negativo é aquele e ainda nesse contexto passar exemplo tá então aqui não foi necessário porque is é uma tarefa trivial e o texto é the food was aí o sentimento e ele espera então a resposta do modelo aqui por exemplo ó usando o sentimento neutro a sumarização a mesma coisa só que o contexto é justamente o documento que a gente quer sumarizar e a instrução é olha aqui explique o texto acima em uma sentença e aqui do modelo Então esse contexto é o que eu falei lá que no GPT 3.5 tinha 4000 tokens Então eu tinha que administrar fazer formas ali para administrar documentos maiores eu tinha que dividir Então hoje a gente vai ver que isso aí aumentou muito e aí tem estratégias que ficaram famosas assim porque o modelo erra e nós vamos ver até nos exemplos que a gente tem que o modelo rra mesmo GPT 4 então tem essa essa aqui que ficou bem famosa que é Chain of que é uma cadeia de pensamento né E ela seria assim a gente dar uma explicar pro modelo um um raciocínio mais complexo para que ele tenha mais sucesso na resposta então aqui ó ele explicava que o Roger tinha cinco bolas de tênis ele comprava duas dois pacotes mais de bola de tênis e aí né então cada ptin mais TRS então quando tinha E aí ele dava a resposta 11 aí ele fazia uma outra pergunta com um cálculo aqui ele errou aqui então aqui no Chain of Talk o que que ele faz ele diz na resposta dele em vez de ele falar direto que é 11 Ele explica como ele chegou primeiro o Roger começou com cinco bolas Depois tinha dois pacotes de com três bolas cada e eh Então são seis bolas de tênis então 5 + 6 são 11 então mediante essa essa a explicação de como

- *Corpus ID:* 974
- *Score:* 0.8441150784492493
- *URL:* oculto
- *Início:* 01:29:19
- *Fim:* 01:31:37
- *Transcrição:* sentimento para mim ele é o melhor candidato o sentimento tá aqui tem sentimento só que eu tenho que conseguir relacionar que ele é o Russomano então que a gente chama de correr referência eu nunca votaria no candidato do PT eu tenho que saber que o candidato do PT naquele momento era sei lá de uma ou o Haddad então este é um erro a gente dividiu em sentenças a gente não tinha condições de tratar isso aqui a gente sabe que a gente perdeu uma série de sentenças que tem sentimento e muita coisa que eu não vou falar aqui para vocês tá que vocês não terminava você tentaram negar tentando fazer isso tentando fazer um monte de coisa mas para vocês verem o trabalho que dá e muitos desses problemas gostaria de notar a gente não tô aqui vendo se o nosso classificador funcionava o que nos levou voltar para cá e tomar novas decisões análise de sentimento gente tem duas abordagens basicamente eu faço todo esse processamento né E aí eu tenho duas grandes opções uma se chama baseada em dicionário tem dicionários que dizem lá essa palavra positiva essa palavra é negativo gostei ou positivo odiei negativo se chama Lexus de sentimento e a outra coisa é passar por uma aprendizado de máquina mas para isso tem que ter dados que tem rótulo E aí então Quais são os problemas bom baseado em dicionário tem que ter um dicionário Ela é super simples não vai custar nada então vamos de dicionário porque se eu fizer aprendizado de máquina eu vou precisar anotar várias das minhas frases por três anotadores eu vou ter que pegar uma frase dizendo Russomanno é o melhor cara do mundo e esse cara vai ter que dizer essa frase é positiva é Serra é um engodo ele vai ter que dizer que isso aqui é negativo isso aqui é um processo Custoso existe 85% em média de consenso por isso que não pode ser só um anotador

- *Corpus ID:* 7422
- *Score:* 0.8425230383872986
- *URL:* oculto
- *Início:* 00:32:55
- *Fim:* 00:34:57
- *Transcrição:* esses caras são horizontais ou verticais não sei sei depende do que a gente assume como referência né mas eles têm naturalmente uma orientação eh bem Evidente né uma orientação é bem né o formato do texel é bem diferente numa orientação do que na outra aqui a mesma coisa né a gente tem uma um Riscado né Eh isso aqui difere desse cara por granularidade é mais fino né aqui a gente tem detalhes mais finos do que aqui eh eventualmente a orientação também tá diferente independente do que a gente assumiu como orientação para cá o DK é o inverso né então esses dois padrões aqui eles diferem né Por granularidade talvez eu pudesse comparar esse cara esse primeiro aqui com esse quarto né então são dois padrões que TM granularidade diferente os dois parecem não ter uma orientação Clara os dois parecem ter mais ou menos o mesmo formato tá E por aí vai certo então a gente poderia descrever regiões da minha imagem através dessas desses critérios aqui né de determinação de de componentes de texels tá hã E mais uma vez isso aqui de uma forma ou de outra é usado nos algoritmos de learning também né Eh a questão é que essa coisa agora é aprendida né então a gente tem eh algum tipo de filtro convolucional por exemplo né e o meu f filro convolucional ele vai ter uma resposta horizontal para esse cara aqui uma resposta vertical para esse cara aqui né E quando eu faço uma imagem com essa outra cara aqui ele diz olha a resposta que eu tinha para cá é totalmente diferente a resposta que eu tenho para essa né então lá pelas tantas eu consigo especializar algum tipo de filtro para construir uma representação de um tipo de textura especializar outro tipo de filtro para construir uma representação

- *Corpus ID:* 3689
- *Score:* 0.8422900438308716
- *URL:* oculto
- *Início:* 01:11:24
- *Fim:* 01:13:54
- *Transcrição:* acima dela né isso eventualmente não acontece nos outros né dos outros até considerar é que o três ele tem quatro ele tem um pouco disso né tem todo câncer zero acima que tem metade de elementos acima que tem terço metade então daria por essa por essa considerar essas opções todas né já que não Porque aqui tenho que sair dele esse aqui poderia se fosse considerar essa essa euística então a minha ideia minha sugestão é considerem diferentes fatores isso vai ajudar vocês combido silhueta ou SSD você chegarem num consenso e ele serve também para vocês verem isso ele é bom olha só eu tenho eu tenho alguns elementos aqui que talvez estejam mal classificadas eu posso ir a fundo olhar esses elementos e trazer para outro cluster né Se eles forem mal ou não estiverem bem classificados né então é uma análise que é um pouco subjetiva digamos assim mas usando essas relíticas você tem aí condições de poderem escolher não é uma escolha definitiva mas uma pelo menos reduzirem digamos assim um espaço de análise né para dois três quatro possíveis né quantidades E aí fazerem a classificação mais detalhada dessas que foram sugeridas por essas técnicas E aí tomarem uma decisão que vai depender então mais Doutor quando isso for muito muito já é muitas possibilidades né então tomar uma decisão junto com o analista de negócio específico Bruno Bruno tem uma bom dia bom dia Leandro eu queria saber se esses elementos que estão localizados à esquerda aí na verdade das classes se eles não poderiam ser considerados outlanders não necessariamente são outlares né porque aqui o que ele mede é o quanto o pão próximo ele tá do do digamos assim daquele Clã do centro daquele clã então é verdade silhueta significa se ele tá próximo de um significa que ele é

- *Corpus ID:* 1018
- *Score:* 0.8406428694725037
- *URL:* oculto
- *Início:* 00:14:24
- *Fim:* 00:16:50
- *Transcrição:* para fazer uma mineração nesses dados que dados né é a quantidade de estrelas a data que aquela avaliação foi feita o texto da avaliação se os outros usuários entendem que aquela avaliação Foi útil ou não né E se há alguma resposta oficial inclusive né do publicador daquele aplicativo então fazendo uma mineração desses dados jogando isso dentro de um data freio é possível fazer uma análise sentimento e verificar em que em que estado o usuário está perante aquele aquele aplicativo né se é um sentimento positivo se é um sentimento Negativo você tá ali mais no meio do caminho e também dá para avaliar se esse sentimento mudou não no decorrer do tempo porque porque os comentários aí são todos datados então dá para você pegar intervalos de tempo e ver se naquele intervalo de tempo o sentimento foi mais positivo e a partir de uma determinada data o sentimento passou a ser menos Positivo né E aí dá para fazer associação dessa mudança de sentimento de positivo negativo com as críticos lançadas ou seja ah Elas têm uma fia o sentimento mudou né para mais ou para menos então a gente entende que tem muita informação são né são 4,15 milhões de avaliações dá para fazer sim uma extração né eu vou parar aqui o grupo continua tá e mas eu acho que dá para entender o ponto né então assim É uma grande oportunidade que esse grupo aqui destacou então novamente parabéns né a relevância também destacada eu vou confessar que eu fiquei bem triste porque eu adoro o sologov chocada não só que alguns de algumas notas baixas mas com a quantidade de gente que achou que eu o comentário Foi útil fiquei bem eu fiquei triste imagina vocês né enfim o setor aí responsável porque eu acho que mudou muito a nossa

- *Corpus ID:* 8052
- *Score:* 0.8404160737991333
- *URL:* oculto
- *Início:* 00:14:09
- *Fim:* 00:16:23
- *Transcrição:* pode ser simplesmente uma classificação binária então positivo ou negativo pode ser uma classificação ternária positivo negativo e neutro ou eu posso colocar em cinco níveis muito positivo positivo neutro eh negativo e muito negativo ou eu posso em vez de querer a avaliar o ou seja se é positivo ou negativo eu vou querer analisar a emoção Então esse texto aqui a pessoa que escreveu tava com raiva ou esse texto aqui a pessoa tá demonstrando uma surpresa então existem categorias também de de emoção que tão que tão associadas aqui dentro também existe a tarefa da gente identificar o o alvo da opinião né ou o aspecto então alguém comprou um notebook e disse que ah o processador é ótimo mas o o computador é muito pesado Então tá avaliando ah o o processador que é um aspecto tá avaliando o peso que é que é outro aspecto e podem ser algumas positivas algumas negativas Então essa é uma área bem abrangente a gente vai usar aqui exemplos dessa dessa área nas próximas aulas detecção de spam então a gente recebe um monte de e-mail como é que faz para detectar se é spam ou não é spam de maneira automática é uma tarefa de classificação eh detecção de plágio Então eu tenho dois textos como é que eu como é que eu decido se eles têm uma similaridade que é assim acima do do normal detecção de hate Speech ou de discurso de ódio qu as redes sociais a gente vê a disseminação desse desse problema né Eh cada vez mais eh impactante então a gente vê demonstrações de racismo sexismo homofobia xenofobia como é que a gente faz para identificar eh postagens que tem esse conteúdo também é uma tarefa de classificação reconhecimento de entidades nomeadas que é uma tarefa de rotulação de sequências então eu recebo uma sequência de texto e aí eu vou marcar esse daqui é um substantivo Oh desculpa esse aqui é o é é o nome de

- *Corpus ID:* 8635
- *Score:* 0.8396885991096497
- *URL:* oculto
- *Início:* 00:23:29
- *Fim:* 00:25:42
- *Transcrição:* eh situações entre o n grans com n = 1 até 4 tá e depois ele multiplica todas essas e aplica uma punição sobre se a geração de texto foi muito curta em relação ao texto esperado né então esse é um indicador que é bastante utilizado para esse tipo de função ele aparece eh em vários artigos aí como um indicador básico e eh o código que eu adaptei para fazer a avaliação em cima dos resultados produz e um valor que é relativamente baixo né Ele é 0.17 mais ou menos né Lembrando que assim de cá a gente pode achar Ah só uma coisa tá ele é entre zero e um e quando é perto de um significa que o texto é mais perto da exatidão e perto de zero é mais distante então o que que acontece a gente pensa Ah é muito ruim né mas assim o padrão Aí que aparece nos artigos é o seguinte não não se obtém algo tão alto assim geralmente tá ele é um pouco mais baixo do que o que se diz aí pelos artigos mas eh não a gente não pode entender que é completamente descartável né vamos dizer assim por ser muito baixo eu coloco então aqui só algum para para exemplificar uma expressão visual dos resultados do que que é obtido e só pra gente ter uma ideia né de O que significa esse indicador né nesse nesse nível então eu coloquei aqui alguns casos a imagem do lado esquerdo com o texto impresso em cima da imagem é o resultado do modelo na aplicação desse outro dataset e esse outro dataset eu não falei antes mas ele também tinha as marcações disponíveis de maneira que eu ele é comparável né com o que a gente obteve então aqui por exemplo a gente percebe que ele errou um pouco né ele entende que é um homem e não uma criança ali e ele fala de celular então assim não é exatamente o que ele tinha na imagem né e se na hora que você vai ver o texto marcado aí já tem é bem diferente aqui por exemplo ele fala de eh escovar os dentes né não tem nada a ver né mas assim ele identifica


**Resposta gerada pelo LLM**: Os níveis de granularidade na análise de sentimento podem ser: nível da review inteira, de um parágrafo da review, de uma sentença, ou em um nível mais fino, o aspecto.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. Supporting Evidence: The RESPONSE directly addresses the PROMPT by listing the different levels of granularity in sentiment analysis, which includes the review as a whole, paragraphs, sentences, and aspects. This comprehensive answer covers all parts of the PROMPT, providing a clear and relevant explanation of the topic.  Score: 3


---
