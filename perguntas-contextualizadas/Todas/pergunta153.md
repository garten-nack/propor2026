**Pergunta 153**: O que é a curva ROC e como ela é utilizada para avaliar o desempenho de um modelo?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 7826
- *Score:* 0.8463444113731384
- *URL:* oculto
- *Início:* 01:18:39
- *Fim:* 01:21:14
- *Transcrição:* revocação ã para dar um valor numérico único né então a curá ela meio que faz isso também mas ela tem um certos problemas tá para problemas multiclasse a precisão e revocação dão uma ideia boa do qu método de quão bem o método foi para cada uma das classes independentemente e o F1 score faz isso agrupando tudo num valor só tá outra forma de avaliar esses classificadores é usando a tal da Rock Curve tá que mostra eh a taxa de verdadeiros positivos versus a taxa de falsos positivos né que é afinal das contas a especificidade e a eh sensibilidade que é o Recall e e a precisão também tá hã basicamente a Rock ela considera essa esses valores de de especificidade e sensibilidade para diferentes taxas de aceitação tá então o que que eu quero dizer com essas taxas de aceitação se a gente voltar lá no nosso Exemplo né de rede eu tenho tem um score para cada ã para cada saída né para cada possível classe tá outra possibilidade uma possibilidade é pega o maior né outra possibilidade é se for maior do que tanto eu considero como tal classe né então esse se for maior do que tanto esse tanto é justamente o que a gente varia aqui nessa construção da da Rock Curve tá e ao invés de plotar essa rock aqui a gente poderia também eh calcular a área sob a curva né E quanto mais próxima de um essa área melhor tá então eh é um outro indicativo aí de de de desempenho do modelo tá por um problema de classificação binária né uma rock que tem a área 05 aqui ela é é um classificador aleatório né um classificador que chuta assumindo que o dataset tá balanceado né então se meu dataset tá balanceado e o meu classificador chuta qualquer coisa né ele tem 50% de Chan de acertar tá então

- *Corpus ID:* 1454
- *Score:* 0.8460201621055603
- *URL:* oculto
- *Início:* 00:31:39
- *Fim:* 00:33:40
- *Transcrição:* preparamento e avaliação do meu modelo com base em um só conjunto eu posso tá dizendo Olha esse modelo sai super bem sei lá 96% de chance né de desculpa de acerto mas na verdade é foi um processo de avaliação que favoreceu de alguma forma não intencional o meu modelo E aí quando eu aplico esse modelo eu vejo que nunca na taxa de acerto se eu pegassem se eu aplicar esse modelo ficar monitorando ele ou seja algumas predições dele né eu fazer uma amostragem e comparar com predições de especialista ou uma outra forma de rotular os dados eu consigo ver que ele nunca chega Nessa taxa de acerto né é normalmente Esse é o risco que a gente corre da gente estar estimando um desempenho que não vai se concretizar ok Então eu tinha perguntado se já era modelos melhores né Eu acho que já era modelos mais confiáveis né assim mais confiáveis mais robustos eu acho que é isso sentido tá porque talvez por exemplo com rodagem duas vias a meu desempenho lá seja uma taxa de acertos de 96% com rodagem por cento tá bom vamos para acontecer isso mas eu confio mais um 92% embora o desempenho do ponto de vista né valor absoluto ali ele seja mais alto no Rosal de todos os dias então assim na prática a gente vai usar o de três vezes a gente isso mas tem uma diferença que aí é o ponto para saber a gente normalmente vai usar o de três vias porque assim a gente vai sempre lidar com algoritmos que tem um outro e preparando né para serem configurados ou a gente vai pelo menos tem que decidir qual o algoritmo que a gente usa então na semana que vem a gente vai vendo na vez Então a gente poderia se deparar com duas perguntas dois algoritmos se ninguém conhece mais nenhum né usa o knivete para esse problema e aí quando

- *Corpus ID:* 2290
- *Score:* 0.8442206978797913
- *URL:* oculto
- *Início:* 00:22:44
- *Fim:* 00:24:49
- *Transcrição:* inclusive ver que eventualmente o trecho de 0.5 que é o que a gente usa por padrão não é o ideal para o meu problema de repente eu tenho que ajustar um pouquinho esse trecho que eu vou usar para determinar qual vai ser a classe predita positiva ou negativa então a curva rock ela é focada em problemas binários depois que eu tenho essa curva o que acontece é que essa curva pode ser sinalizada também com uma métrica que é a auto escola né ou aoq rock né para ser mais específico em relação a curva rock então é a mesma ideia da curva de precisão eu consigo quantificar essa área sobre a curva quanto mais próximo de um quer dizer que digamos assim para todas as predições eu fui só elevando a taxa de falso positivo de verdadeiros positivos desculpem sem nunca elevar a taxa de falso positivo então basicamente eu teria um l invertido aqui então eu teria um score máximo de um se eu tenho 0.5 é um classificador aleatório ou seja tudo sobre essa diagonal como eu comentei é que eu tenho a taxa de falso positivo igual a de verdadeiros positivos então para cada verdadeiro positivo que o modelo retorna ele retorna falso positivo também então é um modelo que a gente não pode confiar na sua produção então é interessante que aqui eu tô comparando né é a curva aqui de rock e a curva precisamos Abrir Qual a direita para um mesmo conjunto de dados né para o modelo de agressão Logístico mesmo conjunto de dados é assumindo dados balanceados tá aqui por exemplo tenho duas classes na proporção 50% 50% certo agora interessante a gente perceber uma propriedade quando eu tenho aqui dados desbalanceados ou seja tenho uma classe que representa a maior parte dos meus dados e essa classe né é que representa a maior parte dos meus atos

- *Corpus ID:* 1440
- *Score:* 0.8431347012519836
- *URL:* oculto
- *Início:* 00:09:28
- *Fim:* 00:11:25
- *Transcrição:* Então essa pergunta que a gente quer se fazer o quão boas né são essas saídas esses rótulos preditos ou esses valores preditos desse modelo que a gente tá desenvolvendo tá então a gente vai justamente tentar responder essa pergunta aqui certo e claro tem algumas questões importantes a gente vai falar obviamente a ideia de qualidade como mensurar a qualidade como mensurar desempenho mas tem um ponto muito importante como é que eu vou ter exemplos desconhecidos ou seja como é que eu vou gerar esses exemplos que para mim para o meu modelo estão desconhecidos mas eu de fato sem a saída e eu uso essa saída Eu escondo essa saída e uso ela no momento somente da avaliação Para ser capaz de mensurar o Comper aquela pressão do modelo tá dos resultados reais então quando tem esse normalmente esse único conjunto de dados disponível esses exemplos desconhecidos é como se tivesse determido um modelo que a partir de agora vai fazer as predições esses exemplos desconhecidos é o que começa a ser coletado a partir de agora mas eu não posso depender deles né para avaliar o meu modelo porque o meu modelo antes de ser implementado ele tem que ser avaliado para ter certeza que de fato ele vai ser bom então a gente faz isso no momento de desenvolvimento do modelo com a base de dados que a gente tem que realmente é uma base única que varia de tamanho pode ter centenas de instâncias pode ter milhares milhões de instâncias depende do domínio Então qual é uma abordagem bem simples tá essa abordagem de rolda alto que a gente chama em português entendeu mas é extremamente comum vocês usarem o termorroida alto mesmo tá esse termo a gente usa muito em português A ideia é o seguinte da minha base de dados aqui de exemplos conhecidos o que eu vou fazer é o seguinte eu vou separar aleatoriamente o

- *Corpus ID:* 635
- *Score:* 0.8415841460227966
- *URL:* oculto
- *Início:* 00:30:44
- *Fim:* 00:32:52
- *Transcrição:* de verificar se isso é problema tá Deu para entender gente a gente não vai chegar a olhar a validação aqui tá então na prática o mais comum é o primeiro né a regressão da primeira mostra e testa a previsão na segunda correto ao invés de eu aplicar o modelo nas duas amostras isso E aí por exemplo eu posso lá medir o meu R2 eu tenho R2 da primeira né que sai ali no meu modelo e depois que eu fiz a previsão nos dados dos 20% eu tenho observado esses 20% tem que ter variável resposta de desfecho né eu tenho que ter meu Y tá eu tenho que saber se meu cliente pagou ou não pagou E aí eu vou comparar aqui no caso e pagou para não pagou a aula da aula que vem né Porque é logística mas pensem ali que eu tenho alguns dados da academia guardados onde eu tenho o meu Y que é o valor total de gasto eu vou passar nesse modelo que eu criei com a primeira amostra nesta amostra que o modelo não conhece ainda então eu tenho Y observado e y estimado E com isso eu tenho posso fazer tudo que eu quiser eu tenho resido eu posso calcular R2 eu posso calcular aqueles resíduos ao quadrado que assuntos ao quadrado que a outra métrica eu posso calcular isso eu posso calcular tudo e aí eu vou comparar isso que eu calculei nessa amostra do 20% com tudo que eu calculei lá na hora de construir o modelo dos 80% e tem outras métricas tá gente uma que o colega falou que é curva rock é utilizado quando a minha variável lá binária tá na regressão logística por exemplo modelo de sobrevivência também usa curva rock tá então é uma outra métrica de qualidade do modelo quais o que você pode usar para testar a previsão na segunda que que eu posso usar É porque por exemplo na primeira mostra com os 80%, eu fiz a análise aí eu quero fazer essa ligação eu criei equação aí eu vou pegar essa equação e vou passar na segunda

- *Corpus ID:* 1444
- *Score:* 0.8407974243164062
- *URL:* oculto
- *Início:* 00:15:31
- *Fim:* 00:17:58
- *Transcrição:* porque eu quero ver o quão bem o meu modelo tá saindo então eu tenho que ser capaz de comparar a saída acredita com a saída esperada E para isso né existe tem que ter alguma saída esperada que é esse dado ao outro lado Bom vamos lá então como é que a gente então só para enfatizar bem espaço a passo Tá eu vou supor aqui uma divisão 75,25 Tá então a gente pega os dados a gente divide eles aleatoriamente entre treino e teste por exemplo 75 para treino 25% para teste tá a primeira etapa isso aqui vocês vão ver que tem uma função que a gente usa para fazer isso tá não sei que plana a gente não precisa fazer manualmente esse conjunto de Treinamento então ele é usado para gerar o modelo tá então o que vocês podem ver que eu usei somente o conjunto de Treinamento gera um modelo com base nisso Então na verdade esse modelo ele só conhece 75% dos meus dados então eu uso esse modelo para fazer uma predição sobre o conjunto de testes pessoal Desculpem a garganta ela realmente não tá ajudando muito hoje então vou usar esse esse modelo e esse conjunto de teste justamente para ver para aplicar esse curso de teste ao meu modelo esse modelo vai me retornar uma predição Eu tenho um atributo ao E aí eu posso comparar o rótulo com a produção faz uma Estimativa de desempenho certo alguma dúvida aqui tudo certo Tá talvez isso aqui seja para quem já trabalha ou você já passaram né bastante básico Tá mas é tem um ponto importante tá para comentar isso é um detalhe depois que eu avaliei esse meu modelo o desempenho do meu modelo por exemplo ele tem uma taxa de acerto de 96%, tá ótimo para o meu domínio Esse é ótimo depende muito domínio né do que a gente conhece sobre os dados mas vamos supor uma taxa certa

- *Corpus ID:* 2790
- *Score:* 0.840611457824707
- *URL:* oculto
- *Início:* 00:23:20
- *Fim:* 00:25:33
- *Transcrição:* Carol já colocou depois de seleção Unidos passa pelo robux Killer depois a gente tem aplicação de PCA para redução da dimensionalidade é interessante é o seguinte nesse ponto da da PCA né cara tô testando poucos atributos Por que que você vai reduzir nós percebemos uma alta correlação é entre alguns atributos então a gente não quis eliminar de forma manual Vamos então automatizar que ele se parece ser bem mais interessante e para evitar né aquela questão do desbalanceamento nós aplicamos o esmalte com o Mac links E isso gerou né Essa bikeline que nós chamamos de prévia Carol por favor Beleza então a gente não vai detalhar tudo absolutamente tudo né mas um exemplo aí para cada passo ali do pai que Line a gente fazia uma análise né do resultado então um exemplo é esse aqui é do Passo do robux o que acontece a gente percebe que os originais e repara dados aqui na faixa de 80 2000 foram reduzidos para escadas muito mais próximas e porque ele trata o outline ele ele elimina os layer não ele não é mineiro ele coloca aqui de uma forma mais reduzida então você não perde a essência do dos dados originais Então esse esse método pareceu muito interessante Carol por favor Ok então a partir né do do Pai pilaine a gente partiu então para os treinamentos e otimização dos hiper parâmetros Ok então na disciplina passada Claro Nós aproveitamos o mesmo data 7 Esses são os resultados obtidos na disciplina anterior né onde os modelos foram esse aqui nesse ranking como a gente tratando aqui depressão de doenças né interesse é o recalcada só um pouquinho Carol é o record só que nem não apenas dele a gente percebeu que em um determinado experimento Ele simplesmente praticava tudo como como positivos como doença e aí era péssimo portanto não apenas é interessante cara a gente quis então né fazer um novo Spotify cara a gente fez o pipeline

- *Corpus ID:* 632
- *Score:* 0.8402762413024902
- *URL:* oculto
- *Início:* 00:25:51
- *Fim:* 00:28:08
- *Transcrição:* o máximo meu IC não consigo enquanto menor melhor né eu minimizei o máximo meu a EC E aí eu tenho o meu modelo né alcançado na melhor modelo alcançado vejam que não existe o melhor modelo existe o meu melhor modelo tá porque o Lisiane por exemplo de não jogar fora a renda e sim trabalhar com a renda olhando comprometimento de renda um outro analista pode ter uma outra ideia diferente tá tranquilo gente Ok veja só uma coisa sobre a culpa rock é uma medida de qualidade do modelo semelhante ao R2 só que o menor melhor o R2 Quanto maior melhor tem as fórmulas podem procurar na internet lá que tem umas formas feias lá para vocês tá o r faz para nós e para nós valer que o entendimento do uso dessas dessas medidas vai aparecer aí cedo daqui a pouquinho tá eu vou usar as duas opções com a minimização do AIC e com a o b valor lá atendendo até tal valor deixa eu mostrar uma coisinha ali que tá ali no slide que eu não chamei atenção quando o R2 for alto né quando a gente tiver multicolinidade geralmente pode acontecer as variáveis que entram elas são correlacionados com isso então elas contribuem o R2 vai aumentar mas os meus coeficientes vão ficar não significativos ou até pode acontecer de ficar negativo uma coisa bizarra mas vejam que o meu R2 Aumentou e o que que é bom um R2 alto tá me dizendo que esse modelo tem boa predição se o objetivo for só para dizer lembra lá tinha dois objetivos nos modelos né ou eu estimar o Y com base nos dias futuro ou eu avaliar a importância de cada X no Y olhando os coeficientes se o meu objetivo for sua estimar o y eu posso usar a culinária da

- *Corpus ID:* 636
- *Score:* 0.8392141461372375
- *URL:* oculto
- *Início:* 00:32:18
- *Fim:* 00:34:48
- *Transcrição:* binária tá na regressão logística por exemplo modelo de sobrevivência também usa curva rock tá então é uma outra métrica de qualidade do modelo quais o que você pode usar para testar a previsão na segunda que que eu posso usar É porque por exemplo na primeira mostra com os 80%, eu fiz a análise aí eu quero fazer essa ligação eu criei equação aí eu vou pegar essa equação e vou passar na segunda mostra tá e como é que você sabe se ficou legal eu vou avaliar as métricas eu vou calcular o R2 ou R2 não pode ser muito diferente do R2 da primeira amostra eu vou calcular a soma dos resíduos ao quadrado esta equação eu pego o x o X1 x2x3 dessa segunda mostra coloca dentro da equação e vou ter o y estimado para cada um dos indivíduos dos 20% e com base nisso ela tem duas colunas aqui eu observei e é que eu tô estimando com base nesse modelo que eu acabei de criar com a primeira amostra e esta diferença são os meus resíduos e é tudo com base nos resíduos que a gente avalia porque eu resido disso quando eu tô errando né e eu espero não errar mais do que eu tô errando na primeira não eu espero não errar muito mais na segunda porque eu tô errando na primeira geralmente gente espera alguma quedinha mas tem que ser bem pequena se tiver uma queda substancial é porque o meu modelo decorou os dados ao ser treinado ao ser construído tá como é se houver uma queda é porque ele é amostra de validação na hora que eu posso passar aqui testando validando na segunda mostra o meu R2 cair substancialmente o meu erro ao quadrado a soma dos erros ao quadrado subir substancialmente do errando muito mais é porque ao rodar a regressão na primeira amostra o modelo ali né acabou decorando os dados da primeira amostra e não serve mais para generalizar para a população bom eu vou falar isso tudo aqui fiz tudo

- *Corpus ID:* 2309
- *Score:* 0.8387145400047302
- *URL:* oculto
- *Início:* 00:14:51
- *Fim:* 00:17:00
- *Transcrição:* é que não tem nenhuma sobreposição entre treino e teste certo então os resultados são visto como mais Independentes entre si ao longo de múltiplas execuções porque muitos desses testes assumem Independência entre os dados e essa amostragem permite a variar medir essa variação de desempenho em função dos dados de Treinamento eu mantenho os demais configurações de preparamos então como eu falei esse teste foi implementado com alguns ajustes para esse caso de aprendizado de máquina de avaliação de modelos preditivos mas é bastante Fácil de usar né No momento que a gente tem uma implementação pronta por fim se eu quero comparar múltiplos modelos ou seja se eu tô avaliando três quatro cinco modelos né existe um teste de esse teste de q de cálculo né que é uma generalização daquele primeiro teste que a gente é discutiu né da tabela de contingência então ele faz essa generalização para comparar esses acertos e erros entre três ou mais classificadores e é interessante isso porque quando a gente está comparando três ou mais classificadores a gente roda esse teste de teste de coco para verificar se existe uma mudança uma diferença de desempenho significativa Entre esses classificadores certo se o valor de p é significativo ou seja se ele é menor que 0.05 sempre assumindo né que a gente adotou 0.05 como trecho então diz assim existe uma diferença significativa entre ser modelos Tá três ou mais mas não diz exatamente quais modelos diferem entre si e aí o que a gente tem aqui é uma o que a gente chama de uma análise pós-rock a gente detecta que existe uma diferença significativa entre esse ser classificadores e depois a gente faz o teste demanda para par para avaliar quais classificadores


**Resposta gerada pelo LLM**: A curva ROC (Receiver Operating Characteristic) mostra a taxa de verdadeiros positivos versus a taxa de falsos positivos (especificidade e sensibilidade).  Ela considera esses valores para diferentes taxas de aceitação, variando um "tanto" que determina a classificação de cada classe.  A área sob a curva (AUC) indica o desempenho do modelo; quanto mais próxima de 1, melhor. Em problemas binários, uma AUC de 0,5 indica um classificador aleatório. A curva ROC é usada em problemas binários e pode ser quantificada pela AUC.  Um score de 1 indica que para todas as predições, a taxa de verdadeiros positivos foi elevada sem elevar a taxa de falsos positivos; 0,5 indica um classificador aleatório.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately explain what the ROC curve is and how it is used to evaluate model performance. Supporting Evidence: The RESPONSE effectively defines the ROC curve, explaining its components (taxa de verdadeiros positivos e taxa de falsos positivos) and how it is used to assess model performance through the area under the curve (AUC). It also provides context on the significance of different AUC values, which directly addresses the PROMPT's request for information on both the definition and application of the ROC curve in evaluating model performance. Therefore, the RESPONSE is relevant to all parts of the PROMPT.  Score: 3


---
