**Pergunta 5**: O que é acurácia?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2004
- *Score:* 0.8418599963188171
- *URL:* oculto
- *Início:* 00:29:13
- *Fim:* 00:31:20
- *Transcrição:* não tava não reconheço ainda pela voz Denis pode falar aqui se colocou naquilo consenso ele ele tende a Gerar maior acurácia Beleza a gente pode dizer o mesmo para rival e precisão sim é uma boa pergunta na verdade assim aqui eu tô usando a Croácia Mas é em termos da nossa medida de desempenho tá por exemplo se eu criar um modelo um samba né tentando maximizar minha curar se ou seja acurácia foi a medida que eu usei base para melhorar esses modelos a tendência é que eu tenho esse mesmo efeito sobre acurácia eu até prefiro né uma uma assim usar o desempenho como uma palavra mais genérica mas dos livros da parte teórica que vocês né vão ler sobre Iansã ou falam muito dessa condição básica de taxa de erro a taxa de erro é um menos acurácia né mas assim a gente a tendência é que a gente observa esse efeito de uma forma geral talvez nem todas as métricas melhorem né Se eu olhar avaliar três quatro cinco metros talvez em todas melhorem mas a tendência é assim que a gente consiga reforçar aquelas métricas que foram o nosso foco em razão do treinamento desses modelos aqui né se eu quero modelos que maximizen Recall por exemplo ou talvez assim talvez eu tenho três modelos que tem um bom Recall Mas tem uma precisão mais baixa abaixo do que eu gostaria e ao criar um samba eu mantenho um bom Recall mas melhor a minha precisão Então esse efeito sobre o desempenho pode variar um pouco de acordo com o problema a forma como a gente está criando um samba e até o nosso objetivo de questão mesmo né de digamos assim de negócio com o modelo de aprendizado de máquina tá E esse modelo lançando ele ele esse samba ele a gente chama ele um modelo Claro que ele vai ser um modelo que é baseado numa agregação de múltiplos modelos mas a gente denomina ele como um modelo também

- *Corpus ID:* 1040
- *Score:* 0.8378767371177673
- *URL:* oculto
- *Início:* 00:54:49
- *Fim:* 00:57:20
- *Transcrição:* então é até bem mais complexo que isso bom então qual deles que eu vou usar bom deles é a exatidão né Vocês costumam nos dias de hoje eu não gosto dessa palavra falar em assertividade porque não é aqui a gente tem que falar de assertividade no sentido de acertar com senha não com dois dessas né um modelo ele aprende lá uma função de mapeamento Eu entro com dados ele mapeia para aquilo ali né Então como que eu Prevejo corretamente a classe e aí então a gente sabe que tem alguns modelos que são mais menos robustos a determinadas propriedades dos dados então se eu tenho mais valores faltantes é uma das métricas de desempenho que eu tenho a curasse é quando é a quantidade de vezes que eu acerto tá E aí a gente vai ver hoje que além da curasse a gente tem outras métricas chamadas precisam revogação F1 né porque pode ser que não me interessa só a saber quantas vezes eu acerto né mas saber de todos os casos que eu tinha que acertar quantos que eu acertei né quando eu disse que era de uma classe quando eu acertei isso é um pouquinho diferente da acurácia tá então essa habilidade de prever corretamente ela pode ser medida de diferentes formas a outra coisa pode ser a robustez ou seja Qual é o tipo de dado que eu consigo tratar né e habilidade que eu tenho de lidar com as distorções que existem no meu dado então por exemplo as armas de decisão as versões iniciais só trabalhavam com valores discretos depois elas passarem contínuos elas não lidam bem com ausência de valores elas lidam muito bem se eu tenho uma super variável discrimite né porque ela tem um ponto forte se tudo é mais ou menos igual ou seja seu conjunto de valores né não uma variável mas o conjunto a combinação de valores que que funciona as aves de decisão não funcionam muito bem É então tem uma série de técnicas que tem as suas fortalezas ou as suas falhas né em relação às propriedades dos

- *Corpus ID:* 1433
- *Score:* 0.8338696360588074
- *URL:* oculto
- *Início:* 01:23:44
- *Fim:* 01:24:41
- *Transcrição:* a gente vai falar de algumas medidas bem básicas de desempenho só para a gente poder ter uma base para discutir ao longo dessa disciplina que é acurácia taxa de acerto a precisão e o Recall a sensibilidade tá então são três medidas que a gente vai usar ao longo dessa disciplina para comparar a avaliação dos modelos aí na segunda disciplina a gente vai estender essa ideia de medidas de desempenho tudo bem eu vou propor o seguinte pessoal podemos fazer o nosso intervalo agora fechamos aqui voltamos daqui a 15 minutos até daqui a pouco gente toma um cafezinho aí machucado

- *Corpus ID:* 1467
- *Score:* 0.8336557745933533
- *URL:* oculto
- *Início:* 00:52:38
- *Fim:* 00:54:42
- *Transcrição:* então a gente pode ter uma taxa de acerto geral que é a Croácia né que basicamente a soma desses acertos verdadeiro positivo verdadeiro negativo sobre todos todos as instâncias mas às vezes eu quero ver especificamente bom esse modelo ele Tá acertando bem para aquelas positiva ou ele tá evitando falsos negativos então aí entram outras medidas de desempenho que focam nesses tipos de acertos de erros específicos então como eu falei a gente pode definir né A curasse A partir dessa Matriz de confusão Eu coloco aqui as fórmulas né acurácia e erro de uma forma genérica Mas aí tem essas outras medidas que a gente vai falar de duas agora e depois a gente na próxima disciplina a gente vai estender para outros então uma muito importante que se usa bastante é a sensibilidade ou Recall tá também chamado que a taxa de acerto na classe positiva tá existem alguns problemas que eu quero eu quero acertar muito bem classe positiva tá se eu disser que alguns negativos são positivos para mim tem Impacto menor do que eu dizer que positivo são negativos ou seja esses falsos negativos aqui eu quero evitar como eu falei domínios médicos né de saúde isso é muito relevante então a taxa de acerto na classe positiva o que que ela faz ela tá olhando porque eu destaco aqui para todas as instâncias que são de fato positivas dentre essas instâncias que estão de fato positiva que que é isso aqui né por isso que vem o denominador Esse é o denominador que isso aqui são todas as instâncias positivas dentre as instâncias positivas eu tô vendo quantas ela conseguiu classificar corretamente por isso que o meu numerador aqui o verdadeiros positivos tá então

- *Corpus ID:* 2003
- *Score:* 0.8333462476730347
- *URL:* oculto
- *Início:* 00:27:40
- *Fim:* 00:29:49
- *Transcrição:* saídas dos modelos para você tem três modelos analisar para um conjunto de teste como é que tá a saída Ah tá todo mundo dizendo que é a mesma coisa tem uma certa variação né Qual é a proporção dos que estão variando então isso a gente poderia quantificar digamos assim mas mais importante para salientar isso vai ficar mais claro ao longo da aula é que os algoritmos que a gente vai discutir que adotam essa esse paradigãs Rambo eles não fazem necessariamente essa verificação para gerar uma sample tá é como se eles eu tenho uma perguntada ali só eu acho que para a gente depois seguir uma uma ordem ali falou oi era bom deixar aí o momento depois aí para as perguntas né não porque a senhora tá no meio de um raciocínio aí aí a gente se perde né não é melhor não eu prefiro se vocês puderem levantar a mão porque fica mais porque daí eu paro e faço as perguntas se vocês puderem Acho que fica melhor tá para a gente claro que às vezes eu demoro atender uma pergunta eu vejo ali porque realmente eu quero terminar de explicar uma parte para atender as perguntas eu acho que fica melhor essa dinâmica mesmo certo eu não sei agora quem tá falando mas eu vou seguir Aqui tem duas perguntas depois a gente fala a pergunta do colégio que era o Antônio Fagner que tava fazendo a pergunta o Denis eu não sei se foi o Denis que falou agora tu não tava não reconheço ainda pela voz Denis pode falar aqui se colocou naquilo consenso ele ele tende a Gerar maior acurácia Beleza a gente pode dizer o mesmo para rival e precisão sim é uma boa pergunta na verdade assim aqui eu tô usando a Croácia Mas é em termos da nossa medida de desempenho tá por exemplo se eu criar um modelo um samba né tentando maximizar minha curar se ou seja acurácia foi a medida que eu usei base para melhorar

- *Corpus ID:* 7829
- *Score:* 0.8320932984352112
- *URL:* oculto
- *Início:* 01:24:02
- *Fim:* 01:26:39
- *Transcrição:* para esse tipo de situação e eu mostro que não tá vamos lá então basicamente ã a minha acurácia né ela me dá valores aí que não são tão bons quando a minha não são tão realistas na verdade quando o meu meu algoritmo eh quando meus dados são desbalanceados tá então a gente fez Todas aquelas contas né para esse conjunto de dados para problema de três classes hã e a nossa curaça deu 58.33 né que é 1 2 3 4 5 cinco acertos tá de cachorro eh um acerto de gato e um acerto de peixe né então isso dá sete acertos isso 1 2 3 4 5 6 s acertos tá são sete acertos de 12 né Eh nota que se o meu algoritmo chutar que tudo é cachorro o que que vai acontecer vou acertar esse esse esse esse esse esse esse esse eu vou errar esse esse esse esse esse tá então meu algoritmo se eu se ele chutasse que tudo é cachorro ele ia acertar os mesmos sete valores né claro outras imagens Mas ele também ia acertar sete de 12 imagens Tá qual algoritmo é melhor o que tenta aprender alguma coisa ou o que chuta aqui tudo é uma classe só Bom pelo meu jeito de falar o que tenta aprender alguma coisa melhor né Mas a gente pode quantificar isso né acurácia não vai ser a métrica que vai dizer que um é melhor que o outro acurácia vai dizer que os dois são iguais tá mas se eu considerar precisão de cada uma das classes eu vou ter 100% pro cachorro de precisão né e para meu gato meu peixe eu vou ter zero né então poderia usar um F1 determinar qu melhor desses dois ou usar tal da corcia balanceada então a corcia balanceada é basicamente a sensibilidade mais especificidade por dois pro problema binário

- *Corpus ID:* 2018
- *Score:* 0.8316631317138672
- *URL:* oculto
- *Início:* 00:52:13
- *Fim:* 00:54:25
- *Transcrição:* como zero porque a probabilidade para classe zero eu vou chamar de propriedade para classe positiva para não ficar essa confusão de zero e um Então essa Instância aqui por exemplo todas aí explicando o esse gráfico né de mapa de calor então a cor mapeia para probabilidade quanto mais próximo de um mais escura cor quanto mais próximo de 0 mais clara é no eixo X eu tenho as árvores tá as 500 e uma árvores e no Y eu tenho as minhas instâncias de testes são as mesmas para todos então o que eu tô observando aqui é que tem alguns casos como essa primeira instância por exemplo que praticamente todas as 500 em uma árvores né pelo menos visualizando assim parece todas dá uma probabilidade muito alta para classe positiva né providade de um enquanto a Segunda instância que essa linha todas as classes todas as árvores desculpem dá uma probabilidade muito baixa para classe positiva Então ela é classificada né acaba sendo classificada como zero Se a gente fosse desculpa como negativo né que aqui confunde porque a classe zero é positiva e percebam que tem alguns casos que já existe uma diversidade maior né então por exemplo esse caso aqui a gente tem vou dar um zoom aqui e vocês vão ver que a gente tem algumas cores né que oscilam enfim que não são nem azul Né Marinho nem o bege Então a gente tem alguns casos que as predições mudam um pouco aqui tem algumas árvores que dizem que a probabilidade é baixa para classe positivo outros dizem que é alta tá então isso seria uma forma de visualizar caso a gente queira verificar se de fato a gente tem diversidade né acontecendo e a gente tem alguns casos não né mas a grande maioria a gente tem tá E aí a questão é o seguinte vocês bom a gente tinha tido aqui em média um desempenho de 90% de acurácia eu tô pegando aqui acurácia como taxa de acerto tá como meu desempenho e aí a

- *Corpus ID:* 2285
- *Score:* 0.8299914598464966
- *URL:* oculto
- *Início:* 00:14:25
- *Fim:* 00:16:38
- *Transcrição:* vá aumentando o meu Recall e diminuindo a minha precisão o que eu não quero é que eu diminua muito a precisão a medida que aumente o recalque então uma das coisas que a gente usa para quantificar Qual é esse traidor entre precisão e por qual além da uf no score é a área sobre essa curva de precisão tá então aqui eu coloquei como sigla ao prc né então vocês vão ver de repente algumas siglas diferentes mas basicamente sumariza o desempenho Então essa essa área quanto maior é melhor né o desempenho então aqui por exemplo me dá uma área de quase 0.9 e justamente indica isso indica que eu consigo manter uma boa precisão a medida que eu vou aumentando o meu recall então existem métodos nos teclando que estimam essa área e que nos permitem também visualizar essa curva por outro lado a gente tem uma outra métrica que foca na taxa de acerto da classe negativa o Recall ele é a taxa de acerto para classe positiva certo mas se eu quiser ouvir o quanto que ele acerta para classe negativa Então essa métrica se chama especificidade Então se vocês perceberem aqui na matriz de confusão ela é uma métrica que foca nessa linha da classe verdadeira em que todos são negativos e basicamente ela Analisa quantos verdadeiros da negativos verdadeiros negativos eu tenho Entre todos que são verdadeiramente negativos ou seja quantos os negativos eu consegui classificar corretamente com esse modelo certo então aqui a gente tem a métrica da especificidade que é bastante utilizado também em conjunto com a sensibilidade que a taxa de acerto para classe positiva né sensibilidade no nosso mesmo recall e existe uma métrica que a gente usa a partir da especificidade que a taxa de falsos positivos que é 1 menos as especificidade certo então a gente consegue ver quantos solos positivos tem

- *Corpus ID:* 7830
- *Score:* 0.8293570280075073
- *URL:* oculto
- *Início:* 01:26:01
- *Fim:* 01:28:30
- *Transcrição:* dois são iguais tá mas se eu considerar precisão de cada uma das classes eu vou ter 100% pro cachorro de precisão né e para meu gato meu peixe eu vou ter zero né então poderia usar um F1 determinar qu melhor desses dois ou usar tal da corcia balanceada então a corcia balanceada é basicamente a sensibilidade mais especificidade por dois pro problema binário e uma média das sensibilidades aqui pro problema multiclasse tá então minha métrica de acurácia balanceada é basicamente esses valores aqui 71.4 33.33 50 dividido por três tá então é o que você tá faz o que você tá fazendo aí é colocando peso para os outros eh para os outros positivo para as outras classes que foram positivadas né Eh é basicamente basicamente isso vai tá esse peso ele tá meio implícito na no cmputo da sensibilidade né então a sensibilidade importante a especificidade também né mas a sensibilidade é importante e e a gente basicamente faz a média delas né então a ccia balanceada para esse modelo aqui vai ser de 51.59 E se eu comparar esse modelo com aquele com um outro aqui né que acerta todos os cachorros e e erra o resto eh lá pelas tantas eu vou ter um peso Grande para uma das classes né mas peso zero pras outras duas tá se se peso cai bem nessa brincadeira aí mas sim a ideia é que eh basicamente eu calculo a a acurácia por classe depois eu tiro uma uma média das acurácia por classe tá e da outra forma sem ser balanceada eu calculo uma acurácia Global essa minha acurácia Global ela pode ser misleading né então é mais ou menos essa interpretação Professor eh comparando o F1 com essa acura é balanceada o F1 ainda é

- *Corpus ID:* 1363
- *Score:* 0.8279720544815063
- *URL:* oculto
- *Início:* 01:06:00
- *Fim:* 01:08:04
- *Transcrição:* porque vai ter algumas fronteiras que para acertar todos os dados que a gente conhece elas tem que ser muito complexas e ao ser muito complexa ela acaba se sobre ajustando aos dados que a gente tem então às vezes é bom a gente não acertar tudo os dados que a gente conhece porque a gente gera modelos que gene ralizam melhor para novos dados o Alexandre tem uma pergunta professora acurácia para o modelo né eu tenho como colocar ver apurácia da instância e por dizer não isso aqui tá 57% como a senhora falou naquele exemplo eu vou deixar ele aqui numa classe Indefinido porque aí eu nem tomou uma decisão arriscada no caso isso é isso pode ser usado é comum existe não existe essa questão de usar coração Pode pode sim isso aí seria Alexandre a questão de uma probabilidade de predição então a gente pode assumir por exemplo o modelo vai nos dar uma probabilidade zero assim isso é uma classificação binária eu posso decidir por Olha se for 05 eu digo é maligno com a probabilidade 57% ou eu posso dizer assim olha só vou dizer que é maligno se a probabilidade para cima de 70%, entre 40 e 70 eu vou dizer quem conclusivo sei lá 8:40 e 60 entende a gente como desenvolvedor enfim desse modelo junto com obviamente quem entende do domínio quem vai usar a gente pode definir como é que a gente vai dar essa resposta em cima da probabilidade que o modelo nos retorna se a gente quer atribuir uma classe junto com uma probabilidade a gente quer dizer olha só vou dizer que é maligno se a probabilidade associada maligno for acima de tantos por cento então tem como a gente fazer isso sim é E aí uma coisa que a gente faz ver é que com base nessa Fronteira de decisão linear que existem várias Como já foi mencionado os modelos podem tomar sua decisão os modelos que a gente gera para qualquer novo dado que cair aqui vou botar Verde qualquer novo dado eu


**Resposta gerada pelo LLM**: Acurácia é a quantidade de vezes que se acerta; é a taxa de acerto, a soma dos acertos verdadeiro positivo e verdadeiro negativo sobre todas as instâncias.  Também pode ser definida como 1 menos a taxa de erro.  Em alguns casos, a acurácia pode ser enganosa, especialmente em dados desbalanceados.  A acurácia balanceada é uma alternativa que considera a sensibilidade e a especificidade.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately define "acurácia" and provide context that relates to the concept. Supporting Evidence: The RESPONSE provides a clear definition of "acurácia," explaining it as the rate of correct predictions and discussing its calculation. It also mentions the potential misleading nature of accuracy in imbalanced datasets and introduces the concept of balanced accuracy, which adds depth to the explanation. This comprehensive approach addresses the PROMPT fully and accurately.  Score: 3


---
