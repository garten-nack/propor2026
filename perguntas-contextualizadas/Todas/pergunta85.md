**Pergunta 85**: Quais algoritmos são sensíveis à diferença de escala dos atributos?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 2118
- *Score:* 0.8792046308517456
- *URL:* oculto
- *Início:* 00:20:27
- *Fim:* 00:22:32
- *Transcrição:* os valores de entrada de um atributo usando pesos né nessas conexões tudo isso se eu tiver diferentes escalas vai impactar nessa agregação que ela faz os valores para propagar na rede né para depois calcular a saída e o erro tá então esses algoritmos precisam passar por uma normalização dos dados algoritmos baseados em árvores de decisão incluindo os sambas não são algoritmos afetados por essa diferença de escala tá então eles não demandam normalização não quer dizer que vocês não possam fazer se vocês estão usando vários algoritmos e alguns demanda normalização e outros não vocês podem aplicar normalização por padrão e usar em todos os algoritmos não tem nenhum problema mas eles não demandam né E se vocês forem lembrar árvores de decisão Analisa atributo por atributo aquele atributo em relação à classe né o quanto que aquele atributo diminui a impureza em relação àquela classe em relação àquela saída então justamente como eu analiso os atributos separadamente se um varia de 0 a 100 em outro varia de 0 a 1 isso não vai impactar não vai impactar porque eu não tô analisando eles de uma forma combinada digamos assim e o nave base é a mesma coisa né a probabilidade condicional ela é feita ou separadamente pela pela suposição de dependência condicional né das classes então eu não tô usando eles juntos né então isso diminui ou me diga esses efeitos indesejados das Diferenças de escala não quer dizer que vocês não possam fazer vocês podem fazer tá não tem nenhum problema em relação a isso tá tem uma pergunta do Antônio Fagner pode falar Oi Mariana é só para acrescentar o seguinte né quando a gente se a gente for fazer isso para todos os algoritmos por exemplo fazer normalização é importante a gente é tipo deixar dentro de uma função porque porque os dados que a gente vai receber que não conhece a gente a gente tem que

- *Corpus ID:* 1404
- *Score:* 0.8713851571083069
- *URL:* oculto
- *Início:* 00:38:01
- *Fim:* 00:40:10
- *Transcrição:* eles funciona bem na maioria dos problemas mas se vocês tiverem a oportunidade de fazer uma comparação dessas medidas de distância é interessante porque para alguns problemas algumas medidas de distância podem sair melhor que outras e não e essa melhor não ser a padrão que é normalmente euclidiano que aqui certo e tem um ponto importante aqui a gente vai trazer um aspecto que já tá associado com uma etapas para processamento de dados que normalização mas não tem como a gente não falar disso nesse algoritmo tá quando a gente tem medidas de distâncias em atributos quantitativos essas distâncias elas são normalmente afetadas pela escala de valores Isso quer dizer se eu tiver um atributo por exemplo idade e outra atributo salário a minha distância euclidiana para salário ela vai ser sempre né provavelmente sempre maior do que idade porque porque o valor de salário ele muda ele varia o intervalo maior em quantidade eu vou estar falando sei lá de 0 a 120 por exemplo né exagerando 120 salário eu vou tá falando de zero a sei lá quantos né centenas de mil reais por exemplo né então qualquer aqui eu deixo um exemplo qualquer variação qualquer comparação entre esses valores do ponto de vista de distância vão resultar normalmente um valor maior para aqueles atributos que variam intervalos maiores né então aqui a distância por exemplo é cinco e essa distância que é 4.900 o efeito disso pessoal é que isso causa um impacto desproporcional esses atributos no cálculo das distâncias certo então a consequência disso é que quando tiver calculando a distância de um novo ponto para os existentes o salário ele vai dominar esse cálculo ele vai dominar essa distância porque se eu tiver uma variação né de sei lá 800 para vamos supor 950 que é uma variação relativamente pequena do ponto de vista

- *Corpus ID:* 1430
- *Score:* 0.8658828139305115
- *URL:* oculto
- *Início:* 01:19:12
- *Fim:* 01:21:15
- *Transcrição:* sem atributos né dependendo Claro do número de Instância sem atributos podes estimativa dimensionalidade muito alta para o número de instâncias que eu tenho para conseguir treinar o meu modelo e é isso impacta porque o que vai acontecer que quando eu tenho uma dimensão muito grande Tá por exemplo sem atributos essa diferença de valor quantitativo né de distância do vizinho mais próximo para o vizinho mais distante vai ser muito pequena vamos porque eu tenho 100 atributos né e eu tenho variações em 10 atributos digamos assim 10 entre 100 a quantidade né o impacto dessa variação no valor de distância final é muito Sutil Então o que acontece é que o algoritmo acaba tendo não funciodo bem digamos assim né perdendo o seu potencial preditivo porque essa essas diferenças né quantitativas entre instâncias próximas realmente semelhantes em distâncias distantes passa a ser muito pequena então ele é um algoritmo que ele se beneficia né Mais Uma Vez Mais um motivo se beneficia do processo de seleção de atributos ou de técnicas de redução de mensalidade como acho que foi o Antônio que comentou o PCA por exemplo tá seleção de atributo é uma forma de reduzir dimensionalidade tá IPCA também é outra forma que é mais a linha que a gente chama de extração de atributos tá Então são duas formas de reduzir a dimensionalidade do meu problema então o PCA eu posso ter um problema com 300 atributos reduzido para 34 dimensões são ficar com as três quatro componentes principais Então isso acaba sendo uma das desvantagens então algoritmo que a gente tem que cuidar um pouquinho quando tem problema com alta dimensionalidade porque o KNN ele passa até essa dificuldade diferenciar o que tá longe o que tá perto porque a medida de distância Varia muito pouco em função do grande número

- *Corpus ID:* 1405
- *Score:* 0.8641526699066162
- *URL:* oculto
- *Início:* 00:39:37
- *Fim:* 00:41:48
- *Transcrição:* o efeito disso pessoal é que isso causa um impacto desproporcional esses atributos no cálculo das distâncias certo então a consequência disso é que quando tiver calculando a distância de um novo ponto para os existentes o salário ele vai dominar esse cálculo ele vai dominar essa distância porque se eu tiver uma variação né de sei lá 800 para vamos supor 950 que é uma variação relativamente pequena do ponto de vista de salário eu já tô falando aqui de uma diferença de 150 é mais do que qualquer valor que tá no intervalo idade por exemplo tá e o que acontece é que o algoritmo vai entender como resultado dessa diferença né ele vai entender na hora de encontrar os mais próximos que o salário é um atributo muito mais importante do que idade para determinar a similaridade porque por causa desse Impacto né dessa dessa dessa diferença de proporção de valores entre dois atributos por isso que quando a gente trabalha com esse tipo de algoritmo que que tem como base medidas de distância comparação entre pontos a gente tem que fazer a normalização dos valores ou seja normalizar esses valores numéricos para que eles variam na mesma escala porque aí eu não vou ter um impacto maior eu não vou estar fazendo com que a minha distância seja dominada pela pela ideia pela pelo valor do salário tá só porque salário varia em intervalos maiores e sempre vai impactar mais então quando eu falo normalização não é transformar uma curva normal é que todos os intervalos eles vão ser eles vão ser mapeados para o mesmo valor por exemplo todos os atributos vão variar entre 0 e 1 tá claro que depende eu vou comentar você só se puder botar rapidinho o teu Porque tem uma voz no fundo é claro que tem diferentes formas de normalização tá então a forma de normalização que eu tô comentando aqui com vocês Essa normalização nem Max

- *Corpus ID:* 3612
- *Score:* 0.8619449138641357
- *URL:* oculto
- *Início:* 00:37:46
- *Fim:* 00:40:25
- *Transcrição:* me ajudam né avaliar esse objetivo avaliar os algoritmos e bom vocês perceberam que os intervalos Como eu havia dito de valores como são aleatórios eles são bem variáveis né E aí então para medir a distância o ideal né normalmente é reescalar redimensionar as dimensões para que elas fiquem dentro da mesma faixa de valores então aqui Até onde eu sei você já devem ter usado algo parecido em outros em outras situações Então o que eu fiz eu gerei novamente o conjunto dados primeiro deles aquele com que tem eles ficam mais são três classes e fica um pouco mais mais separados apesar de os dois de se manterem um pouco de sobreposição E aí eu eu executo não é o que eu crio uma Instância do escalonador ou padronizador padrão E aí é um aplico né eu aplico o método como eu tinha dito tem uma API que tem funções com nomes parecidos né então eu peço para ele fazer o vídeo ajuste dessas features que foram geradas pela make blobs e coloco numa outra variável então se você compararem né as features originais com umas features ajustadas vocês vão ver que todas elas estão dentro de um mesmo intervalo e esse existem vários vários métodos de padronização né imagina você já tenham estudado isso mas eu trago alguma coisa sobre isso também esse método em especial em específico ele faz padronização da seguinte forma ele faz uma reajuste de maneira que todos eles acabam tendo a mesma média no ponto zero e no máximo dizia o padrão de um então eles vão acabar ficando aí entre vai fazer um ajuste né Considerando o desvio padrão específico de cada dimensão e colocando a média o centro no ponto zero mesmo que eles não tenham então com isso eles acabam ficando dentro de um de um padrão né que fica mais fácil de eu aplicar alguma métrica de distância e essa e esse essa

- *Corpus ID:* 2493
- *Score:* 0.8612785339355469
- *URL:* oculto
- *Início:* 00:18:20
- *Fim:* 00:20:13
- *Transcrição:* algum que não tá entre Stop Car nesse conjunto que eu selecionei será que não melhora o meu desempenho então isso são coisas que o algoritmo não conseguem analisar tá então reforçando algumas coisas que eu já comentei então são super rápidos de plástico computacional tá então grandes conjuntos de dados são úteis principalmente se eu quero fazer por exemplo eu quero fazer da forma correta do ponto de vista de vazamento de dados tá que todos esses métodos de seleção de atributos eles são suscetíveis a vazamento de dados ele é um método super rápido então mesmo que eu coloque isso embutido dentro de uma validação cruzada ele é mais rápido do que que a gente vai ver tá ele realmente é um método porque basicamente ele vai analisar um score para cada atributo então claro que também depende o número de instâncias mas o cálculo não Não envolve outras combinações como outros algoritmos que a gente vai ver E aí como ele é independente de algoritmo então assim eu não tô analisando algo relacionado ao desempenho de um algoritmo específico para rankear esses atributos eu posso usar por diferentes algoritmos de aprendizado de máquina Então posso fazer uma seleção e aplicar diferentes algoritmos enquanto outros que levam em consideração o algoritmo Precisam fazer esse efeitos aplicados separadamente algumas desvantagens né tanto correlações entre atributos tá não entre atributo e a classe ou alvo né Mas entre atributos assim como relações de múltiplo linearidade coloquei que um exemplo né Se eu tivesse um atributo custo por produto número de produtos vendidos e custo total Eu Tô analisando cada um deles individualmente em relação à classe mas é claro que esse custo total ele tem relação com esse

- *Corpus ID:* 3570
- *Score:* 0.8609451651573181
- *URL:* oculto
- *Início:* 00:48:46
- *Fim:* 00:51:09
- *Transcrição:* claro cada família de algoritmos vai separar esses elementos de uma maneira diferente e alguns são famílias que identificam elementos dessa forma esférica que isométrica outros são mais adequados para elementos que tenha se esse alongamento outras para elementos que estão agrupados numa certa distribuição Então vai caber a nós tentarmos Identificar qual é a família mais adequada Caroline por favor Oi professor só olhando essa tudo bom olhando essa esse primeiro exemplo aí do slide ele meio que vai contra o que foi colocado como definição de cluster né que tem que estar separadinho ali exatamente mas pode acontecer pode acontecer aqui eu botei esse exemplo de propósito porque essa separação né normalmente para Nós seres humanos elas se dá com base em alguma noção de de distância não é mas o que esse Crush representa que existe alguma outra propriedade que não necessariamente é a distância entre eles e que fazem eles estarem separados né E isso não é muito fácil de entender então às vezes onde eu quero chegar às vezes eu preciso ser capaz de observar os atributos que eu que eu tenho naquele quantidades e escolher um atributo que seja mais adequado para fazer essa separação então talvez o atributo aqui não seja não tenha sido usado a tributo correto né E aí se eu focar numa tributo trazer um pouco a cor ela conseguiria Ela não é uma coisa espacial mas conseguiria separar em três porque eles têm cara que eu tô forçando a barra né as cores foi eu que coloquei mas supondo que tivesse um atributo cor ele seria um que me permitiria separar em três grupos adequados Então essa analogia só para mostrar que que às vezes eu posso não estar escolhendo ou usando as propriedades atributos que me permitiriam separar os objetos de uma

- *Corpus ID:* 2182
- *Score:* 0.8608871698379517
- *URL:* oculto
- *Início:* 00:31:09
- *Fim:* 00:33:19
- *Transcrição:* interessante Ah tem uma pergunta deixa eu passar a Caroline Pode falar É o seguinte é sobre essa questão de vieses né como você tinha comentado pode vir da coleta dos dados e pode ser amplificado pelo algoritmo pode acontecer o caso de a gente ter o dado balanceado ou seja coleta foi bem feita e ainda assim o algoritmo ter esse viés acertar mais para um tipo de classe do que para outro pode acontecer pode acontecer porque às vezes a gente olha a distribuição de um atributo né Por exemplo a classe ou por exemplo quero ver seu atributo ali borracha ou idade o gênero tá mais ou menos equilibrado né às vezes mesmo que ele esteja equilibrado o que o algoritmo vê né na maioria dos algoritmos de uma forma muito clara é um padrão entre atributos E aí isso isso pode estar causando nessas diferenças por exemplo eu tenho alguns padrões que eu tenho daqueles daquelas combinações de atributos eu tenho muito mais exemplos torna mais fácil aprender aquele Exemplo né E às vezes tem o que a gente chama essas variáveis que são relacionadas assim por exemplo tem um caso bem famoso de um algoritmo que era usado para atribuir risco de saúde no sistema usado nos Estados Unidos e aí teve uns pesquisadores que acharam que o algoritmo ele prejudicava pessoas negras então se eu tinha uma pessoa branca uma pessoa negra com os mesmos sintomas a pessoa branca era atribuído um risco maior então no processo de triagem ela tinha preferência E aí foram observar que o problema é porque tinha outras informações ali dentro que eram enviesadas Entre esses grupos que o algoritmo tava entendendo né que havia essa diferença e essa informação era custos com saúde Então como as pessoas a população Branca tinha mais custos com saúde né e ele esse custo com saúde era usado como proxy uma

- *Corpus ID:* 1489
- *Score:* 0.8607732653617859
- *URL:* oculto
- *Início:* 01:29:56
- *Fim:* 01:32:06
- *Transcrição:* então por isso que eu coloco aqui no tipo uma observação a gente não vai tratar essa essa remoção ou seleção de atributos nesse momento tá mas a tua observação tá certíssima o motivo de eu não fazer nesse notebook é que o foco notebook a gente explorar o algoritmo em alguns outros aspectos tá só para esclarecer sentido porque que não é feito mas a observação tá correto deveria tirar alguma delas fazer redução de mencionalidade fazer seleção de atributos então lembrando da estrutura do nosso notebooks é que tem uma pergunta né Vamos botar um zoom aqui de novo Como é que se atributos né se desculpem se apresentam em relação à distribuição de valores né se tem muita discrepância entre os seus intervalos então aqui aquilo que a gente discutiu sobre Sim a gente tem atributos variando entre 0 e 0.013 atributos variando entre 0 e 4000 Então existe uma grande discrepância Entre esses intervalos tá então isso é a pergunta é para fazer com que vocês analisem esses gráficos sobre uma perspectiva que ela é muito importante para o algoritmo que a gente tá estudando certo mas todos esses atributos estão na mesma unidade de medida né 100 milímetro ômetro que for isso a gente exatamente a gente não não tô entrando nesse nível de detalhe mas a gente pode assumir que a gente pode usar esses atributos como eles estão eu nem sei exatamente como é que a medida de distância aqui tá ou desculpar medida a unidade de representação deles para ser sincero teria que puxaria a descrição dos dados E aí pessoal quero chamar atenção para duas coisas aqui antes da gente nesse período final da aula primeiro eu quero mostrar para vocês como é que a gente usa o roldauto com o site planner tá que essa biblioteca que

- *Corpus ID:* 1537
- *Score:* 0.8605474233627319
- *URL:* oculto
- *Início:* 00:49:47
- *Fim:* 00:52:00
- *Transcrição:* menores e nesse caso Então tudo na evidez é extremamente necessário a gente fazer o que a gente remover os atributos que são altamente correlacionados E aí ele funciona melhor seria isso isso Exatamente esse esse tipo de algoritmo é essa esse pré-processamento para achar os atributos relacionados normalmente É vantajoso né Por nave base Exatamente é que como eu falei ele ele até certo ponto robusto a isso mas assim mas a gente sabe se eu tiver um conjunto de dados com muitos atributos que são muito dependentes entre si é claro que isso vai ter algum Impacto então melhor a gente tirar isso E tu trouxe um ponto interessante que é a questão da normalização ali no KNN aqui por exemplo normalização não é necessária não é não é necessário ele é um algoritmo que não tem não é sensível essa questão da normalização porque ele tá olhando atributo por atributo ele nunca pega os dois atributos juntos ele vai analisar as probabilidades do valor de idade data classe sei lá covid e do valor de salário de um exame lá do de leucócitos lá do covid ficção coisas em escala totalmente diferentes né e não tem problema não tem não ele não é sensível a esse tipo de questão tá o Dênis tem uma pergunta pode falar Denis até mesmo aprofundando a pergunta do Fagner então assim se a gente tivesse aquelas escalas muito diferentes isso iria impactar muito negativamente o nosso algoritmo aqui no livebays se eu tiver atributos correlacionados tem duas possibilidades ou vai atrapalhar muito Talvez seja isso ou digamos assim eles não ajudam muito no resultado final a pergunta eles atrapalham muito se por acaso eles permanecerem lá é que aí tudo depende é difícil em tudo depende da quantidade de tributos redundantes assim no sentido de correlacionado sabe se tiveres muitos atributos correlacionados no fim eles não vão auxiliar tanto né nessa questão


**Resposta gerada pelo LLM**: Algoritmos baseados em medidas de distância, como o KNN, são afetados pela diferença de escala dos atributos.  Algoritmos baseados em árvores de decisão, incluindo o Naive Bayes, não são afetados por essa diferença de escala.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should address the question about which algorithms are sensitive to the difference in scale of attributes. Supporting Evidence: The RESPONSE correctly identifies that algorithms based on distance measures, such as KNN, are sensitive to the scale of attributes, which directly answers the PROMPT. It also mentions that tree-based algorithms, including Naive Bayes, are not affected by this difference, providing a complete view of the topic. Therefore, the RESPONSE is relevant to the entire PROMPT.  Score: 3


---
