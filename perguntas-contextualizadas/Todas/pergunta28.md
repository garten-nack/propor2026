**Pergunta 28**: Quais são os principais fatores que contribuíram para a melhoria da qualidade da tradução automática de textos? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 8642
- *Score:* 0.8674771785736084
- *URL:* oculto
- *Início:* 00:35:33
- *Fim:* 00:37:42
- *Transcrição:* slide que a gente tirou de um de um trabalho de um summit né e também de alguns documentos do Google eh no entanto né a gente pode avaliar aí a amplitude das pontuações ela variou de 0 a 62 né que é uma variação considerável na qualidade eh avaliando a a estatística descritiva a gente pode ver que a maioria das as traduções se encaixa na faixa de 16.9 a 31.79 né que que vão de traduções com sentido Claro mas com erros gramaticais até boas traduções eh no entanto a há presença também de traduções com pontuações abaixo de 10 né consideradas praticamente inúteis e algumas na contramão né algumas com pontuações acima de 60 que é de acordo com essa tabela eh eh indica a qualidade superior à humana né Eh os resultados observados indicam a variabilidade na qualidade automatizada destacando talvez a necessidade de eh aprimoração da implementação eh para garantir tradução mais consistente e precisa né já me adiantando um pouco os trabalhos futuros aí eh durante a nossa análise nós identificamos nove casos em que a pontuação foi zero destes três foram causados por referências em espanhol né que foram classificadas incorretamente durante a fase de pré-processamento como em inglês nós utilizamos uma biblioteca que faz detecção automatizada Ocorreu algum erro lá na detecção do do que causou essa falha na base né digamos assim na no dataset Enquanto o outro foi resultado de uma referência em inglês que Diverge significamente do texto original em português também falha na base eh nesse caso é porque a a o texto original em português e o texto inglês que consta no no no capis Eles eram muito diferentes mesmo Sim acho que houve uma falha na tradução lá do do autor eh Além disso Os cinco candidatos em português foram

- *Corpus ID:* 8641
- *Score:* 0.8611810207366943
- *URL:* oculto
- *Início:* 00:33:55
- *Fim:* 00:36:11
- *Transcrição:* trabalho prévio do colega né e esse processo nos permitiu analisar a qualidade das traduções geradas Pelo modelo llm com base nos resumos eh dos trabalhos do CAPS né do CAPS perdão explicando um pouco mais sobre o dataset né Eh ele é proveniente do catálogo de teses e dissertações Caps n tem um total de 82.36 eh eh eh eh registros e 59 atributos eh oferece informações sobre trabalhos acadêmicos e de todos esses atributos O que é particularmente relevante pro nosso estúdio são o resumo e o abstract Né que é o DS resumo DS abstract esses atributos são fundamentais paraa Nossa análise pois ele permite avaliar a qualidade das tradições né e utilizando a métrica BL podendo quantificar a precisão dessas tradições geradas e identificar eh áreas para melhorias refinamento no processo de tradição automatizada né aí na tela tem uma tabela com um exemplo né de uma observação do texto DS resumo no texto original na língua eh no caso português o abstract traduzido que é o Ground truth e o abstract llm que foi a nossa referência né o nosso candidato eh estes são os resultados que nós tivemos nós obtivemos com a tradução automatizada utilizando o modelo llm né Eh a média das pontuações Blur foi de aproximadamente 25 indicando uma qualidade Geral moderada das traduções né Eh eh de acordo essa referência né é de acordo com a tabela de interpretação que tá no slide que a gente tirou de um de um trabalho de um summit né e também de alguns documentos do Google eh no entanto né a gente pode avaliar aí a amplitude das pontuações ela variou de 0 a 62 né que é uma variação considerável na qualidade eh avaliando a a estatística descritiva a gente pode ver que a maioria das as traduções se encaixa na faixa de 16.9 a 31.79 né que que vão de traduções com sentido Claro mas com erros gramaticais

- *Corpus ID:* 8422
- *Score:* 0.8577990531921387
- *URL:* oculto
- *Início:* 01:25:13
- *Fim:* 01:28:08
- *Transcrição:* Folha de São Paulo pronto tá ótimo obrigado que pode ser usada é que assim a a não ter um Data Set anotado uma coleção de teste daí implica muito trabalho braçal e que a gente não não precisa ter nesse nesse momento Aí sim o trabalho Fica dá muito trabalho bom dia professora aqui Bom dia Antônio alísio falando eh na nossa proposta né Nós eh é aquela proposta de fazer tradução usando um um modelo generativo né llm open source Eh desculpa a nossa ideia é usar a base de dados lá do CAPS né Uhum para fazer a tradução do resumo e comparar com o abstract que tá na própria base isso aí eh eu eu não encontrei a informação de que qual de qual a origem da tradução que consta na base do CAPS assim obviamente é que consta no documento original né É deve ser o autor a que o autor fez Ok essa tradução ela precisa ser feita por algum profissional específico não não tu vai tu vai comparar com a tu vai considerar que o teu Gold Standard é a tradução que a pessoa fez se a pessoa usou Google Translate se ela pegou um dicionário leu e traduziu palavra por palavra e fez um trabalho horroroso é é que é a que tu é a fonte que tu tem para para dizer que eh que tá essa tradução que a pessoa botou no artigo se eu se eu conseguir chegar perto dela tá tá bom o suficiente e aí as métricas costum para paraa tradução costumam ser o o b e o ruj ble b l e u e e o ru r oou u g e beleza OK hã tinha outra pergunta eh ru é infelizmente não tô lembrando mas eu pergunto depois tá mas pode mandar ali no no teams ou então no no fórum Ok obrigado Professor mais alguém pessoal se lembrarem de alguma pergunta pode e mandar mandar pra gente na nossa próxima aula então a gente vai falar do dos modelos generativos né do gpts e E

- *Corpus ID:* 8669
- *Score:* 0.8568640351295471
- *URL:* oculto
- *Início:* 01:24:54
- *Fim:* 01:27:22
- *Transcrição:* metodologia como a gente viu que ficou ia ficar um trabalho muito grande então a gente separou em vários colabs em cada um desses colabes a gente fez essa estratégia aqui a nossa primeira foi uma espécie de uma validação da tradução original que já veio no no dataset que ele tinha a tradução em inglês e uma em português a gente fez um comparativo uma validação da similaridade entre essa tradução em português com uma out eh tradução usando uma ferramenta de de tradução à parte e a gente fez esse comparativo da similaridade depois nós fizemos um comparativo dos desempenhos eh usando o modelo Random Forest do da tradução em inglês com a tradução em português fizemos um novo comparativo entre os modelos handle Force e us svm com relação à tradução em português e por último nós fizemos uma uma análise de hiperparâmetros né usando o Grid search paraa gente ver se haveria ou não melhoria no desempenho ou possíveis detecções de overfitting aí aqui a gente teve o assim na prática a gente começou com o nosso estudo de similaridade a gente viu que eh entre a tradução em português e a nova tradução de uma amostra de de 100 amostras usando a simbilidade do Cosseno não a gente teve uma sim similaridade média de 71 por. teve um desvio padrão até alto em torno de 13% e a gente viu que teve traduções que ficaram eh perto de 99% Teve um caso lá que a gente viu que foi 100% mas a gente viu também que teve algumas eh traduções que ficaram bem abaixo como essa daqui que vocês estão vendo aqui no achurado na na linha dois esse id2 aqui a gente viu que a tradução a nova tradução ela ficou muito pobre ficou até estranha assim ele traduziu tão querido a outra foi eh é um bebê então a gente viu que realmente podia haver grandes discrepâncias entre as traduções né aqui ao lado a gente tem esses gráficos um histograma e um gráfico de barras que estão no nosso relatório né devido à limitação de tempo

- *Corpus ID:* 8034
- *Score:* 0.8567936420440674
- *URL:* oculto
- *Início:* 01:09:41
- *Fim:* 01:12:07
- *Transcrição:* e aí o a tradução que eu tinha na época era a princesa da Rocha Então essas traduções aqui elas foram obtidas com sistema sistran então lá em 2003 a gente não tinha ã Google Translate não tinha sistemas que a gente pudesse usar na internet eh e e ter uma tradução gratuita então o melhor sistema comercial disponível que a gente podia comprar então vai Compra um CD instala instala o o software era o cstr então na época eu investi boa parte da minha bolsa de doutorado em comprar esse esse sistema mas aí a gente vê que a qualidade não era lá das melhores ó Michael jackson's funds ele achou que era os ventiladores do Michael Jackson a Princess of rock é a princesa da Rocha aqui eh soviet ru ele chamou de régua em vez de de domínio né E a minha favorita é essa última aqui que ah ela deve tá cortada para vocês mas é eh o Iraque de Iraque back to Square One on sanctions ele achou que era o traseiro do Iraque ao quadrado um em sanções em sanctions nem traduzido depois então isso 2003 dá um salto para 2021 eu fui lá no Google Translate E aí eu coloquei todas essas H essas esses originais em inglês e pedi para ele traduzir e ele acertou 100% de de todos quer claro isso não foi um experimento científico exatamente mas ele mostra que a qualidade da tradução melhorou absurdamente então acredito que vocês devem ter percebido também essa essa evolução na na tradução mas o que que possibilitou esse salto de qualidade em processamento de linguagem natural e Recuperação de informação Então eu tenho aqui sete razões Então a primeira é a disponibilidade de corpora então disponibilidade de texto tanto em inglês como multiling então texto é fácil da gente conseguir então eu posso vou lá na Wikipedia consigo baixar dum de toda Wikipédia de um idioma específico e são mais de 200 idiomas existe um um um Corpus chamado common craw que tem

- *Corpus ID:* 8648
- *Score:* 0.8566945195198059
- *URL:* oculto
- *Início:* 00:45:51
- *Fim:* 00:48:53
- *Transcrição:* eh implementação resultados e e e né então foi no caso respectivamente Salvador Ant al e o Jeferson mas né todo mundo contribuiu com todas as partes né de maneira geral tá jo Parabéns pessoal um tema novo assim é inovador vocês trazer coisas modelos bem atuais então muito legal parabéns beleza obrigado o próximo agora é o trabalho o não vai apresentar né o pessoal pediu para não apresentar então ah trabalho cinco Antônio Carlos e João Vocês estão prontos podemos apresentar Professor Dea eu tentar compartilhar compartilhar uhum ainda não tá compartilhado só para te avisar agora apareceu a apresentação agora simim tá eh Bom dia a todos eh Carlos Henrique falando eh a gente vai apresentar Nossa nosso trabalho aqui sobre o tema de simplificação textual tá a equipe era eh o Antônio Fagner eu ch né E o João Ok eh Então qual é a nossa motivação né a gente vê muito hoje eh textos nos sites de governo nas mais diversas páginas aí do do Golf BR o próprio páginas aí de governos estaduais que tem eh linguagens muito técnicas né que tem palavras que não estão no uso comum do do dia a dia do cidadão né Então essa é a nossa principal motivação é ver se a gente conseguiria eh achar algum mecanismo para automatizar né Essa simplificação textual para disponibilizar textos mais simples né essa essa é a ideia tá eh a o nosso contextualização aqui a gente tentou utilizar né o verificar se utilizando wordnet e classificadores de postag né do do próprio nltk tem alguns lá eh para simplificar um texto português né Eh o Corpus que a gente vai

- *Corpus ID:* 8645
- *Score:* 0.8559627532958984
- *URL:* oculto
- *Início:* 00:40:38
- *Fim:* 00:42:56
- *Transcrição:* perguntando o quanto aqueles erros não estão arrastados lá dentro porque vocês fizeram uma inspeção nos nos zeros e isso pode ter ocorrido para outros casos né de ter teses ali em português que Como Tu disseste o objetivo era traduzido em inglês para pro português ou aqueles casos do espanhol então talvez tu ten essas ocorrências ou seja acho que tu botaste no trabalho futuro não sei fazer uma limpeza ali uma investigação dessa base porque esse esse erro pode tá acontecendo né E aí tu tá com um valor de repente ele tá desempenhando bem ou separar um conjunto bem confiável sabe e Verê assim que não que atingiu alta pontuação mas talvez pensando para trabalhos futuros sabe porque eu acho que realmente isso aí poluiu o dataset pode est com vários problemas que não gerou zero Mas que que vai baixar aquela pontuação né sim sim é possível eh só eh a gente tem muita informação que pode colocar aqui claro né mas a restrição de tempo nos atormenta um pouco né Eh então assim sobre o Mistral bem rapidamente a ti curiosidade caso alguns dos colegas se interesse né Ele é um modelo feito com licença da par eh ele tem ele foi treinado a especialidade dele é gerar código porém ele teve performances muito boas em outras outros domínios também Existem algumas variações desse modelo para chat para instruct eh enfim esse modelo tá bem Popular mesmo porque ele é um modelo eficiente e capaz de rodar no infraestrutura no no no no infraestrutura de de casa assim praticamente que foi o caso a gente rodou aqui no meu notebook por exemplo eh e a gente eu encontrei alguns trabalhos que criticam o uso do Blur inclusive para essa tarefa atualmente porque a llm ela faz eu eu aí aí é uma off the record digamos assim né algumas tradições que de qualidade eh baixa digamos assim elas apenas não obedeceram digamos assim palavra a palavra sentença a sentença mas no

- *Corpus ID:* 8663
- *Score:* 0.8542735576629639
- *URL:* oculto
- *Início:* 01:13:10
- *Fim:* 01:15:44
- *Transcrição:* dizer uma é uma um trans modelo um pouco mais eh elaborado né e e e aí fazer uma segunda passada tá bom quanto a relação às dificuldades a gente teve realmente demorou muito tempo aqui caiu várias vezes por conta do tempo de processamento tá E e a gente entende também gostaria do P das senhoras né se eh aqui a comparação o modelo euclidiano e poeno né seriam eh interessantes para avaliar a qualidade do texto gerado tá então esse seria o nosso trabalho eh e seria a conclusão se você quiser encerrar já é uma já AC umas questõe zinhas né uma questão né que a gente tava explorando também era trabalhar com D né para eh eh otimizar né esse processamento tá muito demorado né e assim o o A grande questão é o que interessa pra gente é o que eles e e e o case ali né É é é bem adequado é a sumarização extrativa né então assim eh a gente não tá querendo pegar uma ideia né E resumir tá e e e estabelecer né esse resumo com com base na ideia o que a gente quer realmente é processar o texto extrair com informações do texto né tentar montar um resumo e ver a capacidade né que os modelos têm de fazer isso daí a partir daí eh outros trabalhos né out outras visões né o o PST tagen mesmo né e eh enfim a a a sumar eh eh trabalhar com a simplificação né pode ser podem ser etapas posteriores mas assim o que realmente era o o o escopo né do do que realmente resolve o nosso problema é fazer essa sumarização com texto fração de texto e avaliar o desempenho né disso daí isso daí seria adequado para se colocar em prática no no nosso trabalho é isso aí e obrigada eu fiquei com uma dúvida vocês fizeram Esses scores são comp do resumo gerado com resumo de referência é isso esses scores de cosseno isso isso a comparação aqui e vocês não quiseram usar o Rouge que é a métrica padrão para fazer esse a comparação eh eu não eu não não é que não que a

- *Corpus ID:* 8670
- *Score:* 0.8536176681518555
- *URL:* oculto
- *Início:* 01:26:46
- *Fim:* 01:29:22
- *Transcrição:* ficaram bem abaixo como essa daqui que vocês estão vendo aqui no achurado na na linha dois esse id2 aqui a gente viu que a tradução a nova tradução ela ficou muito pobre ficou até estranha assim ele traduziu tão querido a outra foi eh é um bebê então a gente viu que realmente podia haver grandes discrepâncias entre as traduções né aqui ao lado a gente tem esses gráficos um histograma e um gráfico de barras que estão no nosso relatório né devido à limitação de tempo a gente não teve como eh colocar muita coisa aqui até porque não é a proposta aqui da apresentação enfim aí correndo aqui para os os nossos experimentos aqui a gente tem o os nossos desempenhos gente só que o alarme acabou de avisar que deu 5 minutos Mas vamos lá calma que vai dar certo então Eh nós vimos que a com relação a a os nossos comparativos usando o handle Forest o handle Forest para o o os comentários em inglês se saiu um pouquinho melhor mas é foi um pouquinhozinho mesmo a gente olha aqui que a tradução em português ela pegou uma acurácia de 84 P1 e a inglês foi 84.65 né então a gente chegou a essa conclusão que foi um pouco melhor aem inglês mas não não se sobressaiu muito colocamos esses essas matrizes de confusão que evidenci o trabalho os resultados do do experimento e eles também vão estar lá no nosso no nosso relatório final continuando aqui aqui é só para constar a a gente achou interessante eh gerar também uma nuvem de palavras a após o experimento do tanto do do Random Forest em português como do Force em inglês partindo agora para o novo comparativo do Dos comentários em português agora usando o algoritmo svm a gente viu que o svm ele deixou a desejar em relação ao rendow Forest rendow Forest teve uma Acácia de 84% já o svm caiu deixou assim ficou ao nosso ver a

- *Corpus ID:* 8421
- *Score:* 0.8525636196136475
- *URL:* oculto
- *Início:* 01:23:27
- *Fim:* 01:25:58
- *Transcrição:* fica bem livre né não precisa ser um um dataset anotado normalmente qualquer texto que você tiver né um conjunto de e-mails um conjunto de reclamações Você já consegue trabalhar né É mas para pro trabalho a gente precisa avaliar Então como é que como é que tu faria a avaliação por isso que aí precisa do dataset anotado da coleção de teste pod cal da modelagem de tópicos entendeu A modelagem de tópicos é um caso a parte cling é um caso a parte mas as as outras as as tarefas em que que a gente precisa calcular uma métrica que tem que ser baseada num num a gente chama de Ground truth que precisa ser precisa do resultado esperado tipo um gabarito preciso do gabarito para poder avaliar análise sentimento classificação e Recuperação de informação também mas existem coleções de teste que que podem ser usadas então tem a do Glasgow herold do La times em português tem essa da Folha de São Paulo é porque no caso o que vocês querem é que no final a gente faça um tipo de métrica mostrando o que isso o a técnica que a gente aplicou teve tanto por cento de acerto isso isso o teu trabalho é análise de sentimento né Carlos é como eu tá dizendo agora eu acho que talvez eu mude né Eu gostei dear esse tópico de hoje tá se tu quiser uma coleção em português tu me avisa ali me manda uma mensagem no no teams que eu que eu te encaminho a coleção do da Folha de São Paulo pronto tá ótimo obrigado que pode ser usada é que assim a a não ter um Data Set anotado uma coleção de teste daí implica muito trabalho braçal e que a gente não não precisa ter nesse nesse momento Aí sim o trabalho Fica dá muito trabalho bom dia professora aqui Bom dia Antônio alísio falando eh na nossa proposta né Nós eh é aquela proposta de fazer tradução usando um um modelo generativo né llm


**Resposta gerada pelo LLM**: A disponibilidade de corpora (textos em inglês e multilíngues), como a Wikipédia e o Common Crawl, possibilitou o salto de qualidade na tradução automática.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: P

**Answer Relevance (AR)**: 0.67

**Anotação automatizada**: P

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address the main factors contributing to the improvement of machine translation quality. Supporting Evidence: The RESPONSE identifies the availability of corpora as a key factor in improving machine translation quality, which directly addresses the PROMPT's request for the main factors contributing to this improvement. However, it only mentions one specific factor and does not provide a comprehensive overview of other potential factors that may also contribute to the enhancement of machine translation. Therefore, while it is relevant, it does not fully answer the entire PROMPT.  Score: 2


---
