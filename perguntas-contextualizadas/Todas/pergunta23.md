**Pergunta 23**: Qual é o problema que surge quando se tenta treinar um agente para se comportar de forma ótima em um processo de decisão de Markov que pode mudar? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 4272
- *Score:* 0.8772170543670654
- *URL:* oculto
- *Início:* 01:28:10
- *Fim:* 01:30:36
- *Transcrição:* né e é bem difícil para para um agente lidar com isso então normalmente né pelo menos essa discussão Inicial Nossa a gente vai supor né que o agente vai ser treinado especificamente para um único jeito aqui né de de um único mundo né se mudar o mundo pode ser que precise retreinar o agente é é difícil não é tão simples de transferir né o conhecimento de um mundo para outro mundo eh Então a gente vai nesse momento se simplificar um pouco a vida do agente de forma que ele não precise né se preocupar com eh mudanças né no processo processo de decisão de marcov então a gente ele vai eh como se fosse um overfitting né ele vai ser especialista naquele processo de decisão de marcov pros nossos propósitos isso já vai ser suficiente aqui ok Eh agora são 10 E5 vamos daqui a pouco né na 10 e 10 a gente faz um intervalinho aí Ah tá até aqui ó representação como um gráfico Então deixa eu copiar se já tava previsto de fazer isso aqui foi bom ter feito aqui em cima porque aí eu conseguia ver né o desenho ali então vou copiar recortado aqui e colar no outro slide al aí Aqui tem mais espaço beleza e claro né aqui tá incompleto viu gente tem ação de ir para é ó de para baixo e aqui a a direção que eu tô desenhando a seta não importa tá é só eh tô rotulando aqui o rótulo estando correto né E aqui esquerda é ação de para baixo eu fiz com a setinha para cima mas não tem problema mas aí né para completar esse desenho teria que completar aqui o que que acontece quando o a gente vai para baixo né o que que pode acontecer e aqui à esquerda também completar né o que que pode acontecer então tá incompleto aqui esse desenho vou deixar os três pontinhos aqui para simbolizar o

- *Corpus ID:* 4247
- *Score:* 0.8708701133728027
- *URL:* oculto
- *Início:* 00:41:45
- *Fim:* 00:44:00
- *Transcrição:* tenham isso em mente né uma ação no presente pode afetar resultados eem um futuro distante e do ponto de vista do ambiente pode ser que eu avalie só esse futuro distante aqui né para o agente aprender a a se comportar tem temos uma dúvida pode falar oi oi Anderson Bom dia é assim eh eh Nessa situação você tem eh uma recompensa somente no final e ele tem um tempo por exemplo para para fazer milhões ou como você falou bilhões de testes Mas dependendo da situação eh será que pode ser que você vá dando Recompensas intermediárias porque porque ele pode avançar mais rápido na verdade tô falando isso mas é uma pergunta eh dependendo da situação pode ser que eu tenha que eh eh eh projetar algum tipo de recompensa para que ele Aprenda Mais rápido sem ter que ele aprender todos os passos possíveis e só no final e receber observação perfeita né o o a palavra-chave ali de poder computacional eh tem tudo a ver com com com com essa pergunta com esse comentário até né porque tá certo né não é é mais que uma pergunta é um comentário correto então a decisão né de de como que eu vou projetar né A recompensa aqui no ambiente tem que pensar se eu tenho poder computacional para treinar né o o algoritmo ali o algoritmo tá pronto né mas ele precisa interagir com o ambiente para para saber né para se comportar bem então eu eu tenho recursos computacionais suficientes para treinar esse esse a gente se eu tiver então eu posso né dar a recompensa digamos definitiva né da desse que é a tarefa mesmo no caso aqui do jogo eu só preciso dizer se o agente ganhou ou não mas se eu não tiver aí eu vou precisar de um pouquinho mais de cuidado para projetar recompensa para realmente né guiar o agente de forma que ele tenha né pontos aqui mais eh constantes de avaliação Para que ele não precise jogar toda uma partida né E só só no final lá que ele vai saber se aquela eh sequência de

- *Corpus ID:* 4484
- *Score:* 0.8704701662063599
- *URL:* oculto
- *Início:* 01:00:16
- *Fim:* 01:02:35
- *Transcrição:* né aqui mesmo que no caso anterior né O Agente tá errado aqui né Ele vai morrer de ir para baixo mas ele tá decidido né então ele tem Grande Chance mesmo de ir paraa ação que ele acha que é melhor já que o ag gente tá muito indeciso e se a gente fosse fazer Epson guloso em cima disso né o ag gente ia tomar uma decisão muito drástica de 90% das vezes para baixo né então a o fato de haver o soft Marx ajuda a modelar talvez de uma maneira bem direta né A questão de decisão versus indecisão é uma vantagem né do soft Marx mas né quando o ag gente tá Decididamente errado pode ser uma desvantagem Mas enfim foi para ilustrar aqui o caso né de de uma os potenciais nas preferências estarem mais equilibradas e isso gente de só só para completar eu já abro para PR pra dúvida no início do treinamento do agente é legal que isso ocorra aqui ó a gente digamos está indeciso Para justamente né ele ter boa chance de escolher todas as ações né ao invés de de um é ali de 90% das Às vez ele escolher só uma delas né então a política soft Marks é interessante Nesse sentido porque ela favorece bastante a exploração no começo do do treino do agente pode pode falar dúvidas eh acho que eu me perdi aqui em algum momento mas é o seguinte nas aulas anteriores a gente havia dito havia dito que se a esperança de uma determinada ação fosse um pouquinho maior que que as outras 100% das vezes ele ia o agente ia Tom aquela ação porque eh a recompensa maior digamos assim né OK e agora a gente tá falando já em probabilidade ou seja dependendo do do do do valor Aí ele vai 6% das vezes tomar aquela ação e não ação que tem um valor maior ele vai todas as vezes né Eh eu não sei onde foi que eu me perdi mas que mudou um pouco a filosofia entendeu isso tá então aqui ó eh antes né do a gente tava aqui no nesses nessa nesse paradigma né Eh de da política C ela tem um éon tá não é que é 100% das vezes % das vezes ele não pega

- *Corpus ID:* 4242
- *Score:* 0.8695549964904785
- *URL:* oculto
- *Início:* 00:33:22
- *Fim:* 00:35:35
- *Transcrição:* se você tiver já pronto um algoritmo e realmente já existe o seu trabalho vai ser modelar né o ambiente descrever esse ambiente para que o agente Quais são as percepções né que o a gente pode fazer Quais são as ações dele e cada ação que ele faz como que o que o ambiente muda é exatamente isso tá o o matematicamente falando é o processo de ção de marcov é descrever Exatamente isso quais são as possíveis situações do ambiente quais jeitos né que que o o ambiente pode estar né tem a ver com as percepções do agente Quais são as ações que o a gente pode fazer e quando a gente faz uma ação Como que o ambiente muda e gera a próxima percepção do agente é isso e matematicamente então pensando nesse seguinte na seguinte ideia tá eh e em aprendizado por reforço né antes de entrar ali na matemática do processo de ção de marcov isso aqui é muito interessante ó o feedback né o o que que o agente recebe é uma avaliação da da agente dele um sinal né sobre a ação que ele fez então pensa assim tá aprendendo a jogar basquete E aí você vai tenta arremessar a bola numa cesta aí a bola cai um pouco abaixo da cesta lá então você é uma avaliação né você tem essa avaliação a bola Foi muito baixa mas você não tem né não é que vai ter um oráculo que vai ser materializar no meio das nuvens e dizer ó seu ângulo do seu braço tinha que ter sido 45º e a força de de de 15 newtons para empurrar a bola isso essa instrução né é o feedback instrutivo né seria como se fosse é um aprendizado supervisionado né alguém te disse exatamente qual é a ação correta e esse é o caso do aprendizado supervisionado aqui em aprendizado por reforço a gente não tem isso a gente tem só uma avaliação um sinal né de como que a ação foi feita a gente não sabe qual é a ação correta não sabe E aí um um E aí do ponto de vista do agente de aprendizado por reforço é um pouco ainda mais complicado do ponto

- *Corpus ID:* 4241
- *Score:* 0.868367075920105
- *URL:* oculto
- *Início:* 00:31:45
- *Fim:* 00:33:56
- *Transcrição:* sei lá 40 provavelmente né ação de comprar seria a melhor né então cada hum cenário cada processo de decisão né tem seus números e a sua digamos política né o seu jeito de agir associado e a gente vai ver qual é a matemática que permite modelar todos os processos de decisão de marcov hã por qu né a gente tá nesse paradigma aqui né de interação do agente com o ambiente e o processo de decisão de marcov modela justamente aqui é o ambiente né então vamos supor tá supondo que já existisse aqui então um algoritmo de aprendizado por reforço e já existe mesmo né existe umas bibliotecas de Python muito boas de aprendizado por reforço que tem um monte de algoritmo lá já Pronto né então você tem uma bandeja de prata o algoritmo que vai treinar e aí você tem um cenário né Eu quero modelar o mercado ficeiro ou eu quero modelar um jogo e o quero que o algoritmo aprenda a jogar você precisa modelar esse ambiente essa regra aqui então obedecendo nessa interface num jeito que o agente né o algoritmo consiga usar e essa interface do ponto de vista matemático é o vou chamar de mdp aqui que é em inglês tá markov decision process então o processo de decisão de marcov né ou em português aqui né processo de decisão de marcov é justamente lida como modelar o ambiente tá as regras digamos do jogo né que o agente vai estar envolvido então se você tiver já pronto um algoritmo e realmente já existe o seu trabalho vai ser modelar né o ambiente descrever esse ambiente para que o agente Quais são as percepções né que o a gente pode fazer Quais são as ações dele e cada ação que ele faz como que o que o ambiente muda é exatamente isso tá o o matematicamente falando é o processo de ção de marcov é descrever Exatamente isso quais são as possíveis situações do ambiente quais jeitos né que que o o ambiente pode

- *Corpus ID:* 4245
- *Score:* 0.8674972057342529
- *URL:* oculto
- *Início:* 00:38:28
- *Fim:* 00:40:40
- *Transcrição:* pessoal que é mais novo já talvez não esteja Ah se bem que tá né fizeram várias versões novas aí aí tem que saltar l o bigodinho dele aqui tem que saltar numa plataforma né e chegar do outro lado aqui e pegar a moeda né tem a moedinha dele aqui e eu não preciso ficar dando eh por exemplo se eles consegu fazer o salto aqui né eu não preciso recompensar imediatamente esse salto nesse ambiente do Mário ele pode simplesmente a recompensa ser a pontuação na tela lá né Tem um score lá de quantas moedas ele pegou né então ele vai ficar andando aqui na aleatório na na no cenário E aí eu não preciso ficar dando recompensa para ele eu posso simplesmente fazer né O Agente eh maximizar justamente essa recompensa aqui que ele recebe de ele precisa fazer muitas ações essa ideia de decisão sequencial né precisa fazer muitas ações para conseguir maximizar essa recompensa conseguir ganhar ponto né então ele pode é percorrer um longo caminho até chegar aqui sem nenhum sinal de que aquilo foi bom ou ruim e quando ele saltar e pegar moedinha Aí sim essa pontuação muda não lembro quantos pontos vale sei lá mais 10 aí que isso né só mesmo assim é suficiente para a gente aprender nesse comportamento de seguir reto e depois pular e pegar a recompensa né Eh Ou seja eu não preciso est constantemente ali né digamos incentivando a ag gente a fazer coisa certa eu posso só eh avaliar quando realmente é importante né o o o agente a ação final digamos assim né que que vale a pena outro exemplo tá eh aqueles sistemas lá que jogam jogos de tabuleiro né ou xadrez ou go enfim tem um tabuleiro lá com as pecinhas com as pecinhas lá e aí o a jogada é colocar uma peça né então eu não preciso dizer se a jogada foi boa ou ruim na hora que o que o que o computador fez eu nem eu sei se a jogada é boa ou ruim mas eu sei que depois de uma sequência de jogadas se

- *Corpus ID:* 4278
- *Score:* 0.8667318820953369
- *URL:* oculto
- *Início:* 00:00:05
- *Fim:* 00:02:23
- *Transcrição:* certo então estamos aqui para segunda parte da aula 1 então a gente passou por um monte de conceitos né E o principal que a gente tava a gente viu até o momento Então essa ideia né de que o processo de decisão de marcov e tudo mais mas que a gente tem uma questão bem uma divisão né em aprendizado por reforço aqui do inglês né do agente e do ambiente né e a gente tava trabalhando no momento então estávamos trabalhando com o ambiente né o processo de decisão de marcov que é a matemática né Por baixo dessa definição dos Estados ações transição e recompensa e agora a gente tá voltando a nossa atenção para o agente né o comportamento dele então né A partir Como que o agente pode se comportar para maximizar aquela ideia de uma sequência de Recompensas né Não só recompensa imediata mas levando em conta que uma decisão tem Impacto né a longo prazo como que ele pode se comportar então a gente definiu a política que é justamente a ideia de que para cada estado tem as ações que o a gente quer fazer né E com a probabilidade de cada um um exemplo é em cada estado ele sempre querer fazer a mesma ação que é por exemplo que tá acontecendo aqui então isso é uma política válida né que mapeia qualquer estado para ações exceto os estados terminais né o estado terminais não tem ação né então aqui realmente não tem sentido desenhar nenhuma setinha para onde a gente iria querer ir porque aqui termina a interação dele com o ambiente e aí agora Existem várias maneiras né do agente se comportar no ambiente essa é uma delas outra por exemplo né desenhar de vermelho aqui seria o a gente agir né mapear a cada estado para essa setas em vermelho aqui né E talvez essa outra maneira seja faça menos sentido ou seja pior Enfim então

- *Corpus ID:* 4253
- *Score:* 0.8661031126976013
- *URL:* oculto
- *Início:* 00:51:41
- *Fim:* 00:54:11
- *Transcrição:* gente tem que aprender a maximizar uma sequência de Recompensas né E pode ser o caso por exemplo né de ã por por alguma questão do do projetista lá eh alguma ação né ser recompensada negativamente mas ao final ela por exemplo se ele tomasse outra ação aqui ação sei lá a linha um aqui outra ação alternativa né que fosse tivesse uma recompensa maior né chamar aqui de mais 20 né aí ao final de tudo aqui ele recebe 200 mais 200 e aqui ele recebe Men 100 ou seja se eu fosse olhar só a recompensa imediata eu não ia querer fazer essa ação A1 aqui porque ela é pior do que a alternativa né a alternativa valia já me dava 20 de de recompensa imediata e essa outra me deu men1 mas né a sequência aqui ó me levou a ganhar 200 né E essa sequência aqui me levou a ganhar Men 100 então Eh o agente vai ser capaz de pensar na sequência de Recompensas né então combinar essa sequência de Recompensas em uma em um valor né agregado dig digamos assim de retorno então esses daí são os algoritmos do quem faz Day trade né ah é o mais um menos um aqui né é Pode pensar dessa forma né na visão micro e na visão macro n então Eh o agente de aprendizado por reforço né Tem tem que aprender de uma forma macro né a maximizar a recompensa dele né ou seja diferenciar gratifica instantânea do valor de uma coisa a longo prazo e a gente vai aprender todo esse mecanismo também né E quando a gente tiver pensando né no no na parte da Matemática aqui a gente vai est vai vai ver né isso bem de uma maneira bem clara agora sim né chegamos a definição aqui do processo de Decão de marcov né em inglês marcov decision process Mark of decision process esse nome aqui é é um matemático Russo tá Andrei markov lá do do começo dos anos 1900 né do século XX ou talvez até antes não lembro agora e tem um monte de

- *Corpus ID:* 1086
- *Score:* 0.866065502166748
- *URL:* oculto
- *Início:* 00:47:09
- *Fim:* 00:49:22
- *Transcrição:* Ah é um problema de análise sentimentos é um problema de detecção de posicionamento tá aqui um monte de dados de treino faça o modelo que vocês quiserem a gente vai testar o melhor modelo separando um conjunto de dados aqui que vocês nunca viram na vida e aí a gente vai dizer quem é que vai ganhar a competição numa empresa pode ser justamente um critério de aceitação ou um benchmarking para você saber em quão bom é um modelo de vocês tá bom uma outra técnica que se usa bastante é chamada técnica de roubouch mas aí ele tem que ficar claro sempre tem um de treino sempre tem um de teste a de Round eu pego os meus dados como um todo e eu reservo uma percentagem deles para testar recomendada razões típicas aí vai depender da quantidade de dados que eu tenho tá mas tipicamente é 7030 ou 80 20 Tá mas não menos do que isso senão a gente mas assim a quantidade de dados sobre o qual a gente testa é muito pequeno e portanto a nossa capacidade garantir que a generalização é boa é limitada bom é de novo importante que a gente preserva as propriedades dos dados nos dois conjuntos porque eu não posso fazer uma divisão aleatória eu entrego dados para treinar onde né tem por acaso toda uma característica temporal por acaso né E vai testar a circunstância completamente diferente se tem para vocês alguns exemplos na nossa disciplina é não adianta eu pegar os dados das eleições da Dilma lá em quando ela ganhou a primeira eleição que tava todo mundo né Muito feliz com o governo e etc e tal e feliz que era a primeira mulher que tava concorrendo a presidência é assim por diante e testar né a capacidade desse modelo na de uma oito anos depois onde ela tava com né todo um problema de implicação os termos que são usados as fichas que foram usadas para dizer esse sentimento

- *Corpus ID:* 4393
- *Score:* 0.8647998571395874
- *URL:* oculto
- *Início:* 00:06:52
- *Fim:* 00:09:10
- *Transcrição:* por fazer aquela ação ação de pra direita nesse estado E verificou né que na verdade o valor dessa ação é um pouquinho maior ele porque ele ganha uma Recompensas minha negativa lá o os 0.04 mas né ele chega num estado que tem né uma perspectiva melhor aqui né de de conseguir um valor melhor E aí por isso o valor aqui sobe né na direção da de atualização correta e é o que a gente viu né quando executa ali o o agente Solto No Grid ali ó por por 10 iterações né ele vai atualizando as a recompensa o que ele acha que vai ganhar né estimativas de valor E aí quando ele cai num estado terminal né tanto O negativo quanto Positivo já fica mais claro para o próprio Agente né quais ações que levam aquela consequência ruim ou aqui em cima né que levam para consequência boa e aí que ele ainda no E olha que como que é isso é legal de ver como que é lento né o aprendizado no que learning porque olha só o aprendizado a partir de onde tem recompensa que aí o ag gente começa a propagar né aqueles valores maiores de recompensa para os outros lugares então o estado terminal né o valor dos Estados terminais demora para ser reconhecido ali no início e isso é importante a gente observar né que é um uma limitação dessa abordagem do que o learning e a gente vai nós vamos né tratar disso também no restante aqui da da aula hã Professor uma perguntinha Não na verdade é um comentário no tem um padrão para esse para para essa formatação de linha e coluna porque no Exercício lá acho que é o o a atividade três eu fui fazer eu errei porque tava 3S do e a gente tá vendo o primeiro como linha e só que lá é o contrário lá o três é a coluna E aí é certo que lá no exercí tá falando né coluna três eu que também não prestei atenção mas atrapalha um pouco assim assim quando o senhor fala que três do aí três é a linha né só que lá no exercício três é a coluna aí atrapalho eu errei um pouquinho as exer


**Resposta gerada pelo LLM**: É difícil transferir o conhecimento de um mundo para outro; pode ser necessário retreinar o agente se o mundo mudar.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: P

**Answer Relevance (AR)**: 0.67

**Anotação automatizada**: P

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to get a maximum score of 3. It should address the problem of training an agent in a changing Markov decision process. Supporting Evidence: The RESPONSE identifies a relevant issue related to the training of an agent in a changing environment, specifically the difficulty of transferring knowledge and the potential need for retraining. This directly addresses the problem mentioned in the PROMPT. However, it could provide more detail about the implications of these challenges or how they specifically relate to Markov decision processes. Nonetheless, it is relevant to the core issue presented in the PROMPT.  Score: 2


---
