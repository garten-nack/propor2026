**Pergunta 81**: Qual é a diferença entre o OpenCL e o CUDA?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 5435
- *Score:* 0.8854999542236328
- *URL:* oculto
- *Início:* 00:46:28
- *Fim:* 00:49:04
- *Transcrição:* não programa elas com cuda porque cuda é realmente é focado uma API da NVidia né então pra gente programar placas outras placas de outras fabricantes em geral a interface de programação é chamada opencl que a gente pode usar opencl também para programar eh gpus da NVidia certo mas aí o desempenho não é tão qu tá me ouvindo tá me ouvindo Pronto acho que o Alexandre tá com o microfone ligado isso rad1 exatamente eh então assim a gente usa o opencel para programar essas placas da AMD eh mas assim a forma de programar é a mesma entendeu a o o fato de copiar os dados pra placa lançar o processamento lá copiar os dados de volta essa forma de funcionamento é exatamente a mesma em opencl em cuda entendeu então a gente usa cuda porque é domite no mercado digamos assim né pelo menos de super computadores eh e mas que a gente ganha essa intuição para se programar em qualquer placa aceleradora Fala aí Vander Bom dia professor ainda sobre essas placas nvia eh é uma é uma dúvid Zinha que eu que eu tenho e tá com alguns anos principalmente na na na pandemia ela lançou as versões lhr né que tinha eh limitação a limitação à mineração de alguma forma isso o senhor o senhor viu no laboratório de vocês e isso isso chegou a afetar digamos assim o desempenho de vocês ou não é é mais é mais focado essa limitação de mineração mesmo bom Ah tu diz mineração de Bitcoin esse tipo de coisa sim é assim a gente tem um acordo com os nossos usuários que ninguém pode usar o clust para minerar Bitcoin então Inclusive a gente suspende as contas de pessoas que fazem isso certo eh e aí né trabalha [Música] com pode fazer mas as pera aém o Alexandre obrigado e porque daí eu acabo

- *Corpus ID:* 5434
- *Score:* 0.8672025203704834
- *URL:* oculto
- *Início:* 00:44:48
- *Fim:* 00:47:10
- *Transcrição:* onde a gente pega um pouquinho de serer só para terminar e aí depois a gente fala eh de numba Então como que a gente consegue levar esses os conceitos que a gente viu lá realmente no baixo nível entre aspas programação em C para numba programação em Python n então aqui daí vocês se vocês têm uma placa aceleradora na máquina de vocês que hoje em dia né computador sempre tem uma placa de vídeo H GPU né Por menor que seja você já consegue tirar proveito dela daí eh no CAB digamos assim num Júpiter notebook local da máquina de vocês né lembrando né que o cab tem H tipos de runtimes diferentes e um deles é o runtime que é um GPU que eles oferecem uma placa muito boa né Nessa versão gratuita só que ela tem um tempo de uso limitado assim pros usuários Não pagos né então a gente vai usar com parcimônia pra gente não eh exaurir digamos assim o o acesso né Mas normalmente acontece comigo Eu normalmente não consigo fazer as demonstrações das aulas porque eu uso bastante de maneira bastante intensa naquela naqueles dias então essa parte normalmente daí eu uso o pcad aqui para poder ter tempo de GPU livre fala Bruno pode pode perguntar em relação à cuda ela é sempre ligado a Nvidia sempre qual que seria o concorrente para MD qual que seria o concorrente assim a AMD tem placas eh GPU também né que são hã como é que era o nome agora não me recordo exatamente nome mas aí a gente não programa elas com cuda porque cuda é realmente é focado uma API da NVidia né então pra gente programar placas outras placas de outras fabricantes em geral a interface de programação é chamada opencl que a gente pode usar opencl também para programar eh gpus da NVidia certo mas aí o desempenho não é tão qu tá me ouvindo tá me ouvindo Pronto acho que o Alexandre tá com o microfone ligado isso rad1 exatamente eh então assim a gente usa o opencel para

- *Corpus ID:* 6412
- *Score:* 0.8527013063430786
- *URL:* oculto
- *Início:* 00:35:10
- *Fim:* 00:37:39
- *Transcrição:* simplificar a programação usando um único espaço de endereçamento entre aspas virtual que eh conecta então a memória RAM do do computador com a memória RAM da GPU ok mas isso tem uma sobrecarga em geral eu não vejo as pessoas usando isso assim nos kernels embora simplifique a programação não precisa alocar memória fazer kudal é outra forma de alocar memórias tá faz com que tu tenha uma camada de abstração extra Ok fechando parênteses então e ainda sobre terminologia né Nós temos os kels que são funções GPU lanadas pelo anfitrião e são executadas no device então a chama isso de kernos Ok ou um k Ok então qual que é a vantagem que o numba traz em relação à aquilo que a gente viu na aula passada e aula de hoje é que ao invés de tu programar em C que seria por vezes talvez desafiador demais OK a gente vai programar em Python Ok para fazer esses kos então o que a gente vai ver agora então é quais são assim princip as diferenças né como que por exemplo a memória compartilhada que a gente viu hoje no in da aula se manifesta nesse universo como que a gente marca uma função para ser uma função que vai se tornar um Kernel é uma função que não vai ser compilada pro host ela vai ser compilada pra arquitetura da GPU e assim por diante Ok então a primeira forma assim que pra gente marcar que é uma função deve ser executada dentro da GPU vai ser o uso desse decorador Deixa eu só aqui F desse decorador a chamado cuda @ cuda.jit Ok então ele é bem similar ao @ jit que a gente acabou de ver só que toda vez que vocês virem @ Git pensa imediatamente em CPU Ok se a gente tá com uma placa aceleradora da NVidia Então a gente vai usar um @ cuda PG Ok e antes que alguém faça pergunta o numba realmente tem um foco bem maior para aceleradores da NV Ok inclusive o

- *Corpus ID:* 6229
- *Score:* 0.8512248396873474
- *URL:* oculto
- *Início:* 00:18:54
- *Fim:* 00:21:01
- *Transcrição:* eles são independentes eles são eles conseguem fazer o cálculo assim independente dos outros cores tá já esses cores da da da GPU não esses cores da GPU eles são extremamente simples né então por isso que eles conseguem botar um monte deles e a principal diferença é que eles não têm controle independente eles não a gente não consegue programar um Core ok a gente só consegue programar grupos de cores e esses grupos o grupo mínimo o mínimo assim nas arquiteturas atuais é de 32 Então se tu tiver uma thread e tu quer executar dentro de uma dessas placas tu vai ter que usar 32 32 cores tu não vai conseguir usar só um Core por vez porque e aqueles outros aqueles outros 31 cores eles vão ficar ociosos eles vão ficar sem fazer nada tu vai perder eles então na realidade a programação eh com com cores eh aqui ela é muito mais eh eh de vazão mesmo assim a gente pensa quer fazer uma programação que gera alto volume de de de tráfego memória né e e cálculo né para conseguir justificar e conseguir usar todos esses ces sobretudo Então esse é um exemplo tá pessoal aqui tem um outro que é o fpga que eu já mencionei que daí vocês vão imaginar que essa máquina é Idêntica a única diferença é que a placa ali não vai ser daí uma gpgpu vai ser um fpga e esse fpga é um hardware programável né daí tem todo um uso diferente a gente não vai entrar nesse detalhe mas eh a mesma ideia a gente vai ter o endereçamento independente que é o modelo host device eh com host device separados e tudo mais eh para para aqueles que lidam com placas eh aceleradoras da AMD né é a mesma coisa também é o modelo host device né programação opencl que a gente vai ver que a gente não vai não vai entrar muito detalhe mas é a mesma é a mesma ideia Ok o endereçamento independente host device H alguém tem alguma pergunta nesse ponto Roberto eu vi que tu levantaste a mão mas depois tu baixou não sei se já já

- *Corpus ID:* 6313
- *Score:* 0.8511137366294861
- *URL:* oculto
- *Início:* 01:01:21
- *Fim:* 01:03:35
- *Transcrição:* sei lá devia ter o qu uns 300 400 MB de dados de entrada que eram as medidas tá do problema e tal mas o cálculo que é feito em cima desses dados é muito muito cálculo é muita é muita é muita aritmética de números complexos entendeu então envolve muita muita multiplicação muita divisão muita soma então então assim é é difícil te dar uma resposta que sirva de parâmetros de comparação com outros problemas eh buen eh então voltando aqui pros pro colab ã Então assim Então a olha só ele disse ó warning você está conectado com um runtime GPU mas você não está utizando a GPU Então vou executar aqui de novo antes que eles cortem a minha Instância aqui né então assim Tem mais informações sobre a tela T4 aqui no próprio site da NVidia né então deixo aqui o link para vocês terem uma noção de qual é essa placa é uma placa que tem um um consumo energético relativamente pequeno aí que tem o dat sheet dela com ações talvez de cud cores esse tipo de coisa que tem um monte de informação aqui deixa eu ver se eu consigo dar um zoom aqui e 5% vamos lá aqui ó é uma placa que entrega entrega 65 teraflops em precisão mista e precisão simples que é ponto flutuante simples 8 tflops e aqui tá a quantidade de kud cores 2560 cud cores tem nessa placa aqui aí tu pode mais ou menos imaginar assim que cada cuda eh cada SM vai ter 128 aí tu divide esse número por 128 tem uma ideia de quantos SMS tem ali dentro né OK então assim indo adiante aqui no nosso na nossa então assim pra gente fazer a os testes aqui a gente vai eh como a gente vai est programando em C né a gente precisa instalar um Plugin do do do Júpiter aqui para que a gente possa eh automaticamente Criar kernels scuda e usar o compilador nvcc que já está instalado Então esse aqui vai permitir a gente dar Play num bloco de código do

- *Corpus ID:* 6373
- *Score:* 0.85109943151474
- *URL:* oculto
- *Início:* 01:04:50
- *Fim:* 01:06:56
- *Transcrição:* exemplo é um bom questionamento é eh por exemplo a a Nvidia ela no passado ela tem várias gerações né a Nvidia ela nasceu como uma uma empresa de gamers né ela começou fazendo placas GPU para gamers para joguinhos né para funcionar bem Chegou momento que a galera começou a deturpar essas placas porque eles perceberam que dava para usar ah openl né não sei se vocês já tiveram a oportunidade de programar em jogos mas tem opengl que se usa muito né que é aquela forma de tu fazer gráf né então opengl pessoa começou a usar opengl para fazer procedimento de Alto desempenho então tu programava em opengl E aí eles se deram conta disso e disse não mas vamos fazer então uma API de programação Então quem vem dessa dessa vertente que já sabia programar em opengl e vai para cuda eles não veem nenhuma dificuldade porque eles já faziam isso com os milhares de triângulos que tinham que ser renderizados para tu construir a imagem que vai pr pra tua tela lá do teu jogo entendeu então o que que aconteceu a ví ela foi para essa vertente e abraçou HPC entendeu E aí surgiu cuda com o tempo H eles dominaram esse mercado assim hoje em dia né acelera isso por muito tempo eles dominaram bastante hoje tá tem bastante competição de novo né Mas o que aconteceu foi que surgiu a inteligência artificial de novo e voltou essa Vibe aí né com as redes Profundas né que vocês já viram em disciplinas anteriores que a gente vai ver de novo agora na CD 010 né E aí eles abraçaram inteligência de ial E como que eles abraçaram Inteligência Artificial sabem sabem aqueles cud cor ali que a gente viu esses cores aqui que estão dentro dos SMS então cada SM hoje tem os kaces tradicionais que são feitos para aritmética normal mas tem kud cores que conseguem fazer então operações específicas de treinamento de machine learning entendeu então tem aqueles cud cor que são específicos para fazer multiplicação de

- *Corpus ID:* 6234
- *Score:* 0.8503984212875366
- *URL:* oculto
- *Início:* 00:26:47
- *Fim:* 00:29:04
- *Transcrição:* o Tool kit cuda né para programar essas placas mas isso aí é daí em software daí né Não tem absolutamente mais nada em hardware hardware É só colocar a placa ali né luí Guilherme pode perguntar Professor eh eu fiquei com uma dúvida quando você fala cuda cors e eu eu tenho uma formação eu entendi assim um ambiente uma arquitetura Risk uma arquitetura padrão anterior tipo 286 386 quando você fala C cor tá falando no nível de firmware ou é realmente o o do tipo a placa vem com algo dentro dela que especifica o cud Core que seria para mim imaginaria um firmware Algo que estaria é numa camada intermediária entre o hard e a nossa aplicação eh seria isso não os cud cores eles são físicos entendeu ou seja lá no hardware no chip lá da placa tem lá um tem lá no no no sei lá no no silício lá tem lá uma representação daquele cuda cor então é uma coisa Física mesmo entendeu como se fosse um risque né você tivesse uma arquitetura tem uma arquitetura Exatamente é uma arquitetura cuda entendi tem uma arquitetura daí arquitetura que a gente chama de arquitetura cuda a gente pode dizer assim Entendeu perfeito é arquitetura da iní a gente chama entende então assim não não é uma coisa não é uma não é uma abstração isso aqui entendeu é fisicamente mesmo tem 3584 unidades cores de processamento só que eles não são independentes né A gente vai ver asas figurinhas em seguida beleza vamos adiante então então assim como é que funciona né quando a gente quer usar essas placas né a gente vai ter normalmente um Core CPU ou seja um daqueles cores lá que eu tenho oito tá um daqueles cores da CPU eles vão ficar dedicados para um deles para cada uma das gpus Ok e esse Core da CPU ele tem uma responsabilidade de alimentar a placa fazer a placa funcionar Ok então como é que isso acontece é sempre essas

- *Corpus ID:* 6372
- *Score:* 0.848219096660614
- *URL:* oculto
- *Início:* 01:03:07
- *Fim:* 01:05:25
- *Transcrição:* mas bom daí isso aí já é outro assunto enfim H beleza H então tá então tá pessoal acho que era basicamente isso não sei se vocês têm alguma pergunta sobre essa parte de programação cuda etc eu eu entendo eh compartilho de vocês a preocupação assim que bom eh realmente muito baixo nível né Mas eu insisto tá que é importante a gente entender bem como é que funciona essas coisas tá porque eh para usar a cuda e não assim a gente até vai ver na sexta-feira e na terça-feira da semana que vem ah métodos assim mais alto nível assim que a gente usa placa entendeu mas para se assegurar que esses métodos estão efetivamente usando bem a placa é importante saber como ela funciona entendeu porque a a maior inquietação que surge é é quando tu tem lá o teu workflow que usa um midle que tem suporte para GPU quando tu tá executando o teu workflow eh tu tá esperando né tu vai lá olhar a placa e a placa não está sendo usada 100% entendeu aí bom o que que tá acontecendo entendeu então é nesse caminho desses detalhes aí que é onde fica realmente daí a a explicação para em geral o mau uso da GPU Antônio alí pode perguntar eh professor o senhor mencionou o uso de asic como uma alternativa mais eh eficiente do ponto de vista de de energia elétrica processamento de Bitcoin né o exemplo que o Senhor deu é eh isso também é verdade para tarefas de machine learning treinamento por exemplo é um bom questionamento é eh por exemplo a a Nvidia ela no passado ela tem várias gerações né a Nvidia ela nasceu como uma uma empresa de gamers né ela começou fazendo placas GPU para gamers para joguinhos né para funcionar bem Chegou momento que a galera começou a deturpar essas placas porque eles perceberam que dava para usar ah openl né não sei se vocês já tiveram a oportunidade de programar em jogos mas tem opengl que se usa muito né que é

- *Corpus ID:* 6411
- *Score:* 0.8476115465164185
- *URL:* oculto
- *Início:* 00:33:28
- *Fim:* 00:35:46
- *Transcrição:* hora não pessoal Segue o jogo Então vamos lá bom então era isso sobre a atividade dirigida número 20 e aí agora a gente vai então se encaminhar pro final da aula então a gente vai ver então o último material que a gente tem para hoje que é o numba para programar in cuda então isso aqui tem bastante a ver com o que a gente viu na aula passada e no início da aula de hoje ok então agora a gente volta a falar de gpus oken então PR GPU né então Eh especificamente para kuda tá então o modelo de programação vai ser exatamente o mesmo então tudo que a gente viu antes eh sobre essa questão do anfitrião e do device vai acontecer aqui também então a gente vai ter exatamente a mesma terminologia o host é o anfitrião que é a CPU com a sua memória com a sua seu barramento PCI e o device é a GPU com sua memória com seus processadores stream processors lá e que está conectado no HOST por intermédio do barramento PCI então nós temos a memória do host e a memória da placa e elas são duas coisas completamente separadas dois espaços de endereçamento completamente separados Talvez uma coisa que vale a pena mencionar aqui que eu não mencionei lá como a gente viu a aquele assunto é que existe nessa na api cuda vocês vão ver que se vocês forem pesquisar que vai aparecer um negócio chamado unified Memory que é memória unificada em português que foi um esforço né que ainda existe na realidade de tu simplificar a programação usando um único espaço de endereçamento entre aspas virtual que eh conecta então a memória RAM do do computador com a memória RAM da GPU ok mas isso tem uma sobrecarga em geral eu não vejo as pessoas usando isso assim nos kernels embora simplifique a programação não precisa alocar memória fazer kudal é outra forma de alocar memórias tá faz com que tu tenha uma camada de

- *Corpus ID:* 6337
- *Score:* 0.8452096581459045
- *URL:* oculto
- *Início:* 00:06:38
- *Fim:* 00:08:55
- *Transcrição:* melhor placa da NVidia a melhor placa das concorrentes a Nvidia hoje em dia é realmente a fabricante que que consegue entregar o melhor desempenho digamos assim né por isso que a gente vê o e essa api aí mas o que eu quero ressaltar é que de novo né É que esse raciocínio que a gente viu aqui nesse exemplo aqui por exemplo ver que tá aqui esse aqui por exemplo onde a gente tem que calcular dentro do nosso kerno Ok temos aqui o kerno temos a entrada A e B e queremos gerar uma saída C Ok e onde nós temos que aqui dentro desse kerno calcular um índice que vai ser único para cada threads E aí depois fazer o uso desse índice para fazer o que a gente quer em paralelo então nota que isso aqui a gente vai ter um monte de trads fazendo isso aqui ao mesmo tempo né esse raciocínio esse método é o mesmo para todas as arquiteturas eh de aceleradoras entendeu que são do estilo GPU Então a gente vai encontrar né esse tipo de construção aqui com algumas diferenças certo quando a gente for programar para MD A gente vai usar opencl Então vai existir vai vão existir algumas diferenças assim de como marcar que o Kernel vai para aquela eh para aquela GPU da MD eh o cálculo do índice vai ser exatamente o mesmo porque esse aqui é um raciocínio eh que eh transversal digamos assim né inclusive eh para CPU também funciona assim né e h a parte do código daí bom em geral em código C né então a gente vai C C mais mais então o código vai ser eh idêntico né então o que vai mudar vai ser a cadeia de compilação né o algumas marcações de como que fica a parte da api talvez a gente tem que usar algumas outras funções para inicializar as funções pra gente copiar os dados não serão Obviamente as mesmas mas existe correspondência aí entendeu de um para um com essas outras eh arquiteturas com essas outras formas de programar com essas outras apis né então dá para


**Resposta gerada pelo LLM**: O CUDA é uma API focada na NVidia, enquanto o OpenCL é uma interface de programação usada para programar placas de outros fabricantes, como a AMD.  Embora o OpenCL também possa ser usado para programar GPUs da NVidia, o desempenho não é tão bom. A forma de programar é similar em ambas as APIs, envolvendo a cópia de dados para a placa, processamento e cópia de volta dos dados.  O CUDA é dominante no mercado, pelo menos em supercomputadores.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately explain the differences between OpenCL and CUDA. Supporting Evidence: The RESPONSE effectively addresses the PROMPT by clearly stating that CUDA is focused on NVidia while OpenCL is used for multiple manufacturers, including AMD. It also mentions the performance differences and similarities in programming approaches between the two APIs. This comprehensive explanation covers the key aspects of the differences between OpenCL and CUDA, fulfilling the requirements of the PROMPT.  Score: 3


---
