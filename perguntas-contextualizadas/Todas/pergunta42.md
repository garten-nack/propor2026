**Pergunta 42**: Como o TensorFlow organiza as tarefas de treinamento em um grafo de tensores?

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 6812
- *Score:* 0.8816606402397156
- *URL:* oculto
- *Início:* 00:48:22
- *Fim:* 00:50:28
- *Transcrição:* dessas execuções eh Tem uma função talidade adicional que a gente talvez vai comentar bem rapidamente que é o grafo de tensores eh que permite a gente observar o o grafo de tarefas né que é o gráfico de tensores que está por trás do treinamento né pra gente poder Observar se tem algum tipo de problema de desempenho enfim analisar esse tipo de de informação Fala aí Antônio alí eh duas coisas e primeiro é que a questão do callback uhum antes né eu vendo aqui na documentação que você pode escrever eh callback customizado né para sua necessidade então isso exato você poderia criar lá alguma rotina de envio para algum com3 da vida para Bucket isso é poderia fazer dessa forma também é isso aí a segunda é que é uma é uma dúvida eh o tensor Word Eu lembro que na disciplina do Professor Anderson é como é o nome da disciplina dele esqueci vale mas professor do Anderson isso aprendizado profundo ele teve acho que umas duas disciplinas ou três com vocês né uma delas acho que foi aprendizado profunda eh eu lembro de ter sido eu não lembro se ele falou ou eu encontrei na internet o ponto é que eu na época eu tava tava tava um pouco tava a aparentava estar abandonado o desenvolvimento e eu isso procede doens do tensor board quer dizer isso do tensor board especificamente é o o tensor board digamos assim que ele faz parte do tensor Flow entendeu ele vai junto com o tensorflow então tipo como o tensorflow tá ativo né eu entenderia que o tensor board continua ativo entendeu Mas não tem informação assim específica se se o pessoal o que eu observo é que tipo é uma é uma forma assim eh bem simples de tu analisar os dados

- *Corpus ID:* 6697
- *Score:* 0.8802908062934875
- *URL:* oculto
- *Início:* 00:34:18
- *Fim:* 00:36:19
- *Transcrição:* exportados em formato portável que é hdf5 então h o modelo quando ele salvo ele é nesse formato D F5 e ele é um grafo também né tipo assim a rede neural é um grafo então a gente consegue eh eh eh executar né executar esse grafo né se a gente tiver o tensor Flow H instalado Tem vários otimizadores para melhorar o treinamento né isso aqui eu não vou entrar em nenhum detalhe nisso aqui então provavelmente vocês já abordaram esses otimizadores diferentes a gente vai sempre pegar um deles assim não vamos se preocupar muito com isso porque eh esses esses otimizadores eles envolvem a operação matemática ali durante o treinamento né pode mudar mas o paralelismo vai ser sempre o mesmo ok Não importa qual que é o otimizador olhando para API de programação tensor Flow ela é muito grande né É realmente Impossível a gente detalhar tudo talvez talvez com o que vocês viram nas outras disciplinas e nessa aqui talvez vocês vão conseguir cobrir mais né mas a gente vai sobrevoar alguns dos assuntos aqui né porque naquela ideia de que a gente não tem como aprender tudo de uma vez de vez em quando a gente aprende as coisas e nem sabe por que é aquilo então vai ser mais sob demanda assim tipo assim ah a gente precisa a gente tem um volume de dados precisamos organizá-las Então vamos lá então vamos olhar o TF data Ah eu preciso fazer alguma coisa específica do k vou lá vou estudar o TF então mas é importante mesmo assim a gente ter essa visão geral porque sobretudo classificado dessa forma porque daí a gente consegue saber o que existe né é importante saber o que existe de vez em quando a gente não sabe o que coisa existe a gente implementa tudo e diz pô já existiu o negócio né então ess tem essa classificação tá a gente vai se focar então mais no TF data que é essa parte no TF distributes que é como que a gente distribui os dados

- *Corpus ID:* 7021
- *Score:* 0.8790091872215271
- *URL:* oculto
- *Início:* 00:13:22
- *Fim:* 00:15:35
- *Transcrição:* porque a gente chama de operações Raízes que são essas folhas que aparecem no grafo eh que elas não dependem de ninguém seria assim a origem né de uma informação então notem que aqui nós no nosso gráfico de tensores nós temem temos nós que na realidade dependem de outros nós então esses não são raiz porque eles precisam eles eles têm uma dependência eles precisam primeiro resolver a dependência deles para eles poderem ser executados então por exemplo o f aqui pro F executar eu preciso ter o abde então o f eu não consigo começar por ele já o a o b e o c posso começar por eles porque não Eles não dependem Depende de ninguém né então o que que ele vai fazer ele vai detectar essas operações raízes e vai escalonar isso né ou ou em CPU ou em GPU em função da da da disponibilidade né E aí o que que acontece acontece em do uma operação de eh eh entre trads e intra trads também então essa essa esse essa operação esse esse a por exemplo ele vai ser enfilado isso vai ser executado eh no numa trad e o que pode acontecer é que essa operação ela é quebrada mais uma vez né para ser executada por mais trads digamos assim a gente precisa de uma aceleração Extra como se a gente usasse mais de uma trade para fazer uma tarefa a chama isso de tarefa eh com múltiplas trads Ok e do lado da GPU a mesma coisa acontece né a gente tem um enamento de tarefas no caso a c aqui foi mapeada para executar na GPU E aí tem a c dnn lá dentro né que vai receber essa tarefa e vai fazer o que tem que fazer lá dentro eventualmente quebrando também em pedaços né vai usar os vários tensor cores ou os vários cud acores Então existe essa ideia de duas filas né Especialmente na na na na tanto em CPU quanto em GPU E aí eh essa primeira fila é a fila para tarefas do grafo então a gente en filera e depois a outra fila é pros pedaços de uma tarefa né então existe esses dois tipos de paralelismos né seria o paralelismo

- *Corpus ID:* 6824
- *Score:* 0.8740182518959045
- *URL:* oculto
- *Início:* 01:06:37
- *Fim:* 01:08:42
- *Transcrição:* pode ser deixa eu ver aqui Ah era beleza daí é isso aí eu acho que é o tipo de coisa que poderia se atualizar sozinho né mas não não se atualiza né daí Enfim então tem que clicar ali para poder ver tudo mas aqui aqui aqui tu podes ver também ó que no no caso aqui dessa última execução especificamente que a gente fez com 20 ó tá eu tô mostrando aqui os dados ó a a parte de Treinamento ó foi até a 15ª e depois h a parte de Treinamento não tem até a a 19ª aqui né da da porque a primeira É zero né então tipo achei estranho também não aparecer ã a linha a linha Rosa aqui no meu gráfico acompanhando a linha verde até o final n deveria a validação foi mais né É mas na é mas se tu olhar a execução foi tudo né Tá vendo 20 treinamentos né então assim provavelmente é porque aqui nos arquivos né se tu olhar lá nos logs a última execução que foi feita aqui né Eh o treinamento esse arquivo aqui ele não deu o refres ele não releu ele mas tá lá a informação entendeu provavelmente entende só que bom talvez isso temha a ver com a pergunta do colega antes ali né Talvez o negócio não esteja sendo mais atualizado aí começa esses probleminhas aí ent Mas de qualquer forma já dá uma ideia né tipo assim é melhor tu ter isso aqui do que tu por exemplo tu vai partir do zero né vai usar um callback teu salvar lá as acura e tudo fazer um gráfico né bom com o que vocês já TM Agora no curso já conseguem fazer tranquilo né com pandas por exemplo mas aqui seria uma tipo assim um mecanismo mais simples Ok bom pessoal então agora a gente vai voltar então pro Firefox aqui paraa d03 para dar andamento antes da nossa essa pausa só para falar rapidamente então sobre eh tensores e operações Ok eh que é nossa próxima atividade dirigida tá então o objetivo dessa atividade didática só a gente entender como é que funciona esses tensores do tensor Flow assim que é basicamente um arranjo

- *Corpus ID:* 7955
- *Score:* 0.8738342523574829
- *URL:* oculto
- *Início:* 00:00:02
- *Fim:* 00:02:37
- *Transcrição:* já já coloquei para rolar aqui rodar o cab porque tem um tem que fazer o download aí o dataset não é tão pequeno Tá mas assim ó o que a gente vai ver aqui é o pipeline completo desde da criação da rede Tá inspirada na unet tá eh e para atacar um problema de segmentação semântica de três categorias então tem esse dataset aqui tá que é o oxf Pets que ele basicamente fornece várias imagens eu acho que é gato e cachorro tá mas ele fornece anotações no nível de segmentação semântica o que que eles fornecem basicamente eles fornecem eh para cada Pixel de cada imagem um rótulo um dois ou três um é o foreground né cachorro ou gato sei lá ou bicho que for dois é o contorno do objeto e três é o background tá então é é assim que o dataset foi anotado em vários casos a gente não tem controle né a gente tá usando a o tipo de anotação que o o dataset tá fornecendo tá tem algumas coisas assim ó pessoal é dentro do tensor Flow sobretudo nas últimas versões tem um monte de coisa eh customizada e eventualmente otimizada toda questão de usar ah os os tfds né que são as as estruturas de dados do tensorflow para treinamento tá que a gente não viu aqui e e de fato nosso objetivo não é o objetivo do curso não é eh destrinchar o tensor Flow até porque eventualmente vocês vão usar até o p thort para outra outra aplicação tá então neste cara esse como esse dataset já estava sendo já foi fornecido como um tens ou datas Tá o que que a gente vai fazer dentro desse cara aqui é uma rotina que vai pegar o dataset no formato dele e gerar os dados para que a gente possa consumir né numa no treinamento de de um algoritmo de segmentação tá então basicamente o que que eu vou fazer aqui tá eh eu vou criar por por definição minha e imagens de tamanho 128 por 128 tá são imagens

- *Corpus ID:* 6864
- *Score:* 0.8732000589370728
- *URL:* oculto
- *Início:* 00:35:10
- *Fim:* 00:37:16
- *Transcrição:* todas ele vai usar todas né ele vai usar todos porque isso vai depender na realidade de como que tu instancia né tipo assim vai ser transparente somente se tu instanciar corretamente esse objeto aqui entende isso a gente vai ver mais para frente daí aí tu vai criar tua tu vai tu vai criar essa estratégia de distribuição vai ter lá uns parâmetros a gente vai ver tá E aí tu vai instanciar o teu modelo vai compilar o teu modelo na hora que tu for fazer Fit o Fit do tensor Flow vai perceber que já existe uma estratégia de Distribuição e daí vai distribuindo todo o treinamento nas várias gpus entendeu Entendi beleza entendeu Ah mas se porventura tu quisesse fazer manualmente que também daria Aí teria que teria que ser através desse método que a gente viu aqui tá bom acho que vamos paraa frente então pessoal mas ao menos que vocês tenam alguma pergunta sobre esse assunto ainda que a gente tem mais uma hora de aula nessa manhã de sexta-feira 24 de Novembro black friday né não sei se vocês estão sabendo mas ah tem esse último assunto que eu queria abordar com vocês vocês estão por aí ainda ou já todo mundo foi embora já amos aqui aqui vamos lá vamos lá então vai ter Black para notas também Professor a gente vai ganhar um bônus hoje não B hoje não hoje tem um tem uma tem uma coisa interessante se você observarem bem né na aqui na nossa aula do dia 24 não tem atividade avaliativa Olha só isso isso já é algo positivo entendeu então já tem a tarefa da aula anterior então não hoje não tem Tá bom então vamos terminar a aula de hoje então nessa última hora aí né talvez a gente consiga terminar um pouco antes mas em geral isso aqui causa bastante perguntas tá eh que é a parte de lidar com dados né então ã que é extremamente importante então a gente viu ali já tem uma ideia de mais ou menos como é que funciona os tensores já sabemos como que instancia como é que usa GPU né já vimos ali como

- *Corpus ID:* 6783
- *Score:* 0.8712804913520813
- *URL:* oculto
- *Início:* 00:03:20
- *Fim:* 00:05:20
- *Transcrição:* desempenho entre CPU e GPU só para você terem uma ideia como que o tensorflow pode ajudar para você vocês para atividades cotidianas que não necessariamente envolve treinamento de uma rede profunda tá E no final da aula hoje a aula vai ser um pouco diferente porque a gente vai terminar a aula falando com slides né emz de falar terminar a aula com atividades dirigidas a gente vai falar sobre dados aí eh como é que a gente eh cria um objeto que é o alimentador de dados pro treinamento tá que é o o a interface TF pdata Então essa é uma é um aspecto de tem bastante aspecto de paralelismo também tá então isso a gente vai ver eh no final da aula em preparação pra próxima aula onde a gente vai realmente aí ver como é que isso implementa tá então hoje a gente só vai discutir conceitos e na próxima aula a gente vai ver realmente atividade dirigida não deu tudo para não deu para colocar tudo na mesma aula tem um outro ponto também então que hoje só falando da estrutura da aula né a gente vai começar com esse com essa parte de callback extensor boards depois nós vamos ter três atividades dirigidas e o que eu queria mencionar é que a primeira atividade dirigida no meu computador não funcionou muito bem no Firefox que é o navegador que eu uso de normalmente né então para esse ad02 nós vamos utilizar Então o Google Chrome aqui que o Google Chrome eu percebi que funciona melhor para quando a gente for instanciar o tensor board dentro do cabbi ok então essa é a estrutura da aula de hoje mas antes de começar a aula só queria comentar uma uma uma um assunto que tem bastante a ver com essas duas disciplinas que a gente tá abordando agora que é um assunto que eu tenho me envolvido bastante eh faz um certo tempo já que que envolve obviamente todas essas ferramentas de ciências de dados

- *Corpus ID:* 7970
- *Score:* 0.8711835145950317
- *URL:* oculto
- *Início:* 00:29:41
- *Fim:* 00:32:14
- *Transcrição:* coloquei o exemplo lá o o o exemplo do próprio tutorial do tensor Flow sobre transferência de conhecimento onde ele faz esse comentário aí sobre Bet normalization tá então eh eu até entendo se a coisa ficar meio confusa e a dúvida original é bem pertinente mesmo né Eh mas os dois comandos não fazem exatamente a mesma coisa tá eh pessoal sobre o trabalho ainda tem alguma outra dúvida que não tá relacionada à aquela trad lá do do fórum a tarefa dois Professor isso trabalho dois isso eu tenho eu tenho pergunta eh quando a gente vai treinar o o a rede backbone por completo né Ou seja a gente vai colocar pesos aleatórios na no na primeira execução éal a gente esperar um uma acurácia próxima a 50% em função assim de ter poucas imagens e de e as imagens são bem distintas entre si né então parece ser um pouca coisa né uma cara coroa tá é o que tá acontecendo é isso ele a rede basicamente ela tá meio que tá com desempenho de um de um de uma cara e coroa a pergunta é isso é esperado aí assim ó posso tranquilizar vocês Beleza se vocês chegaram nisso tá tranquilo para esse dataset era isso e era mais ou menos essa ideia se eu treinar rede do zero com estee dataset aí pelo menos com essa quantidade de épocas ela vai ou ela vai demorar muito Eu ten que ter um um número de épocas muito maior para ela começar a e eventualmente ela vai over fitar nos dados de Treinamento Tá mas para aquela eu não sei por quantas épocas vocês treinaram tipo treinei para 40 E aí também deu per do aleatório ali sim então assim ó é é esperado sim tá aí o que que acontece quando eu puxo os pesos do ah de um pré-treinamento na de net aí aí novamente né em termos de aí eu tenho uma pergunta na na questão quatro você diz para usar o mesmo modelo da questão três não fala que é para buscar um os

- *Corpus ID:* 6821
- *Score:* 0.8708568215370178
- *URL:* oculto
- *Início:* 01:01:52
- *Fim:* 01:04:00
- *Transcrição:* gente pode fazer Talvez seja mostrar o treinamento e a validação deixa eu ver se eles são diferentes ou não no caso da validação ele mostra somente a a parte de accuracy Loss ele não mostra os modelos que os modelos já foram treinados né então é só na hora de treinamento mesmo que importa a observação dos modelos então se eu observar dois treinamentos eu esperava vê-los uns um sobre o outro mas na realidade ele tá colocando a abaixo aqui ele mostra eh separadamente né Eh em vez de mostrar o histograma um em cima do outro aqui como diferentes fatias digamos assim vocês têm alguma dúvida pessoal tá tranquilo tranquilo Professor Ok Bueno então Eh além disso a gente tem eh por exemplo a questão dos escalares aqui também tem uma série de outras gráficos que a gente pode observar pode mudar aqui o Smooth esse tipo de coisa então é uma assim se é usado ou não né tem essa parte do grafo né que permite a gente observar exatamente qual é o grafo de tarefas que tá sendo que tá acontecendo né então a gente consegue ver as classes aqui né Qual que é o o otimizador esse tipo de coisa enfim Qual que é o grafo de tarefas que tá sendo executado lá por baixo dos panos né para fazer o treinamento em si Então esse é o tipo de informação que tem a parte de distribu dos histogramas os histogramas a gente já viu né mas parte da distribuição que permite uma simplificação dos histogramas né a gente pode ver também os valores aqui então Esso é o que o tensor board mostra né então uma das coisas que a gente pode testar aqui ó é eu posso lançar o treinamento agora vou pedir 20 épocas aqui ó e normalmente isso vai aparecer aqui no próprio tensor board Então vou dar um Reload aqui então já vai aparecer esse novo treinamento que eu tô fazendo e no time series aqui aqui por exemplo ou talvez aqui na na parte dos histogramas ou distribuições

- *Corpus ID:* 6865
- *Score:* 0.8697112798690796
- *URL:* oculto
- *Início:* 00:36:47
- *Fim:* 00:38:41
- *Transcrição:* a tarefa da aula anterior então não hoje não tem Tá bom então vamos terminar a aula de hoje então nessa última hora aí né talvez a gente consiga terminar um pouco antes mas em geral isso aqui causa bastante perguntas tá eh que é a parte de lidar com dados né então ã que é extremamente importante então a gente viu ali já tem uma ideia de mais ou menos como é que funciona os tensores já sabemos como que instancia como é que usa GPU né já vimos ali como é que lista os devices e tal já Vimos que ele tem preferência por usar gpus a gente vai ver efetivamente assim como é que faz a distribuição na na próxima aula mas um aspecto fundamental na questão do paralelismo do treinamento de redes Profundas é a parte da entrada dos dados tá que até agora foi Bastante elementar né a gente simplesmente especificava x trin lá Y trin e aquilo eram matrizes que estavam na memória né estavam na memória a gente tinha lido aqueles dados né inclusive no ad02 que a gente fez hoje na no ad01 que a gente havia feito na aula anterior a mesma coisa a gente tinha lido todos os dados paraa memória notem como isso é distante da realidade né normalmente volumes grandes né não cabem na memória né tipo tu vai simplesmente não cabe na memória então não tem como tu ler tudo né para tu poder fornecer um X trin como um objeto que tem todos os dados né Então como que o tensor Flow ele é preparado para lidar com essa situação onde os dados simplesmente não cabem na memória não tem como carregá-los previamente pra gente fazer o treinamento então tem todo um mecanismo esse mecanismo é o TF pdata que é o que a gente vai falar sobre isso agora OK então como eu já comentei nessa rápida introdução o aprendizado profundo Ele sempre vai trabalhar com um volume absurdamente grande de dados que nunca vão caber na memória RAM de não importa a quantidade de computadores que tu usar né tipo assim é é bastante grande e eh o


**Resposta gerada pelo LLM**: O TensorFlow detecta operações raízes (folhas no grafo que não dependem de outras) e as escalona em CPU ou GPU.  Operações que dependem de outras são executadas após a resolução das dependências.  Há enfileiramento de tarefas, que podem ser quebradas em pedaços para execução em múltiplas threads (intra-thread e inter-thread) em CPU e GPU, usando vários tensor cores ou CUDA cores.  Existem duas filas: uma para tarefas do grafo e outra para pedaços de tarefas.

**Referências**:
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should provide relevant context to the organization of training tasks in TensorFlow's tensor graph. Supporting Evidence: The RESPONSE directly addresses how TensorFlow organizes training tasks by discussing the detection of root operations, task scheduling on CPU or GPU, handling of dependencies, and the queuing of tasks. It explains the execution process in detail, which is highly relevant to the PROMPT's inquiry about the organization of training tasks in a tensor graph.  Score: 3


---
