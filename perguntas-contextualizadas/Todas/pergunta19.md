**Pergunta 19**: Como o treinamento de aprendizado de máquina utiliza a multiplicação de matrizes e a soma de vetores, e como isso se relaciona com os Tensor Cores das GPUs da NVidia? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 6703
- *Score:* 0.8934205770492554
- *URL:* oculto
- *Início:* 00:43:49
- *Fim:* 00:46:06
- *Transcrição:* vetores que essa operação fundamental Então as gpus né da NVidia né só para abrir um par rápid que a gente já usou na disciplina anteriores A a NV hoje se vende como uma empresa de Inteligência Artificial Porque nas placas eles têm os tensor cores eles TM os cores normais os cuda cores e eles têm os tensor cores o que o que que é esse tensor Core é basicamente multiplicação de matriz e vetor feita num passo de tempo então para tu usar eles tem que usar lá organizar os teus dados mas como é básica de aprendizado profundo né eles fizeram Core né Não são muitos né Não aquele monte de core que a gente uma placa tem sei lá 3.000 cores a maioria é cuda cor normal mas tem um subconjunto que é de tensor cores né a TPU que a gente falou agora no início ela também é uma TPU ela focada nessa operação a operação de multiplicação de matriz soma de vetores que é a operação fund mental do cálculo de back propagation Ok então eu recomendo fortemente ó não são vídeos longos Tá e isso aqui vai vai realmente mostrar para vocês quão elementar é assim a parte matemática né claro que tem toda uma ideia por trás né mas O legal é que ele explica inclusive alguns conceitos que a gente vai ver aqui que é o conceito de por eu não vale a pena eu fazer todo esse processo né Eh dado por dado vamos supor que a gente quer treinar com imagens né que não vale a pena eu eu fornecer uma imagem e fazer o treinamento de minin R neural uma imagem por vez né isso ele explica por daí Porque fundamentalmente a gente vai usar melhor o recurso computacional se a gente fizer mais eh imagens ao mesmo tempo tem uma perda Com certeza né do ponto de vista da descida do Gradiente porque a gente vai olhar um monte de imens juntas assim mas enfim enfim não vou entrar muitos detalhes que vou deixar vocês assistirem realmente recomendo tá por isso até botei nos slides aqui porque desmistifica bastante

- *Corpus ID:* 6702
- *Score:* 0.8840662240982056
- *URL:* oculto
- *Início:* 00:42:10
- *Fim:* 00:44:26
- *Transcrição:* assim de quão básico é esse negócio e esse canal é um cara que faz vídeos de matemática Ele criou Então esse essa playlist sobre neural Networks e O legal é que é super visual assim aparece só a voz dele né super visual ele cria programas em Python para representar a a ideia certo então aqui por exemplo el tem quatro capítulos Ele explica o que que o que que é uma rede neural então né Bem bem assim elementar digamos assim para qualquer pessoa mundana não precisa nem ter um background de computação conseguir entender mas logo em seguida tu precisa um background um pouco matemático mas ele explica os detalhes que é como que as máquinas aprendem né O que que é a decida do Gradiente O que que significa ok então e como que a gente usa né como é que a gente implementa essa descida do Gradiente que é o aprendizado da máquina ok que na realidade é uma função ali que tu vai querer descer na função para achar o ótimo digamos assim e o mecanismo que que que tá por trás disso é o back propagation certo que é exatamente o que o treinamento faz né então o treinamento na realidade é uma de operações de forward back propagation Então como que isso funciona e aí ela termina só para vocês terem uma ideia com esse último capítulo aqui que é o cálculo do back propagation Qual que é a operação de álgebra linear que é feita para que aconteça o cálculo do back propagation E aí lá tá multiplicação de matrizes soma de vetores que essa operação fundamental Então as gpus né da NVidia né só para abrir um par rápid que a gente já usou na disciplina anteriores A a NV hoje se vende como uma empresa de Inteligência Artificial Porque nas placas eles têm os tensor cores eles TM os cores normais os cuda cores e eles têm os tensor cores o que o que que é esse tensor Core é basicamente multiplicação de matriz e vetor feita num passo de tempo então para tu usar eles tem que usar lá

- *Corpus ID:* 7062
- *Score:* 0.8789830207824707
- *URL:* oculto
- *Início:* 00:00:07
- *Fim:* 00:02:18
- *Transcrição:* Pronto acho que agora já tá gravando de novo isso aí bom pessoal então acho que vamos começar de imediato então na nossa atividade dirigida do número sete então que ela tá eh listada aqui na nossa aula número 4ro nós vamos então dar andamento então nessa parte de usando gpus usando a atividade viida 07 aqui ok então prá dirigida 07 que inclusive já abri aqui deixa eu só aumentar um pouco a fonte para vocês ela eh baseia-se no fato que a gente tá usando um runtime eh GPU que é esse aqui E aí agora o que que a gente vai fazer a gente vai ver como é que a gente usa então H uma GPU ou mais de uma GPU né como é que faz essa questão do treinamento distribuído Ok então para isso a gente vai eh usar o tensor Flow usar o keras eh usar o npai e pra gente definir a sementes dos números aleatórios das para tornar todo esse caderno de anotações reprodutível A gente vai definir então a semente de números aleatórios tanto do num pai quanto o tensor Flow Então a gente vai usar essas duas funções vai definir uma semente aqui para que na hora de inicializar os modelos depois com valores aleatórios né seja sempre a mesma ordem para todo mundo ok a gente tenha mais ou menos a mesmo comportamento aqui na hora de fazer o treinamento então PR a gente Então a gente vai usar hoje então o os dados do emist né usando o tensor Flow datas sets então para isso a gente já carregou ali a a parte do tensor Flow então isso a gente tem acesso então a aos dados então basta executar esse bloco aqui a gente vai baixar Então os dados diretamente dessa dessa API de de de dados do Google aqui ok vamos verificar então a disponibilidade de GP pus então para isso a gente vai fazer então aquela função que a gente já viu Inclusive a nas aulas anteriores e aí só para confirmar que nós temos uma GPU disponível Qual que é o nome dessa GPU e verificar se ela foi se ela tá

- *Corpus ID:* 6863
- *Score:* 0.8787577748298645
- *URL:* oculto
- *Início:* 00:33:40
- *Fim:* 00:35:42
- *Transcrição:* seria bem difícil de fazer isso né mas daria para fazer né vamos supor que tu tenha uma matriz gigante tu quer fazer um sei lá uma operação nessa Matriz nesse data né trabalhando com números né supor que é uma matriz de números tu quer fazer tu pode quebrar essa Matriz tu pode fatiar essa Matriz em pedaços né usando toda a parte de fatiamento que a gente viu e jogar o processamento em cada uma dessas gpus se fossem elas físicas realmente teriam ganho bastante grande né como elas são lógicas tu ainda vai ter um ganho né porque a GPU embora ela ela a GPU da NVidia especificamente né embora ela tenha esse conceito de tu mandar um Kernel para executar e ele retornar né dá para trabalhar de maneira assíncrona com essas gpus né inclusive invocação de querne é assíncrono né então se tu fizer a locação de memória síncrona e todo esse tipo de coisa a gente não entrou muito nesse nível de detalhe lá na outra disciplina né mas se tu fizer tu consegue fazer eh a execução de vários KS ao mesmo tempo entendeu então é isso que o tensor Flow tá usando lá por trás entende quando tu faz esse tipo de operação Mesmo trabalhando com gpus lógicas entende então ã isso pode trazer para ti um benefício que é o melhor uso da tua GPU física entende mas vai ter uma maior ocupação isso Dev pos entendi mas se eu por exemplo for treinar al uma rede profunda e eu tiver várias gpus eu não preciso em tese me preocupar com essa divisão ele vai usar todas ele vai usar todas né ele vai usar todos porque isso vai depender na realidade de como que tu instancia né tipo assim vai ser transparente somente se tu instanciar corretamente esse objeto aqui entende isso a gente vai ver mais para frente daí aí tu vai criar tua tu vai tu vai criar essa estratégia de distribuição vai ter lá uns parâmetros a gente vai ver tá E aí tu vai instanciar o teu modelo vai compilar o teu modelo na hora que tu for fazer Fit o Fit do

- *Corpus ID:* 7016
- *Score:* 0.878703773021698
- *URL:* oculto
- *Início:* 00:05:17
- *Fim:* 00:07:31
- *Transcrição:* todas no mesmo formato né formato cor quantidade de bytes né para porque a entrada da rede neural é é Aquela quantidade fixa né Por exemplo não poderia tratar uma imagem Pequena no ex numa rede que foi treinada para uma margem grande não é isso não é normalmente assim a a naqueles que trabalham por imagem né a camada de entrada ela tem ali uma certa quantidade de pixels digamos assim né de de de neurônios de entrada vamos dizer assim então a tua imagem ela que é uma imagem visual né tipo bidimensional com cores e tudo mais ela tem que ser transformada de alguma forma para fornecer cenda centrada entendeu então normalmente o pessoal simplesmente pega a imagem que é uma matriz vamos sepor assim simplificada né eh e tu lineariza ela entendeu tu transforma essa imagem numa uma num vetor num tensor e esse tensor então é alimentado Então se tu tem imagens de tamanhos diferentes ou imagens coloridas imagens eh preto e branco tu tem que fazer um pré-tratamento Isso faz parte do teu workflow né tu vai usar pandas para isso por exemplo para eh colocá-las todas no mesmo tamanho fazer todo fazer algum eventual crop delas né Tipo se tu não quer fazer uma um rescaling né que pode ser Talvez uma coisa que talvez tu queira remover uma parte não sei enfim para se adequar né e o mesmo é válido para qualquer entrada né tipo vídeo também vai ter que fazer esse tipo de de transformação e áudio também Ok bom feito então Eh dito isso Eh vamos adiante então começar então com a parte efetivamente da aula então para isso a gente vai olhar Então esse primeiro conjunto de slides aqui que é usando gpus para serar acelerar cálculo é a é esse primeiro conjunto de slides aqui então tá eh então o objetivo Então desse conjunto de slides a gente entender como que o treinamento acontece né Eh com múltiplas

- *Corpus ID:* 6709
- *Score:* 0.8785186409950256
- *URL:* oculto
- *Início:* 00:53:36
- *Fim:* 00:55:41
- *Transcrição:* Google e o tensor Flow é o é a única até onde eu conheço né o único que tem suporte para TPU entendeu então mas assim depois a gente na semana que vem a gente vai fazer uns experimentos comparativos tá e se vocês têm uma GPU tipo assim não sei se vale a pena tipo assim vocês vão ver que o desempenho de Treinamento paraas nossas para para alguns exemplos simples tá eh é bem equivalente o desempenho de treinamento em GPU e TPU né então não sei se vale a pena tu tu se preocupar com esse tipo de coisa tá ter o TPU lá específico para ti bom então indo adiante aqui a gente logo em seguida já vai passar pra parte de de eh do queras mais específico mas só para ter uma ideia geral né então o tensor Flow a gente vai importar como TF e do tensor Flow a gente vai importar o caras então com isso a gente vai estar usando a p keras implementada pelo tensor Flow e lá dentro daí a gente vai ter então a instanciação de um modelo então a gente pode instanciar um modelo sequencial dessa forma onde a gente vai especificar Então as camadas né através dessa uma lista é uma lista é um vetor de camadas então todas do mesmo tipo aqui né Então a primeira é uma é uma quer dizer assim do tipo Python né todas elas são queras layers mas a primeira é uma é uma de flaten para para pegar a entrada e tornar ela plana né porque a gente só tá trabalhando com tensores Então são vetores né embora a gente possa trabalhar com vetores multidimensionais a camada de entrada de uma rede neural é um é um não é um cubo ela é sempre um vetor Ok então essa fleten ela permite a gente pegar o dado seja ele qual for uma matriz uma imagem whatever transformar num num vetor e isso vai ser o estímulo inicial para minha rede neural E aí depois tem camadas aqui tem uma camada densa tem uma camada de drop Out para prevenir overfitting com 20% que ela vai meio que esquecer um pouco perder um

- *Corpus ID:* 7009
- *Score:* 0.8776053190231323
- *URL:* oculto
- *Início:* 01:48:02
- *Fim:* 01:50:05
- *Transcrição:* a gente paralelização uma né várias gpus para fazer o treinamento entendeu então acho que eu acredito que agora a gente tem uma maturidade suficiente Porque a gente já sabe como é que alimenta os datasets toda a preocupação que tem que foi inclusive o questionamento ali da Caroline de como que a gente coloca os dados na memória da GPU aí isso tá beleza aí agora a gente vai atacar realmente a questão do treinamento paralelizado em GPU entendeu então a gente vai ver primeiro eh múltiplas gpus isso na próxima aula múltiplas gpus no mesmo nó ou seja numa único computador com várias gpus e também vamos vamos ver como é que isso funciona em vários nós vamos supor que a gente tem um cluster de gpus como que a gente prepara o código para o usufruir desse cluster tá inclusive teve agora da turma um não sei se vocês quiserem acompanhar vai ter as defesas não sei se são abertas essas defesas mas ter as defesas de TCC da turma um e Teve um aluno que foi o eh foi o Rodrigo é que o Rodrigo ele basicamente fez esse esse esforço Ele usou não usou gpus né mas usou daí o tensor Flow distribuído ã para usar o cluster para fazer o treinamento entendeu Não só um único nome se vocês quiserem acompanhar aí vocês me avisem que daí talvez eu eu eu passo para vocês aí o o dia ali da Defesa Ok G eu eso acompanhar se possível Professor Beleza então tá eu vou eu vou passar para vocês depois a a o dia da defesa e aí o no dia da Defesa eu posso aí divulgar pra turma de vocês aí o o a apresentação para vocês até terem uma ideia Acho que até importante vocês olharem para vocês terem uma ideia assim do que que são os tccs né tipo assim que tipos de TCC estão acontecendo e tudo mais sei que tá um pouco longe para vocês mas daí vocês já podem ter uma ideia também ah eu até não tá longe não é não tá longe não V Tá longe não eh bom então isso que isso para resumir

- *Corpus ID:* 6483
- *Score:* 0.8730645775794983
- *URL:* oculto
- *Início:* 00:50:12
- *Fim:* 00:52:26
- *Transcrição:* não tem a gente não usa a saída de vídeo dela né a placa mãe da do Servidor lá já tem uma saída de vídeo VGA lá que para nós é mais do que suficiente porque ninguém nunca está de fronte à máquina né mas eh por exemplo os meus alunos né H tem muitos que TM essas placas de gamers em casa né E eles me relatam que usam ela para fazer teste com os kernos ali e tudo mais ou seja fazem cálculo nelas né e usam elas também pros jogos entendeu então eu não sei bem como é que o driver da NVidia funciona eu acredito que dê para funcionar as duas coisas ao mesmo tempo tá porque um jogo eh ele faz a mesma coisa ele faz cálculo também né só que o o cálculo é um pouco diferente porque usa o png né para fazer gráficos e tudo mais e e e aí tipo assim essa aplicação que tá rodando que é um jogo né ela tem coisas a mais porque ela realmente exporta paraa saída de vídeo ali né falasse do hmi aí né então eu acho que as duas coisas podem coexistir acho que não precisaria resetar a placa entendeu tu pode Inclusive durante ah a execução do teu jogo ao mesmo tempo fazer cálculo né tu obviamente vai ter o teu jogo sendo penalizado e vai ter o teu cálculo sendo penalizado porque vai estar competindo Mas eu acredito que funciona de maneira transparente isso aí pelo né porque a placa ela é multitarefa né consegue lançar vários kos ao mesmo tempo ela não faz só uma coisa é como uma CPU n ah buen mais alguma dúvida pessoal mais alguma dúvida não Sem dúvidas Então então vamos adiante então então agora a gente vai voltar aqui então pro nosso Moodle tá E aqui no nosso Moodle Então a gente tem a nossa primeira atividade dirigida que é o sepai então pra gente usar o seupai que é a nossa ad22 tá então a gente pode usar o colab aqui sem problemas tá eh a gente vai então usar uma Instância da GPU Vocês conseguem pegar uma Instância da GPU aí também então vou

- *Corpus ID:* 7017
- *Score:* 0.8728200197219849
- *URL:* oculto
- *Início:* 00:06:58
- *Fim:* 00:09:08
- *Transcrição:* também Ok bom feito então Eh dito isso Eh vamos adiante então começar então com a parte efetivamente da aula então para isso a gente vai olhar Então esse primeiro conjunto de slides aqui que é usando gpus para serar acelerar cálculo é a é esse primeiro conjunto de slides aqui então tá eh então o objetivo Então desse conjunto de slides a gente entender como que o treinamento acontece né Eh com múltiplas gpus Então a gente vai começar falando assim de uma parte mais básica né de como que a gente pode usar uma GPU e depois a gente avança para mais de uma GPU então h o suporte a GPU no tensor Flow como a gente já viu né Ele é nativo e o próprio tensor Flow ele prioriza o uso de gpus em relação a cpus porque as gpus são absurdamente mais eficientes para eh se fazer isso então mesmo que tenha cpus disponíveis ele não vai usá-las ele vai D prioridade para as para paraa GPU ou para as gpus e o e o mecanismo de detecção dessas várias gpus ele é de certa forma automático certo quando a gente tá trabalhando numa única máquina né porque ele já ele já foi concebido para isso então uma das primeiras coisas que ele faz é verificar Quais são as gpus que tem e tudo mais é claro que a gente tem algumas funções por exemplo essas aqui eh que é essa que aparece aqui que que Inclusive a gente já viu né que permite a gente eh listar Então quais são os devices físicos né então Eh como a gente já mencionou né existe toda toda a operação do tensor Flow é transformada num grafo de tensões que no caso seria um grafo de tarefas também né grafo de tensores a gente usa esse nome né porque é o nome da ferramenta né tensor Flow fluxo de tensores e esse grafo de tensores eles são escalonados né entre gpus e entre cpus que porventura estejam disponíveis para fazer o cálculo né então existe a figura de um escalonador né que vai fazer essas decisões claro

- *Corpus ID:* 7021
- *Score:* 0.8719883561134338
- *URL:* oculto
- *Início:* 00:13:22
- *Fim:* 00:15:35
- *Transcrição:* porque a gente chama de operações Raízes que são essas folhas que aparecem no grafo eh que elas não dependem de ninguém seria assim a origem né de uma informação então notem que aqui nós no nosso gráfico de tensores nós temem temos nós que na realidade dependem de outros nós então esses não são raiz porque eles precisam eles eles têm uma dependência eles precisam primeiro resolver a dependência deles para eles poderem ser executados então por exemplo o f aqui pro F executar eu preciso ter o abde então o f eu não consigo começar por ele já o a o b e o c posso começar por eles porque não Eles não dependem Depende de ninguém né então o que que ele vai fazer ele vai detectar essas operações raízes e vai escalonar isso né ou ou em CPU ou em GPU em função da da da disponibilidade né E aí o que que acontece acontece em do uma operação de eh eh entre trads e intra trads também então essa essa esse essa operação esse esse a por exemplo ele vai ser enfilado isso vai ser executado eh no numa trad e o que pode acontecer é que essa operação ela é quebrada mais uma vez né para ser executada por mais trads digamos assim a gente precisa de uma aceleração Extra como se a gente usasse mais de uma trade para fazer uma tarefa a chama isso de tarefa eh com múltiplas trads Ok e do lado da GPU a mesma coisa acontece né a gente tem um enamento de tarefas no caso a c aqui foi mapeada para executar na GPU E aí tem a c dnn lá dentro né que vai receber essa tarefa e vai fazer o que tem que fazer lá dentro eventualmente quebrando também em pedaços né vai usar os vários tensor cores ou os vários cud acores Então existe essa ideia de duas filas né Especialmente na na na na tanto em CPU quanto em GPU E aí eh essa primeira fila é a fila para tarefas do grafo então a gente en filera e depois a outra fila é pros pedaços de uma tarefa né então existe esses dois tipos de paralelismos né seria o paralelismo


**Resposta gerada pelo LLM**: O treinamento de aprendizado de máquina utiliza a multiplicação de matrizes e a soma de vetores como operações fundamentais no cálculo de backpropagation. Os Tensor Cores das GPUs da NVidia são projetados para realizar essas operações em um único passo de tempo, acelerando o processo de treinamento.  Para usar os Tensor Cores, os dados precisam ser organizados adequadamente.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. Supporting Evidence: The RESPONSE addresses how machine learning training utilizes matrix multiplication and vector summation, which are key components of the prompt. It also explains the role of NVIDIA's Tensor Cores in accelerating these operations, directly relating to the second part of the prompt. The RESPONSE is relevant to both aspects of the PROMPT and provides a clear connection between the operations and the hardware designed to optimize them.  Score: 3


---
