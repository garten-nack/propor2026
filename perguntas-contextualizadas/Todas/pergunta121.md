**Pergunta 121**: Quais são os desafios e áreas de aprendizado de máquina envolvidas na criação de assistentes pessoais mais humanos, como a Siri e o Google Assistant? 

**Segmentos recuperados pelo E5**:
- *Corpus ID:* 1291
- *Score:* 0.8874537944793701
- *URL:* oculto
- *Início:* 01:02:05
- *Fim:* 01:04:15
- *Transcrição:* aplicações de a que não eram possíveis com aquelas formas de a mais clássicas que a gente discutiu lá no início certo e aí eu fiz um compilado de algumas oportunidades mas obviamente a gente tem muito além né do que Opa do que a gente deixou só que aí do que a gente vai discutir aqui mas tem toda a questão dos assistentes pessoais por exemplo a Siri eu sempre esqueço o nome dessa da do Google enfim que não é assim essa questão é super complexa porque não é só interação tá não é só assim a questão de ter toda a questão né de receber um pedido interpretar aquilo né processar isso que é o processamento de linguagem natural entendeu que tá sendo pedido né Então essa essa questão de conseguir compreender a demanda e realizar aquela tarefa sendo pedida mas tem toda uma área de aprendizado ou de máquina especialmente aprendizado profundo que lida não então não só com essa resposta tarefa mas com tornar esses acidentes mais humanos no sentido até da voz as voz a voz desses assistentes parecer menos robotizada e mais humana então tem algumas referências eu coloquei aqui por exemplo né vocês vão ver que é justamente uma pesquisa para tornar a voz da Siri mais humana né mais parecida e não tão sintética tá então tudo isso é feito com aprendizado de máquina aprendizado profundo toda parte de personalização de conteúdo recomendações isso é o tipo de coisa que todo mundo no dia a dia percebe-se a gente lê uma notícia a gente recebe sugestões de Notícias se a gente faz compra de um conteúdo de um produto Desculpem a gente recebe sugestões de vários outros produtos que Possivelmente Nós também vamos gostar né Músicas séries enfim né porque porque basicamente existem padrões de comportamentos em humanos que nos permitem dizer olha quem comprou

- *Corpus ID:* 8021
- *Score:* 0.868338406085968
- *URL:* oculto
- *Início:* 00:44:27
- *Fim:* 00:47:08
- *Transcrição:* serving for aut for Now okay we 10 Okay your the first is Lisa Okay Perfect so I will see Lisa at on mayd have a great day bye então aqui a gente já tem uma uma interação bem mais bem mais complexa né que que soa bem natural Ela não é uma não é uma assim ah eu quero marcar um um corte de cabelo paraas para meio-dia Ah tá marcado pronto não tem Idas e Voltas o horário não tá disponível H então foi já demonstra bem mais sofisticação e a gente tem Claro atualmente os assistentes de voz que são eh bem eh difundidos eu acho que a a Alexa da da Amazon é uma das das mais populares depois mais recentemente tem uma perguntinha professora Ah pode falar Opa V professora Então esse último exemplo bastante realista né já já se baseou em redes neurais eu fiquei com a curiosidade e os anteriores lá a Elisa e o e o eugene ah Quais as tecnologias ali envolvidas a Elisa era baseada em regras simplesmente então o desenvolvedor criou uma uma série de padrões ah se a pergunta diz eu estou assim ou eu gostaria de X aí ele responde Ah o que significaria para você se você tivesse x ou então tinha uma série de de padrões já definidos E aí por exemplo quando ele não sabe o que resolver então ele ele eh coloca a a resposta Vamos mudar de assunto fale mais de outra coisa então ele ele ele tinha esse conjunto já implementado o ean eu não lembro o que que h não acho que não era ainda não era nada de de Deep learning a mina sim a mina não desculpa a duplex Voice essa sim já era baseada em em redes neurais recorrentes e esse próximo que a gente vai ver que é o lambda então já é uma já era uma uma rede neural mais avançada treinada prét treinada com um conjunto bem grande aqui 1,5 5 trilhões de palavras e aí ela foi ajustada para tarefas de geração de texto e ela ganhou

- *Corpus ID:* 2847
- *Score:* 0.8682205677032471
- *URL:* oculto
- *Início:* 00:08:28
- *Fim:* 00:10:41
- *Transcrição:* ele dá uma pergunta e ele te responde na primeira edição na primeira e na segunda turma ainda não existia tá o chat GVT E aí esse slide tem que ser atualizado né para refletir isso e nessa disciplina a gente vai ver os fundamentos né do que que com os componentes do chat GPT e a Ju vai ajudar talvez a desmistificar né O que que é o chat de PT quais os limites dele enfim um limite de sistemas baseados né em redes neurais Então tudo isso aqui que a gente tá vendo virtuais né Alexa você pede lá Alexa toca Raul elas entendem é a primeira coisa é processar a sua voz então entender que aquilo que você disse né é converter aquilo para um texto que ela consegue processar e é entender que aquelas palavras toca Raul então associadas né a música né do Raul Seixas especificamente do Raul Seixas Não de outro Raul aí qualquer outro artista Raul né E todos os assistentes virtuais e ela vai e busca uma música e te entrega lá para você ouvir então tem várias tarefas aqui ó que são feitas né por esses assistentes e a gente conseguiria decompor nessas tarefas e consegue o sistema consegue decompor essas tarefas e para cada componente ali por exemplo primeira parte processar a voz e aí contém uma rede neural para isso segunda parte pegar esse processamento e fazer a busca ali né O que que significa o toca Raul e a terceira parte é processar a resposta e te entregar né aquilo que seja mais provável dada aquela pergunta né então tem um monte de coisa acontecendo aqui em uma única solicitação né quando a gente faz por um assistente virtual ou então por exemplo pedir para agendar um compromisso né OK Google Agenda um compromisso na meu almoço com a chefe no na quarta-feira meio-dia então é primeiro ele tem processo é essa voz é entende o que que é almoçar que quer agendar né então ele tem que ir lá na agenda criar um

- *Corpus ID:* 1290
- *Score:* 0.868060290813446
- *URL:* oculto
- *Início:* 01:00:35
- *Fim:* 01:02:44
- *Transcrição:* aí eu comentei vocês né que Justamente a gente tá vai falar que ele habilidade de aprender a partir delas passados Então sempre que a gente falar aqui do nosso aprendizado de máquina a gente vai assumir que tem conjuntos de dados disponíveis certo Então essa é a área né que foi criada enfim aprendizado de máquina e aí depois eu não vou entrar em detalhes na nossa disciplina por conta né que a gente vai ter uma disciplina de aprendizado profundo vocês vão ver mas depois surgiu somente o aprendizado profundo que são modelos que tem uma capacidade de extração de conhecimento tecção de padrões muito maiores assim então principalmente lidando com dados não estruturados que seria a imagens texto vídeo áudio né então existe que foi toda essa área de plano tá de aprendizado profundo então por exemplo eu não vou falar de redes neurais na nossa disciplina porque a gente decidiu juntar no curso eles orais com aprendizado profundo Tá mas a gente tinha um modelo de redes neurais o algoritmo de redes neurais né que que era muito utilizado E aí começou a trabalhar mais uma evolução dessa de aprendizado de redes neurais para ter um número de camadas maior camadas implementando alguns filtros que você era muito bons por exemplo a imagens e é isso foi gerando toda essa área de aprendizado profundo que esse plano certo então todas essas evoluções Elas acabaram gerando muitas novas aplicações de a que não eram possíveis com aquelas formas de a mais clássicas que a gente discutiu lá no início certo e aí eu fiz um compilado de algumas oportunidades mas obviamente a gente tem muito além né do que Opa do que a gente deixou só que aí do que a gente vai discutir aqui mas tem toda a questão dos assistentes pessoais por exemplo a Siri eu sempre esqueço o nome dessa da do Google enfim

- *Corpus ID:* 8487
- *Score:* 0.8670618534088135
- *URL:* oculto
- *Início:* 00:02:00
- *Fim:* 00:04:25
- *Transcrição:* gerando saídas falsas né Essa a Alucinação que a gente vai falar um pouco mais depois ou então saídas tóxicas né ofensivas ou simplesmente saídas que não eram úteis pro usuário então mesmo eles sendo grandes modelos e tendo ótimos desempenh eles tinham esse comportamento Então nesse paper eh nesse modelo 3.5 eles buscaram alinhar Então os modelos com a intenção do usuário e aí eles fizeram uma um ajuste para uma gama de tarefas usando o reinforcement learning eh com a feedback do ser humano né E E com isso eles chegaram a modelos que eles cham de instru GPT porque vocês vão ver aqui que no na estratégia de reinforcement de aprendizado por reforço deles eles usaram instruções de humanos então na primeira fase aqui ele tem uma supervisionado então aqui por exemplo a o promt era eh Explique a a lua h o pouso na lua para uma criança de 6 anos aí elas escreviam Então tinha um grupo de pessoas que escreveram resposta para essa pergunta como o modelo deveria se comportar e esse dado foi usado para fazer o fine tuning então depois disso eles compararam fizeram uma segunda rodada aonde usaram Então esse modelo fado aqui né para para para responder algumas perguntas e aí de novo um grupo de pessoas classificava essas perguntas em ordem de qualidade né De acordo com o julgamento humano então com isso ele aprendeu Como que o ser humano né como que um um um uma pessoa que atribui Rot para aquilo considera o que que é melhor o que que é pior dentro aquelas respostas esse D esses dados todos foram usados então para calcular esse modelo de recompensa E aí entra mais na questão do do reinforcement learning mesmo né de aí ele otimiza isso de forma a ter uma política né que vai sendo atualizada então com isso ele consegue modelos altamente capazes de segir a instrução do usuário né então a pessoa fala a

- *Corpus ID:* 8022
- *Score:* 0.8632148504257202
- *URL:* oculto
- *Início:* 00:46:22
- *Fim:* 00:49:02
- *Transcrição:* implementado o ean eu não lembro o que que h não acho que não era ainda não era nada de de Deep learning a mina sim a mina não desculpa a duplex Voice essa sim já era baseada em em redes neurais recorrentes e esse próximo que a gente vai ver que é o lambda então já é uma já era uma uma rede neural mais avançada treinada prét treinada com um conjunto bem grande aqui 1,5 5 trilhões de palavras e aí ela foi ajustada para tarefas de geração de texto e ela ganhou as as notícias de jornal porque o um um funcionário da Google disse que a tecnologia tinha ganhado vida que a tecnologia tinha assim ideias próprias e bom foi eh e aí saiu só passou um teste de de touring não não passou no teste de de T eh e bem mais recentemente a gente tem então o o o chat GPT que foi lançado no no fim de 2022 o chat GPT a primeira versão era baseada no GPT 3.5 que era o gpt3 só que ajustado para não gerar conteúdo tóxico Porque então foi passado por um um processo de reinforcement learning with Human feedback quer dizer uma realimentação humana para ele eh eh ficar seguro digamos assim para ele não gerar não gerar respostas para perguntas que poderiam H teri um potencial de de gerar algum algum dano então uma a as pessoas podem perguntar como é que eu faço para para matar alguém sem ser detectado ele Ah sim Faça sim então esse esse tipo de de pergunta foi foi eh ele ele tá treinado a não responder eh com base nessa realimentação humana aqui bom então teve o o chat GPT E aí mostra ah como o chat PT falhou o teste de T na verdade não falhou Porque ele nunca nunca se nunca se propôs a se passar por um humano Então quem interage com ele deve saber que que é uma que é uma tecnologia que quem tá gerando aquelas aqueles textos são são é uma máquina Então vem a pergunta Será que alguma

- *Corpus ID:* 1274
- *Score:* 0.8629441857337952
- *URL:* oculto
- *Início:* 00:35:49
- *Fim:* 00:37:54
- *Transcrição:* muito fáceis de resolver na maioria das vezes mas difíceis de escrever formalmente certo então esses padrões a gente consegue reconhecer Mas é difícil a gente escrever algo que eu posso usar isso para automatizar uma tomada de decisão ou para resolver isso em grande escala Então se todos esses problemas que de forma intuitiva ou automática né A gente parece resolver para a gente é intuitivo elegato cachorro mas para a gente descrever isso é difícil tá e tendo essa limitação justamente foi o gatilho para a área de ar evoluir e evoluir para algo que Justamente a gente vai discutir em aprendizado de máquina tá então aí ela evoluiu justamente para não apenas imitar essa inteligência humana reproduzir essa inteligência a partir de um conjunto de de conhecimento prévio mas emular ou simular a forma como seres humanos aprendem a gente aprende a partir de padrões né a gente por exemplo a gente aprende a distingar do cachorro dentro diversas vezes esse é um gato Esse é um cachorro Esse é um gato Esse é um cachorro e a gente vai fazendo esse aprendizado né de que com todas as adversidade intrínseca aquele domínio que aquilo é um gato que não cachorro a gente é capaz de identificar gato cachorro independente suas características físicas né que são umas mais variadas então a área de ar evoluiu para permitir que esses sistemas de a possam adquirir o seu próximo o seu próprio conhecimento desculpe extraindo esses padrões a partir de exemplos ou de dados tá então Enquanto essa Iara simbólica que aquela usada inicialmente ela tinha um conjunto de regras né tô usando regras como uma palavra bem geral Mas eu tinha uma base de conhecimento né representado de uma forma que eu possa manipular computacionalmente e eu tenho dados a respeito de um novo caso né Por exemplo um novo paciente então eu tenho minha programação clássica que aqui é o meu processo de inferência que a gente chama certo e essa computação clássica

- *Corpus ID:* 8516
- *Score:* 0.8628129363059998
- *URL:* oculto
- *Início:* 00:54:07
- *Fim:* 00:56:41
- *Transcrição:* compara com GT4 né em quantidade de parâmetro então acaba que eu percebi que dando uma instrução complexa né no promt ele não obedecia aí eu quebrei o prompt em múltiplos prompts eu encade encade um promp atrás do outro né então cada inferência né leva sei lá duas três consultas a llm aí é aí que entra o l chain e você pode encadear essas consultas de uma maneira bem simplificada sim ele deixa naquela conversa né daquele Jason lá que a gente fez na mão aqui ele administra isso melhor porque na verdade é o quanto de memória que ele tem para conseguir lidar com isso eu também já vi papers assim que às vezes isso tem também um limite né mesmo que tu vai mandando em partes e ele não administra mais tém aquela primeira parte da conversa n Então eu acho que eles acharam formas aqui para para otimizar isso máximo né legal tá pessoal então vamos voltar eu volto agora deixa eu parar essa essa coisa aqui e deixa eu professora Viviane tá aí eu vou voltar então para a apresentação rapidinho que é só mais alguns slides e a gente termina eh que aqui então assim só um pouquinho de curiosidades do GPT 4 né isso aqui tava no na biblioteca deles então pedindo assim já comprovando que essa capacidade dele de administrar imagens né então o que que tinha de em comum nessa imagem ele e ele claro isso aqui é do próprio opena então aqui ele de fato conseguindo identificar que isso aqui que o homem passando roupa aqui encaixado no táxi é o que tem de emcomum né então Eh eles têm menos alucinações então a gente vê isso como uma luta assim constante então nãoa mais tanta coisa como ele inventava essas os erros que a gente falou eh isso aqui tá no paper deles até primeiro P4 eu acredito que é algo que eles mantém

- *Corpus ID:* 3401
- *Score:* 0.8623808026313782
- *URL:* oculto
- *Início:* 00:05:23
- *Fim:* 00:07:36
- *Transcrição:* esperar né Por exemplo Tá eu vou fazer uma analogia aqui que é o Andrew um ele escreveu um livro né que é machine learning eu acho até que tem esse eu sinto isso no no slide aqui mas já vão antecipar que a ideia é assim ó ele o caso de uso que ele cita né O Causo né digamos assim que a pessoa tá lá fazendo um aplicativo né de reconhecimento de sei lá de raça de gatos e aí pega um monte de gato da internet imagem fase todo o processo aqui né de coleta de dados E aí enfim passa por todas as etapas E aí chega no final aqui no feature e consegue fazer né o modelo E aí dá tudo certo né e agora que solta o aplicativo lá no na Play Store E aí pessoa começa a baixar e começa a vir uma estrela uma estrela todo mundo reclamando reclamando reclamando que o aplicativo não funciona não classifica direito e tal sendo que aqui nesse momento né a pessoa tava testando lá e tava com a curaça super boa detectava todas as raças de gato muito bem e tal e aí na hora que vai ver o que que acontece aqui ó o software foi solto no aplicativo de celular né para as pessoas tirar foto dos gatos e saber né a raça composição etc e só que aí aqui entra foto de celular né não é aquelas fotos bonitinha lá da internet que tinha no data 7 E aí então né os dados Nos quais o aplicação foi liberada não correspondiam né os dados que foram usados aqui então né vai ter que fazer o círculo aqui de novo a coletar dados né Possivelmente dos próprios usuários que fizeram né os do aplicativo e lá retornar o modelo Então essa essa parte né de manutenção faz é bastante sentido né quando se trata de uma aplicação real porque na hora que soltar o troço ali que a gente vai ter né um feedback real de um ambiente real com dados reais né Por mais que tenha sido feito um esforço aqui é na hora da verdade que a gente vai enfrentar ali nas Alô deve ter dado um corte aí perdão

- *Corpus ID:* 1320
- *Score:* 0.8621284365653992
- *URL:* oculto
- *Início:* 00:00:03
- *Fim:* 00:02:02
- *Transcrição:* ei pessoal então desculpa aí vamos Vamos recomeçar aqui então a gente falou né bastante até agora sobre a questão do QI onde é que se encaixa pintado de máquina O que é aprendizado de máquina né do ponto de vista de capacidade de aprendizado ali dentro da em relação uma e a mais clássica né e a simbólica a questão do uso de aprendizado de máquina como uma ferramenta eu vou chamar assim né importante para a parte de ciência de dados agora vou falar um pouquinho mais entrar um pouquinho mais a parte de aprendizado de máquina para a gente entender assim um pouco melhor o que que é o aprendizado de máquina né como uma como uma área do conhecimento e depois a gente vai falar um pouquinho sobre a questão das tarefas principais ele classificação e regressão tá disciplina em si muitas outras coisas que eu vou falar para vocês hoje a gente vai repetir elas ao longo do curso tá então Não se preocupem é claro tirei em dúvidas né a vontade mas não se preocupem se algumas coisas ficarem Claras porque elas vão a gente vai discutindo nessa medida que a gente for estudando os algoritmos especificamente Tá mas eu gosto de contextualizar essa parte mais geral antes da gente falar de questões específicas tá que seria os algoritmos então quando a gente está falando de aprendizado de máquina então a gente quer aprender a partir dos dados né fazer com que os métodos programas sejam capazes de aprender a partir dos dados ao invés de serem explicitamente programados para resolver uma tarefa tá então de aprendizado de máquina está falando de um processo de indução de hipótese a partir da experiência passada essa experiência passada elas são os dados né e a gente normalmente trabalha mas tem um conjunto de dados representativos são os nossos exemplos ali nossas instâncias


**Resposta gerada pelo LLM**: Criar assistentes pessoais mais humanos envolve desafios na interação (receber pedidos, interpretar e processar linguagem natural), aprendizado de máquina (especialmente aprendizado profundo) para tornar as respostas mais naturais e menos robotizadas (incluindo a voz), e personalização de conteúdo e recomendações baseadas em padrões de comportamento humano.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 1.00

**Anotação automatizada**: C

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should address the challenges and areas of machine learning involved in creating more human-like personal assistants. Supporting Evidence: The RESPONSE directly addresses the challenges of creating more human-like assistants by mentioning interaction, natural language processing, and the use of deep learning. It also discusses the importance of personalization in content and recommendations, which are relevant areas of machine learning. This comprehensive coverage of the prompt's requirements indicates a strong relevance to the entire prompt.  Score: 3


---
