**Pergunta 117**: Quais são os quatro níveis de preocupações de visualização no Nested Model?

**Segmentos recuperados pelo E5**:

- *Corpus ID:* 4982
- *Score:* 0.8502355217933655
- *URL:* oculto
- *Início:* 00:12:03
- *Fim:* 00:14:39
- *Transcrição:* trabalho ele tenta identificar como proceder a partir você definiu eh e alguns desses componentes ou tentar detalhar um pouco mais e esse modelo que ele que ela chama de nested Model ou modelo alinhado ele ele separa quatro níveis de preocupações de visualização que você deve ter esse modelo ele tá descrito nesse livro aqui visualization analysis and Design da Tamara mner é um livro texto clássico na área de visualização de dados porque ele tem os conceitos básicos de visualização Mas também de projeto e de desenvolvimento de projetos em visualização Então esse digamos assim dessa disciplina seria o principal livro eh o livro mais usado é o livro onde eh tem muito material des envolvido Se vocês forem no site da da da Tamara eh eu eu chamo ela de Tamara Porque ela foi minha colega de doutorado então eu convivi com ela conheço ela então se você clicar aqui no site você vê que tem muito material sobre esse livro ela tem eh basicamente as aulas que explicam todos os conteúdos do do livro no em vários formatos tem os slides caso você precise pegar alguma parte ela vem eh ministrando cursos sobre esse livro já vários anos na na napo que é a principal conferência de visualização e ela então enfim ela tem bastante eh material disponibilizado especificamente do livro especificamente dessa parte que a gente vai estar vendo aqui mas ela tem adicional sobre outras outros trabalhos que ela desenvolve também então Eh é uma boa referência para vocês manterem em mente então quando ela eh ela começou a a a tentar organizar esse livro já tem a ou pel Men esse modelo que ele foi proposto já há 15 anos atrás em 2009 muito do desenvolvimento de trabalhos de visualização não seguia uma roteiro ou uma uma uma receita de desenvolvimento

- *Corpus ID:* 8852
- *Score:* 0.8471058011054993
- *URL:* oculto
- *Início:* 00:40:48
- *Fim:* 00:43:33
- *Transcrição:* grafos Então são duas características principais né A primeira são as consultas de navegação né Então são aplicações em que a gente precisa fazer esse tipo de consulta e a outra é para quando a gente quer navegar e descobrir informações que não estão explícitas nos dados né então quando a gente navega pelos relacionamentos a gente pode descobrir ou não né informações eh que são relevantes pronto eh alguém tem alguma dúvida ou pergunta até aqui professora eh em relação às visualizações né Eh tem alguma ferramenta pessoal recomende para fazer uma visualização de de um de um brafo em árvore por exemplo é nud e o forj ele tem uma interface né para fazer eh para fazer as visualizações e E aí depois assim tem muita gente que usa eh as visualizações do tipo que vocês viram no na disciplina do komba né Eh do João komba de visualização que foi a outra a última disciplina aquele trabalho de TCC ali que eu mostrei para vocês do da campanha de das eleições de 2018 ele fez o processamento no newf e usou uma interface de visualização eh desvinculada do do sgbd né Então aí eh depende do que pode ser feito mas em geral os os bancos de dados hoje na o lá naquela época o newf J não tinha uma interface como ele tem hoje mas em geral as interfaces assim de visualização não são tão Claras assim como a gente vê na na área de visualização né então é muito comum essa combinação né de usar o banco de dados para processar e de fazer a visualização numa outra interface eh aproveitando a tua pergunta também tem trabalhos que que já que fizeram essa comparação assim porque eh do tempo de de processamento né quando eu [Música]

- *Corpus ID:* 2250
- *Score:* 0.8465386033058167
- *URL:* oculto
- *Início:* 00:56:08
- *Fim:* 00:58:43
- *Transcrição:* queria mostrar para vocês como eu falei eu vou pelo menos mostrar que a questão do notebook para vocês terem uma ideia o que que vocês vão avaliar pedindo uma atualização aqui tá queria mostrar para vocês deixa eu ver aqui tá ele tá aparecendo né esse notebook eu disponibilizei ele para vocês né Eu só queria mostrar que a estrutura porque o site tem ele tem formas a gente trabalhar ou dar outro repetido a gente já fez né já fez alguns notebooks certo e ele também tem a possibilidade de trabalhar com validação cruzada e a validação cruzada ele tem duas funções tá que eu vou comentar por alto aqui com vocês ele tem duas funções Então esse notebook o que que ele faz para esses dados que a pressão de risco de diabetes enfim dá um zoom aqui ele faz um treinamento de modelos com cafold cross-valiation tá usando as duas funções possíveis então basicamente são funções muito parecidas que executam avaliações a partir dos k folds tá mas uma das funções permite que por exemplo que vocês especifiquem um vetor de métricas a ser analisada enquanto a outra classe escola ela é focada em uma métrica só então tem algumas variações assim de implementação que historicamente né o século depois colocou essa ali dentro aí ficaram essas duas versões certo e aí depois o que vocês verificar é primeiro uma avaliação visual mesmo para vocês entenderem como que a validação cruzada de vídeo Os foldes e qual é a diferença de ter uma validação cruzada com muitos pontos e ela repetida tá então que seria que o repitais fold e por fim a otimização de prepaâmetros usando wrest e nessa do crossligation e aqui é interessante porque como eu falei para

- *Corpus ID:* 4047
- *Score:* 0.8464559316635132
- *URL:* oculto
- *Início:* 00:36:02
- *Fim:* 00:38:31
- *Transcrição:* categorias então visualmente intuitivamente dá para perceber que é uma técnica que que funciona funciona bem E aí é interessante nós nós tentarmos além da inspeção visual né serviu para mostrar para vocês que ela é uma técnica que que consegue né representar bem e manter as características originais é interessante fazermos alguma análise de matemática né para verificar se isso em termos de representatividade é ainda útil não é então aqui eu tento mostrar isso para vocês então A ideia é ter uma menor quantidade de perda o possível em relação ao original Mas é uma projeção toda projeção vai ter alguma perda então estudos anteriores mostram que se eu fizer bem feito eu vou ter a perda daí no máximo de 15%, mas eu chego apenas de um por cento que é praticamente nada né em termos de variabilidade que é o que ela representa então quero saber agora se eu tiver reduzindo né de quatro para três dimensões ou de quatro para duas Qual é a perda que eu vou ter efetivamente não é então isso isso pode ser feito calculando a taxa de Valença explicada que é justamente eu quero saber quanto essas duas dimensões conseguem explicar ou representar da variância original dos dados ou três ou mais dimensões existe um atributo que é que é devolvido Pelo modelo que justamente é essa essa taxa de variância explicada e eu consigo avaliar quanto cada componente tende taxa de variância explicada né E aí pelo pela maneira como o modelo funciona é importante dizer e ele vai criando componentes de maneira que o componente eles têm uma ordem eles têm um ranking um ordenamento o primeiro componente gerado é aquele que vai ter a maior taxa de

- *Corpus ID:* 4986
- *Score:* 0.8448539972305298
- *URL:* oculto
- *Início:* 00:20:00
- *Fim:* 00:22:40
- *Transcrição:* vê que nesses três componentes aqui a gente tem basicamente o que a gente tinha anteriormente no no design triangle Você tem os usuários Você tem os dados você tem as tarefas né então é similar né e cada digamos assim a abstração ou forma de proceder é é um pouco diferente Estão tocando nos mesmos pontos né E ela eh aqui Traz essa questão do do idioma que é Como como é que você mostra os dados né Qual é o o o idioma de codificação visual que você tá usando como é que você desenha E como é que você interage com os dados né Como que você manipula os dados então tem eh digamos assim dois específicos escolhas aqui o o idioma de codificação visual e idioma de interação né e por fim um último ponto é é que tudo isso acaba precisando ser implementado né E e essa implementação ela tem que lidar com questões de de eficiência Então os algoritmos que que vão suportar tudo isso precisam ser eficientes porque você precisa para interação suportar a interatividade por exemplo e a questão é uma vez desenvolvido um projeto como esse como é que você como é que você verifica se as suas escolhas foram as melhores possíveis né Então como que você valida o o teu teu projeto né E e aí ela menciona que existem como qualquer projeto existem formas diferentes de você errar e e cada um daqueles quatro níveis que a gente desenvolveu ou porque você não entendeu as necessidades do do usuário ou do domínio né isso É frequente né análise de sistemas você frequentemente você revisa você faz reunião com usuários você faz análise de requisitos você faz diferentes processos para entender as necessidades e às vezes existe uma falha de comunicação por idiomas diferentes que as pessoas conversam enfim você pode também errar no nível de abstração de dados e tarefas né né Você pode est mostrando eh algo que não é exatamente o que o usar quer ver né

- *Corpus ID:* 4984
- *Score:* 0.8446995615959167
- *URL:* oculto
- *Início:* 00:16:05
- *Fim:* 00:18:48
- *Transcrição:* quatro níveis e cada nível tá dentro do um do outro você pode olhar para esse esse quadrado aqui você tem domínio abstração tá dentro dele idioma tá dentro da abstração e o algoritmo tá dentro do idioma então isso vai tocar obviamente em alguns pontos que o modelo do do do design triangul que a gente mostrou anteriormente porque é um modelo simples mas que toca nas componentes importantes né então por exemplo no no design triângulo a gente falava dos usuários então aqui ela ela coloca isso também no na situação do domínio Quem são os usuários al né então isso é colocado de frente assim para que que você tá fazendo o que que você tá fazendo quem que vai se beneficiar da sua visualização né No segundo nível A gente tem então a a abstração Como que você traduz o os dados do domínio os detalhes do domínio para um vocabulário de visualização técnicas que a gente viu na primeira aula que a gente viu na prática com o alter na aula da semana passada né e um pouquinho com com a com com a parte da interface com do stream então Eh o que é interessante aqui também é que eh o design triângulo fala em users data and tasks aqui ela vai tá falando em o quem né os usuários em what né O que que é mostrado e aqui seria digamos assim os dados então aqui ser os usuários aqui ser Unos dados o que que é mostrado Qual é a abstração de dados que você vai tá mostrando e por fim a o How né desculpe Deixa eu passar já chego no R então o que que é mostrado né Eh o e o que é mostrado ele muito frequentemente ele não é mostrado exatamente como tá representado mas existe uma abstração dos dados e esse exemplo a gente viu os diferentes trabalhos que eu desenvolvi desde por exemplo vamos começar pelo trabalho de corredores onde eu peguei o batimento cardíaco e transformei na aqui no

- *Corpus ID:* 2177
- *Score:* 0.8446805477142334
- *URL:* oculto
- *Início:* 00:23:02
- *Fim:* 00:25:13
- *Transcrição:* treinamento do modelo né mas da parte já é essa divisão dos dados dados Independentes treinamento a validação enfim se eu fizer isso de forma Impecável se os meus dados forem muito ruins essa modelagem Impecável do ponto de vista de metodologia ela não salva esses dados ela dificilmente salva esses dados certo então a gente tem que cuidar a gente tem que entender o que que a gente precisa trabalhar em nível de dado né para que a gente possa tentar melhorar esse esse dado aqui que não é não é bom mas a gente também tem que cuidar para que a nossa metodologia do ponto de Treinamento não insira viésis não faço uma avaliação ruim né do ponto de vista de não perceber que tem um overfite tem coisas desse tipo ok e tem a questão da metodologia né do ponto de vista de risco de vieses e esse ponto apareceu um pouquinho naquele notebook que a gente fez né do nave base com dados de risco de doenças cardíacas em que tinha pessoas de diferentes Ali era variável raça né quantificada então e aí chamava atenção o caso ali que por exemplo nós tínhamos não tínhamos asiáticos com registro para doença né cardíaca e a gente discutiu um pouco isso pode ser uma questão de amostra de dados né um viés de amostra enfim a questão é que quando a gente não não Cuida dessa questão metodológica do desenvolvimento dos modelos e nesse ponto tá incluso né A questão da gente avaliar os dados perceber se a gente tem potenciais vieses enfim a gente acaba criando modelos com viés Então o que a gente tem que ver é que o nosso modelo ele não pode se cumprir com o nosso objetivo da tarefa de aprendizado de máquina né claro que a gente quer fazer isso aí mas isso não é a única questão a gente tem que garantir que ele não tenha implicações sociais

- *Corpus ID:* 7869
- *Score:* 0.8441670536994934
- *URL:* oculto
- *Início:* 00:50:19
- *Fim:* 00:52:34
- *Transcrição:* tá Bueno ã então em dois estágios né a gente tem alguns métodos que trabalham com uma única escala tá então rcnn a Fest a fester operam em uma única escala eh e a gente tem alguns métodos também que trabalham em diferentes escalas e usam basicamente os mesmos conceitos aqui que que a fester tá então eh a gente combina né informação das várias escalas e eh gera objetos eh gera detecções de objetos em diferentes tamanhos né Então esse é o principal objetivo de trabalhar com essas várias escalas eu tenho objetos menorzinhos que vão ter ã mais detalhe na minha imagem né No início lá da minha da minha rede convolucional e tem objetos maiores que eu só vou conseguir identificar e casar features né quando eu tiver nos últimos estágios da minha rede convolucional então para cada um deles eu vou tratar de uma forma levemente diferente e e eu vou usar essas features para localização e depois também para pra classificação bom o non Maxima suppression como eu comentei eh eh alguns usam não Maximum outros usam não máxima mas é tradicionalmente chamado de nms tá então ele basicamente pega as regiões que que o modelo diz que são associadas a um determinado objeto tá eh essas regiões elas geralmente têm um score de confiança e esse score de confiança ele tem relação com anotação tá então eu tenho a minha imagem anotada eu tenho um monte de detecção que que estão entorno daquela anotação as que estão mais próximas da anotação do Ground trof tem uma confiança maior as que estão mais distantes tem uma confiança menor e o meu objetivo no non Maxim supression é bom se tudo isso se refere ao mesmo objeto para que que eu vou ter um monte de detecção com o mesmo objeto isso vai só me atrapalhar então eu vou manter uma e eu vou manter aquela que tá com maior confiança tá

- *Corpus ID:* 3738
- *Score:* 0.8436962366104126
- *URL:* oculto
- *Início:* 01:32:19
- *Fim:* 01:34:50
- *Transcrição:* são observação três os três gráficos tridimensionais para o camedroides originais ele realmente ele só vai se confundir mais ou menos ali no mais ou menos o mesmo Ponto tem algumas coisas você vê que ele às vezes ali no ponto de expressão entre entre duas categorias às vezes ele coloca um ponto mais para uma categoria às vezes Coloca mais para outra é esse esse é uma problema desses elas tem dois né Um deles é é isso ele vai ele vai sempre tentar criar esses fugiu o termo agora essas bolhas digamos assim não é quantidade não seguir bolhas como é o caso do último exercício aqui não não vai funcionar direito é porque que a gente tem nós temos esses essas meias digamos assim e mesmo mesmo no caso de bolhas né Sempre nas fronteiras né entre entre mais de uma bolha ele ele pode e provavelmente vai errar alguns elementos né errar no sentido de se eu sei qual é o original Mas se eu não sei original na verdade não é um erro ele tá tomando uma decisão entre Qual é o conjunto mais mais próximo segundo as medidas que ele tá usando e a inicialização dos centros né então a ideia sempre é avaliar em situações como essas aqui né que eu não tenho informação prévia sobre o conjunto de dados e isso aqui são efetivamente 2 grupos ou se é um grupo só né eventualmente Pode ser que seja um grupo só né Aqui nós sabemos que são dois porque a gente viu lá nos dados original mas nós não tivermos isso aí nós vamos ficar sempre com essa com essa possibilidade E aí é uma decisão Nossa enquanto enquanto negócio né se eu diria que dividir em dois é

- *Corpus ID:* 8625
- *Score:* 0.8429808020591736
- *URL:* oculto
- *Início:* 00:06:43
- *Fim:* 00:08:58
- *Transcrição:* aparecendo em vários eh capítulos que estavam lá e e falta de tempo pra gente poder fazer um melhor sento de dados eu falei esses tratamentos para melhorar a tentar melhorar a curaça do dos modelos né Eh e agora com trabalho futuro a gente poderia fazer aqui é melhorar o camento como como falei né é classificar os produtos pela nsm completa tentar fazer uma forma disso né que mais complexo só só usando capítulo que são os dois períodos di já foi complicado então com todos mas assim um trabalho futuro legal se fazer a gente viu que tem muita coisa eh né Eh tentando fazer isso a tentar utilizar o Transformers né para poder ver se melhora e talvez também é o usar ner né para talvez com parte de pré processamento para poder tratar esses modelos e seria isso muito obrigado acho foi foi corrido mas não sei se deu certo para explicar o máximo possível Obrigado sim obrigada Alexandre eh deu para deu para ver bem o geral do trabalho eu pergunto se alguém alguém tem alguma pergunta do grupo eu vou então fazer uma algumas perguntas e a classificação tu ficou no nível de Capítulo vocês ficaram né isso ou seja então Então são 10 classes eu orientei um trabalho onde a pessoa usou NS ncm né então a realmente é quando tu vai entrando ali é muito complexo para chegar naquela classificação final lá que engloba toda aquela hierarquia que tu apresentou né Então tá eh por isso que ficou uma cura tão alta eu acho que eu lembro de trabalhos relacionados terem algo em torno de 90 aí Claro tu tem que otimizar como vocês colocaram aí nos trabalhos futuros poderia chegar nesse valor né e uma coisa que eu não vi também Talvez para pro relatório é quantas instâncias eh vocês tinham Ness nessas 10 classes seria legal vocês apresentarem alguma coisa tinha ali ó tá aham tá bom É no notebook a gente tem professora a gente tem o gráfico lá com

**Resposta gerada pelo LLM**: Domínio, abstração, idioma e algoritmo.

**Referências**:
- [Vídeo de referência oculto]()
- [Vídeo de referência oculto]()

**Anotação manual**: C

**Answer Relevance (AR)**: 0.000000

**Anotação automatizada**: I

**Raciocínio (AR CoT)**: Criteria: The RESPONSE must be relevant to the entire PROMPT to receive a maximum score of 3. It should accurately identify the four levels of visualization concerns in the Nested Model. Supporting Evidence: The RESPONSE lists four terms: "Domínio, abstração, idioma e algoritmo." However, these terms do not correspond to the recognized four levels of concerns in the Nested Model, which typically include different categories. Therefore, the RESPONSE is not relevant to the PROMPT as it provides incorrect information.  Score: 0

---